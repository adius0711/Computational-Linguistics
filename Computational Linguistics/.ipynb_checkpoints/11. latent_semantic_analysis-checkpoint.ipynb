{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJdPbVbId2vz"
   },
   "source": [
    "# Agenda for today\n",
    "\n",
    "* Review some concepts from `distributional_semantics_bow_dimensionality.ipynb`\n",
    "* Discuss implementation of latent semantic analysis and principal component analysis\n",
    "* Learn how to inspect your datasets with unsupervised word vectors\n",
    "\n",
    "# Agenda for Monday\n",
    "\n",
    "* Going over HW2 in class so you can look out for tips for HW3\n",
    "* Discussing the final project assignment and requirements\n",
    "\n",
    "# HW3 - Due Friday October 8th by midnight\n",
    "\n",
    "## HW3 tip (Question 4)\n",
    "\n",
    "Getting Question 4 correct is effectively a prereq for getting Questions 5, 6, 7, and the bonus correct. \n",
    "\n",
    "If you build a dictionary that maps between words and their morphemes and you want to build a dictionary that does the opposite, you need to \"invert\" the dictionary. For each of roots, prefixes, and suffixes, you want a data structure (a dictionary) that has as `key`s roots, prefixes, and suffixes, respectively. The `value` should be a `set` that contains all the words that contain that prefix. If you have a word like \"costumers\", it will have one root (`\"roots\": [\"costume\"]`), no prefixes (`\"prefixes\": []`), and two suffixes (`\"suffixes\": ['er', 's']`). To find other words containing `\"costume\"` as a root, you need to search through all the words in the homework (each line in the original file or each entry in the dictionary you build in Question 2) to see whether `\"costume\"` is in the roots **sub**dictionary. Since we want to do this _for all roots/prefixes/suffixes_, you need to make sure you loop through _all_ keys and make separate sets for each key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WYdVtYLGqQL"
   },
   "source": [
    "# Latent semantic analysis using NLTK and scikit-learn's `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 76,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "id": "rQq-ZB0hGRrR",
    "outputId": "f3ac3c50-2330-4121-e728-9bb251465e9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-6ae69d82-2045-407d-802d-bf2db749821c\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-6ae69d82-2045-407d-802d-bf2db749821c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving abstracts.tsv to abstracts (3).tsv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "\n",
    "abstract_file = files.upload()\n",
    "\n",
    "abstracts = abstract_file['abstracts.tsv'].decode(\"utf-8\").split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjsuzKQoHmRJ"
   },
   "source": [
    "## Earliest word embedding method: Latent semantic analysis\n",
    "\n",
    "The easiest method to learn word embeddings is to build a pipeline that implements Latent Semantic Analysis. The basic ingredients are as follows:\n",
    "\n",
    "1. Bag-of-words representations (using a sparse matrix package)\n",
    "  * Decide what vocabulary terms to keep\n",
    "    * Stop word removal\n",
    "    * Casing or text normalization\n",
    "    * What kind of tokenizer to use for segmentation\n",
    "2. Principal components analysis (PCA)\n",
    "  * Decide the number of dimensions you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxuevEnOLB7o"
   },
   "source": [
    "The bag-of-words representations shown in previous classes are slow and are not optimized. We can use others' implementations of sparse matrices and others' tokenizers to make this job easier for us. Specifically, we will do bag-of-words preprocessing using our familiar `nltk.word_tokenize` and a brand new tool, `sklearn.feature_extraction.text.CountVectorizer`. `CountVectorizer` will turn our lists of words into an unordered vector.\n",
    "\n",
    "Recall that each dimension of a bag-of-words representation corresponds to counts of a *single* word. That means that the `CountVectorizer` is going to give us a vector that is as long as our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNrnTZJ_G4Hc",
    "outputId": "8791c918-612f-4837-ea80-e8efee26af1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer( # instantiate sparse matrix-creator\n",
    "    tokenizer=word_tokenize, # with our tokenization algorithm\n",
    "    stop_words=stopwords.words(\"english\"), # typically remove stop words\n",
    "    lowercase=True) # optionally lowercase words\n",
    "# basically just one line to get a giant matrix\n",
    "bow_abstracts = vectorizer.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98O-6HL-LSnw",
    "outputId": "5dc90f6b-c281-44fe-a4cb-3936db52c8bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27471x74178 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1950546 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ypMHUwtOMN-"
   },
   "source": [
    "### Quiz yourself:\n",
    "\n",
    "<details>\n",
    "<summary>How many dimensions does this matrix have in it (how large is its vocabulary)? \n",
    "</summary>\n",
    "74,178 vocabulary items </details>\n",
    "\n",
    "<details>\n",
    "<summary>How many documents are in the corpus?\n",
    "</summary>\n",
    "27,471 documents </details>\n",
    "\n",
    "<details>\n",
    "<summary>How many total words are in the corpus?\n",
    "</summary>\n",
    "1,950,547 words </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW-odQ2eT4yJ"
   },
   "source": [
    "\n",
    "\n",
    "## Principal Components Analysis\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1280px-GaussianScatterPCA.svg.png\" width=500/> \n",
    "\n",
    "By <a href=\"//commons.wikimedia.org/wiki/User:Nicoguaro\" title=\"User:Nicoguaro\">Nicoguaro</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span> <a href=\"https://creativecommons.org/licenses/by/4.0\" title=\"Creative Commons Attribution 4.0\">CC BY 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=46871195\">Link</a>\n",
    "\n",
    "</center>\n",
    "\n",
    "The goal of PCA is to learn several dimensions that are geometrically **orthogonal** or statistically **uncorrelated** from the other dimensions. We can use PCA to transform large, complex spaces with correlations into smaller, more orderly spaces.\n",
    "\n",
    "The output of principle components analysis is a **projection matrix** that will correspond to all input dimensions and their lower-dimensional representations. For our purposes this means we get **lower-dimensional, latent vector representations of words**. But, we can also use this projection matrix to transform all of our documents (e.g., each abstract) into a latent document representation, too. Let's get a sense of how this works.\n",
    "\n",
    "We can use `scikit-learn` to build a Principal Components Analysis model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1Ba48dzT7FV",
    "outputId": "2a429858-3859-41ef-e14d-501b932cfe8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD # PCA but for sparse matrices\n",
    "\n",
    "N_COMPONENTS = 100\n",
    "\n",
    "pca = TruncatedSVD(n_components=N_COMPONENTS)\n",
    "pca.fit(bow_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKpUAew6U5ZZ",
    "outputId": "1639b2ec-dc34-4fe4-d3c5-72c5baf2d0ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 74178)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# investigate the size of the components\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABvOZUUgVCLu",
    "outputId": "e1d56abf-b995-4752-de17-6f154e697e19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74178,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_[0].shape # 0th dimension values, for all vocabulary items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGVXlSLLN4Fu"
   },
   "outputs": [],
   "source": [
    "word_vectors = pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7EU-iOei-5x",
    "outputId": "eadc8bb8-d8c2-4911-890d-8e78df6991dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a*': 0.8696203469011479,\n",
       " 'algebras': 0.9121852366777992,\n",
       " 'automaton-based': 0.8845172266719301,\n",
       " 'constituency': 0.8887536414738242,\n",
       " 'corner': 0.8657388453578299,\n",
       " 'non-projective': 0.8863091397526464,\n",
       " 'parse': 0.8477854776675771,\n",
       " 'parser': 0.9399177243565137,\n",
       " 'parsers': 0.9193941345745146,\n",
       " 'parsing': 1.0,\n",
       " 'projective': 0.8690298389224291,\n",
       " 'shift-reduce': 0.9111620440324334,\n",
       " 'subalgebras': 0.8845172266719301,\n",
       " 'transition-based': 0.9478722739564406,\n",
       " 'well-typedness': 0.8848748407807268}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the most similar words to 'parsing'\n",
    "# or any other word of your choice\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "_index = vectorizer.vocabulary_['parsing']\n",
    "word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                      word_vectors)\n",
    "_to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                            word_similarities[0].tolist()))\n",
    "dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWfIXlcPn1P2",
    "outputId": "8cc86eca-380f-490b-be2e-ac6350408b2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 0.7346482899086904,\n",
       " 'algorithms': 0.9999999999999998,\n",
       " 'automl': 0.5373835366555085,\n",
       " 'context-free': 0.6002903666609867,\n",
       " 'earley': 0.539177082826312,\n",
       " 'earley-like': 0.5674692888379949,\n",
       " 'equations': 0.5339359442375131,\n",
       " 'grammars': 0.5453604809733716,\n",
       " 'lols': 0.5923359445604042,\n",
       " 'n4': 0.5479863906197606,\n",
       " 'non-deterministic': 0.558096062981535,\n",
       " 'noncrossing': 0.567667089512389,\n",
       " 'programming': 0.5398624042758533,\n",
       " 'stochastic': 0.6197453377639178,\n",
       " 'tractable': 0.5936803414716441}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try again with algorithms, or any other word of your choice\n",
    "_index = vectorizer.vocabulary_['algorithms']\n",
    "word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                      word_vectors)\n",
    "_to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                            word_similarities[0].tolist()))\n",
    "dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "2GoL5yzsXb6u",
    "outputId": "2729ac53-1334-4957-c362-c764bf66968b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.508522e-04</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>1.353582e-06</td>\n",
       "      <td>8.315041e-04</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>-8.676564e-04</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>-2.587595e-05</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.001044</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>-6.164335e-04</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>-1.652553e-04</td>\n",
       "      <td>-2.365831e-03</td>\n",
       "      <td>-0.000893</td>\n",
       "      <td>-1.528323e-05</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>-5.713626e-04</td>\n",
       "      <td>-1.908943e-03</td>\n",
       "      <td>-6.294499e-04</td>\n",
       "      <td>-1.409423e-03</td>\n",
       "      <td>-1.635486e-03</td>\n",
       "      <td>-0.001952</td>\n",
       "      <td>-0.000308</td>\n",
       "      <td>-2.507402e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>-1.253474e-03</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.001527</td>\n",
       "      <td>-1.192067e-03</td>\n",
       "      <td>-0.003359</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>-0.001031</td>\n",
       "      <td>-0.000828</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>-6.482611e-04</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>-9.703770e-04</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>-0.001335</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>-6.243401e-03</td>\n",
       "      <td>9.909191e-04</td>\n",
       "      <td>-2.509745e-03</td>\n",
       "      <td>1.379547e-04</td>\n",
       "      <td>-0.001409</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>-0.001707</td>\n",
       "      <td>3.493320e-03</td>\n",
       "      <td>3.093989e-03</td>\n",
       "      <td>1.338832e-03</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>-0.003934</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.294723e-04</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>1.000498e-05</td>\n",
       "      <td>2.589890e-03</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>-7.997823e-04</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-4.260058e-04</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>6.385847e-04</td>\n",
       "      <td>-0.001910</td>\n",
       "      <td>-0.001181</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.002160</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>5.036685e-04</td>\n",
       "      <td>-1.288864e-03</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>1.099249e-03</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>1.170122e-03</td>\n",
       "      <td>6.274151e-04</td>\n",
       "      <td>-1.800135e-04</td>\n",
       "      <td>8.650981e-04</td>\n",
       "      <td>1.398082e-03</td>\n",
       "      <td>-0.001794</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>1.756000e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>-0.003756</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>-8.416673e-05</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.001710</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>-5.583696e-04</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>2.241782e-04</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>5.565419e-04</td>\n",
       "      <td>-0.002331</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>-0.002078</td>\n",
       "      <td>-2.005173e-03</td>\n",
       "      <td>1.998256e-03</td>\n",
       "      <td>4.128094e-04</td>\n",
       "      <td>-1.059568e-03</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>9.978561e-04</td>\n",
       "      <td>2.563318e-04</td>\n",
       "      <td>1.126380e-03</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>-0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.420705e-03</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>-2.549469e-05</td>\n",
       "      <td>1.308915e-02</td>\n",
       "      <td>0.006480</td>\n",
       "      <td>1.417869e-03</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>5.881375e-03</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.007401</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>-9.184990e-03</td>\n",
       "      <td>-0.004793</td>\n",
       "      <td>0.004727</td>\n",
       "      <td>0.013775</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.013914</td>\n",
       "      <td>-0.001598</td>\n",
       "      <td>-0.004094</td>\n",
       "      <td>-0.001335</td>\n",
       "      <td>-3.387353e-03</td>\n",
       "      <td>1.296457e-03</td>\n",
       "      <td>-0.016870</td>\n",
       "      <td>2.305251e-02</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>0.001778</td>\n",
       "      <td>-1.753166e-03</td>\n",
       "      <td>-1.596347e-02</td>\n",
       "      <td>-1.230162e-03</td>\n",
       "      <td>-5.778465e-03</td>\n",
       "      <td>1.993694e-04</td>\n",
       "      <td>-0.007993</td>\n",
       "      <td>-0.009042</td>\n",
       "      <td>9.619217e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020760</td>\n",
       "      <td>0.005880</td>\n",
       "      <td>0.040556</td>\n",
       "      <td>0.012217</td>\n",
       "      <td>3.051392e-02</td>\n",
       "      <td>-0.010736</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>-0.051142</td>\n",
       "      <td>-0.031155</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.071145</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>0.051603</td>\n",
       "      <td>3.949703e-03</td>\n",
       "      <td>-0.004079</td>\n",
       "      <td>-0.087182</td>\n",
       "      <td>-0.005062</td>\n",
       "      <td>-0.013488</td>\n",
       "      <td>-0.100350</td>\n",
       "      <td>-0.067229</td>\n",
       "      <td>1.053101e-02</td>\n",
       "      <td>0.016405</td>\n",
       "      <td>-5.334781e-03</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>0.016551</td>\n",
       "      <td>-0.047038</td>\n",
       "      <td>0.056713</td>\n",
       "      <td>-2.034352e-02</td>\n",
       "      <td>-1.898306e-02</td>\n",
       "      <td>-1.286549e-01</td>\n",
       "      <td>-2.135997e-02</td>\n",
       "      <td>0.092359</td>\n",
       "      <td>-0.049179</td>\n",
       "      <td>-0.120934</td>\n",
       "      <td>1.804317e-01</td>\n",
       "      <td>4.605868e-03</td>\n",
       "      <td>1.319433e-01</td>\n",
       "      <td>0.295594</td>\n",
       "      <td>0.169570</td>\n",
       "      <td>0.195337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.128181e-02</td>\n",
       "      <td>0.019869</td>\n",
       "      <td>-0.021352</td>\n",
       "      <td>0.032185</td>\n",
       "      <td>1.165490e-03</td>\n",
       "      <td>2.076500e-01</td>\n",
       "      <td>0.104504</td>\n",
       "      <td>-1.515096e-02</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>-4.333096e-02</td>\n",
       "      <td>0.026698</td>\n",
       "      <td>-0.344405</td>\n",
       "      <td>0.098499</td>\n",
       "      <td>-0.231285</td>\n",
       "      <td>-0.036671</td>\n",
       "      <td>-0.324086</td>\n",
       "      <td>-0.137413</td>\n",
       "      <td>4.397925e-02</td>\n",
       "      <td>0.016865</td>\n",
       "      <td>-0.053264</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>-0.046364</td>\n",
       "      <td>-0.062622</td>\n",
       "      <td>-0.117717</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>1.241132e-02</td>\n",
       "      <td>-7.870682e-03</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>-1.160631e-02</td>\n",
       "      <td>-0.011915</td>\n",
       "      <td>0.059466</td>\n",
       "      <td>3.280153e-02</td>\n",
       "      <td>3.591190e-03</td>\n",
       "      <td>-3.401531e-02</td>\n",
       "      <td>6.876528e-02</td>\n",
       "      <td>-3.835569e-02</td>\n",
       "      <td>0.043985</td>\n",
       "      <td>0.056251</td>\n",
       "      <td>-3.816471e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>0.015889</td>\n",
       "      <td>-0.011105</td>\n",
       "      <td>-0.004509</td>\n",
       "      <td>-2.429772e-02</td>\n",
       "      <td>-0.001466</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.056094</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>-0.005804</td>\n",
       "      <td>-0.002112</td>\n",
       "      <td>-0.024453</td>\n",
       "      <td>-1.958156e-02</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.030224</td>\n",
       "      <td>0.013177</td>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>6.087307e-03</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>-5.812074e-03</td>\n",
       "      <td>0.015066</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>0.017520</td>\n",
       "      <td>-0.011365</td>\n",
       "      <td>-2.239241e-02</td>\n",
       "      <td>5.772715e-03</td>\n",
       "      <td>9.259798e-03</td>\n",
       "      <td>-5.331364e-04</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>-0.012139</td>\n",
       "      <td>-0.012085</td>\n",
       "      <td>-7.943342e-03</td>\n",
       "      <td>-1.096266e-02</td>\n",
       "      <td>-4.574424e-03</td>\n",
       "      <td>-0.015592</td>\n",
       "      <td>-0.014714</td>\n",
       "      <td>0.006304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.011907e-03</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>1.514078e-05</td>\n",
       "      <td>4.197763e-03</td>\n",
       "      <td>0.005938</td>\n",
       "      <td>9.898040e-04</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>-6.519758e-04</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>-0.006886</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>-0.006615</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>-0.003836</td>\n",
       "      <td>-0.001154</td>\n",
       "      <td>2.242011e-03</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>-0.002772</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>-0.003699</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.002414</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.325931e-03</td>\n",
       "      <td>3.708289e-04</td>\n",
       "      <td>-0.000508</td>\n",
       "      <td>-6.897928e-04</td>\n",
       "      <td>-0.003537</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>1.220465e-03</td>\n",
       "      <td>-3.527645e-03</td>\n",
       "      <td>-9.539034e-04</td>\n",
       "      <td>2.507737e-04</td>\n",
       "      <td>-1.129052e-03</td>\n",
       "      <td>-0.002814</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>4.277262e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001222</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>6.699878e-03</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>-0.003528</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-1.875825e-04</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>-0.003323</td>\n",
       "      <td>-0.001482</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-2.317243e-03</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>1.126759e-03</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>-0.006643</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>-3.079779e-03</td>\n",
       "      <td>1.993201e-03</td>\n",
       "      <td>-2.435965e-04</td>\n",
       "      <td>7.287516e-04</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>-0.007639</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>1.914519e-03</td>\n",
       "      <td>7.417029e-04</td>\n",
       "      <td>3.868687e-03</td>\n",
       "      <td>0.005458</td>\n",
       "      <td>-0.001561</td>\n",
       "      <td>0.000681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74173</th>\n",
       "      <td>5.508020e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>3.902008e-08</td>\n",
       "      <td>-2.148285e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1.192037e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>-4.733774e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-2.205953e-07</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-8.047728e-06</td>\n",
       "      <td>1.981576e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-3.394681e-07</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>7.566422e-07</td>\n",
       "      <td>-2.323341e-06</td>\n",
       "      <td>-1.497987e-06</td>\n",
       "      <td>1.692201e-06</td>\n",
       "      <td>-7.640939e-07</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.465543e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.277427e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-9.375290e-07</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.185173e-07</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>2.484546e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-1.908105e-06</td>\n",
       "      <td>-3.642953e-06</td>\n",
       "      <td>1.385588e-06</td>\n",
       "      <td>1.230800e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-3.931665e-07</td>\n",
       "      <td>-7.127716e-07</td>\n",
       "      <td>9.729180e-07</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74174</th>\n",
       "      <td>4.454054e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.599545e-08</td>\n",
       "      <td>-2.304717e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.333328e-06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-5.714133e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-2.399629e-07</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-7.914676e-06</td>\n",
       "      <td>1.575722e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-8.899035e-07</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.072393e-06</td>\n",
       "      <td>-1.603309e-06</td>\n",
       "      <td>-1.440518e-06</td>\n",
       "      <td>1.540245e-06</td>\n",
       "      <td>-3.361553e-07</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.330491e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.971055e-07</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-9.060235e-07</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-5.147038e-07</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>7.320796e-08</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-1.315215e-06</td>\n",
       "      <td>-2.715923e-06</td>\n",
       "      <td>2.198376e-07</td>\n",
       "      <td>6.037610e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-5.195195e-08</td>\n",
       "      <td>-1.420769e-06</td>\n",
       "      <td>1.499336e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74175</th>\n",
       "      <td>5.823410e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>7.095883e-08</td>\n",
       "      <td>9.984656e-07</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>7.526233e-07</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>5.405501e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>3.197291e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>2.544112e-05</td>\n",
       "      <td>-9.741280e-07</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>1.348684e-05</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-6.470024e-06</td>\n",
       "      <td>-3.148818e-08</td>\n",
       "      <td>-5.366460e-06</td>\n",
       "      <td>1.071883e-05</td>\n",
       "      <td>-1.519370e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>6.346435e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.346989e-05</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-4.367514e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8.856288e-06</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-1.085567e-05</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-4.863706e-07</td>\n",
       "      <td>-8.467709e-07</td>\n",
       "      <td>7.509932e-06</td>\n",
       "      <td>6.734287e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.888836e-06</td>\n",
       "      <td>-1.853723e-06</td>\n",
       "      <td>1.671838e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74176</th>\n",
       "      <td>5.270179e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>6.521394e-08</td>\n",
       "      <td>7.939130e-07</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>6.862714e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>5.263857e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.471181e-07</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-9.892245e-07</td>\n",
       "      <td>2.078835e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>3.027326e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-1.448615e-06</td>\n",
       "      <td>-3.459200e-06</td>\n",
       "      <td>-5.261019e-07</td>\n",
       "      <td>7.048799e-07</td>\n",
       "      <td>-2.365294e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>3.029928e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.845493e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-7.032820e-07</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>7.331760e-07</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>1.513669e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-5.015745e-07</td>\n",
       "      <td>-3.557156e-06</td>\n",
       "      <td>2.113162e-06</td>\n",
       "      <td>3.097019e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-3.129120e-07</td>\n",
       "      <td>1.037638e-07</td>\n",
       "      <td>1.030804e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74177</th>\n",
       "      <td>5.270179e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>6.521394e-08</td>\n",
       "      <td>7.939130e-07</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>6.862714e-07</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>5.263857e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.471181e-07</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-9.892245e-07</td>\n",
       "      <td>2.078835e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>3.027326e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-1.448615e-06</td>\n",
       "      <td>-3.459200e-06</td>\n",
       "      <td>-5.261019e-07</td>\n",
       "      <td>7.048799e-07</td>\n",
       "      <td>-2.365294e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>3.029928e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.845493e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-7.032820e-07</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>7.331760e-07</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>1.513669e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-5.015745e-07</td>\n",
       "      <td>-3.557156e-06</td>\n",
       "      <td>2.113162e-06</td>\n",
       "      <td>3.097019e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-3.129120e-07</td>\n",
       "      <td>1.037638e-07</td>\n",
       "      <td>1.030804e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74178 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2   ...        97        98        99\n",
       "0      2.508522e-04  0.000346  0.000509  ...  0.000246 -0.003934  0.001443\n",
       "1      1.294723e-04  0.000270  0.000076  ...  0.000689  0.000159 -0.001133\n",
       "2      1.420705e-03  0.003052  0.001362  ...  0.295594  0.169570  0.195337\n",
       "3      1.128181e-02  0.019869 -0.021352  ... -0.015592 -0.014714  0.006304\n",
       "4      1.011907e-03  0.000950  0.001972  ...  0.005458 -0.001561  0.000681\n",
       "...             ...       ...       ...  ...       ...       ...       ...\n",
       "74173  5.508020e-07  0.000003  0.000044  ... -0.000008 -0.000005 -0.000007\n",
       "74174  4.454054e-07  0.000002  0.000035  ... -0.000005 -0.000004 -0.000006\n",
       "74175  5.823410e-07  0.000003  0.000048  ... -0.000010 -0.000007 -0.000007\n",
       "74176  5.270179e-07  0.000003  0.000044  ... -0.000008 -0.000004 -0.000006\n",
       "74177  5.270179e-07  0.000003  0.000044  ... -0.000008 -0.000004 -0.000006\n",
       "\n",
       "[74178 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtZtyV_mXosV",
    "outputId": "b5045938-b1db-4add-aa73-72dfb694774d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40516975, 0.10441562, 0.01776554, 0.0148028 , 0.01274815,\n",
       "       0.0064132 , 0.00601737, 0.00544769, 0.00459983, 0.00425061,\n",
       "       0.00363456, 0.00355068, 0.00336084, 0.00317367, 0.00291097,\n",
       "       0.00256747, 0.0024449 , 0.00228277, 0.00224959, 0.00215169,\n",
       "       0.00203389, 0.00199196, 0.00193537, 0.00189715, 0.00184204,\n",
       "       0.00177774, 0.00167116, 0.00165467, 0.00161592, 0.00160973,\n",
       "       0.00153325, 0.00151213, 0.0014922 , 0.00145026, 0.00142907,\n",
       "       0.00141081, 0.00140521, 0.00137056, 0.00136726, 0.00132098,\n",
       "       0.00131121, 0.00128607, 0.00126673, 0.00125625, 0.00121832,\n",
       "       0.00120132, 0.00117512, 0.00117264, 0.00115996, 0.00114554,\n",
       "       0.0011279 , 0.00111756, 0.00109896, 0.00109112, 0.00107385,\n",
       "       0.00103719, 0.00102193, 0.00100193, 0.00099382, 0.00097781,\n",
       "       0.00097445, 0.00095492, 0.00095173, 0.00092537, 0.00092223,\n",
       "       0.00090699, 0.00090035, 0.00088959, 0.00088761, 0.00088148,\n",
       "       0.00087294, 0.00085711, 0.00084424, 0.00084023, 0.00083245,\n",
       "       0.00082638, 0.00082375, 0.0008179 , 0.00080304, 0.00077736,\n",
       "       0.00077723, 0.0007705 , 0.00076498, 0.00075176, 0.00073997,\n",
       "       0.00073868, 0.0007329 , 0.00072569, 0.00070155, 0.00069419,\n",
       "       0.00069043, 0.00068134, 0.00066774, 0.00065298, 0.00063989,\n",
       "       0.00062758, 0.00062532, 0.00061877, 0.00061188, 0.00060804])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8PrzpXdSZTX",
    "outputId": "54e2d3ea-a40a-4401-c23d-b6b1a36e7220"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+0.85': 0.6888951447558753,\n",
       " '+2.58': 0.6888951447558753,\n",
       " 'decoder': 0.676695790520244,\n",
       " 'down-sizing': 0.70342135214239,\n",
       " 'heads': 0.7374774125445663,\n",
       " 'iwslt-2017': 0.6888951447558753,\n",
       " 'layers': 0.6937435166546098,\n",
       " 'multi-head': 0.7250317776702224,\n",
       " 'multihead': 0.6903641856083451,\n",
       " 'self-attention': 0.7940999368347967,\n",
       " 'straddles': 0.70342135214239,\n",
       " 'transformer': 1.0000000000000004,\n",
       " 'transformer-based': 0.6870660863846572,\n",
       " 'un-pruned': 0.70342135214239,\n",
       " 'wmt-2017': 0.6888951447558753}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try again with \"transformer\"\n",
    "_index = vectorizer.vocabulary_['transformer']\n",
    "word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                      word_vectors)\n",
    "_to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                            word_similarities[0].tolist()))\n",
    "dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdC4yGW8Sd5Q"
   },
   "outputs": [],
   "source": [
    "# let's turn the above into a function\n",
    "def get_sims(word, word_vectors, vectorizer, top_n=15):\n",
    "  _index = vectorizer.vocabulary_[word]\n",
    "  word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                        word_vectors)\n",
    "  _to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                              word_similarities[0].tolist()))\n",
    "  return dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-top_n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtQQdVu-Svom",
    "outputId": "7f036dbb-4ddf-4a7f-8d91-ce0b3c346d0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attribution': 0.7070898248655817,\n",
       " 'authors': 0.6200433892746091,\n",
       " 'authorship': 1.0000000000000002,\n",
       " 'genre': 0.5401800015899374,\n",
       " 'loadings': 0.5392901024670296,\n",
       " 'mbsp': 0.5369898907746585,\n",
       " 'sensing-intuitive': 0.5369898907746585,\n",
       " 'stylometric': 0.6822366028875404,\n",
       " 'sub-genre': 0.5392901024670296,\n",
       " 'thinking-feeling': 0.5369898907746585}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sims(\"authorship\", word_vectors, vectorizer, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2lXl3wNS6vB",
    "outputId": "6cfd563b-3456-4fda-ac6e-e8903a3df9c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clinical': 0.7473857450222506,\n",
       " 'doctors': 0.6860717648173797,\n",
       " 'healthcare': 0.666439776048538,\n",
       " 'hospital': 0.7059888950615654,\n",
       " 'medical': 0.9999999999999998,\n",
       " 'medicine': 0.7282930902760398,\n",
       " 'notes': 0.704215952721517,\n",
       " 'patient': 0.8081778307214346,\n",
       " 'records': 0.7508346603583982,\n",
       " 'treatments': 0.6796447992735681}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sims(\"medical\", word_vectors, vectorizer, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtOQdo9aoKVA"
   },
   "source": [
    "# Interpreting the word vector dimensions\n",
    "\n",
    "Finally, we can try to interpret the top words for each dimension, let's just try the first 7 dimensions.\n",
    "\n",
    "What we'll find is that some of them are interpretable, and others are less interpretable. For example, the 0th component appears to be frequent non-English words (e.g., \"de\", \"é\" from French) and symbols (\\{, \\}). The fourth component (=3) includes $LaTeX$ formatting symbols and other simple non-alphabetic letters. The third (=2) looks to be academi code words, and so on. \n",
    "\n",
    "The degree to which your space is interpretable depends on a few factors:\n",
    "\n",
    "1. How many vocabulary terms you are using at the beginning and how sparse they are\n",
    "2. How many dimensions you want to learn\n",
    "3. What your learning algorithm is to generate word vectors (e.g., PCA vs. co-occurrence/mutual information vs. word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBxB9G1DoUl7",
    "outputId": "18aefb1a-ce1e-4fc7-db2f-87623084fb63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "'\tde\t.\t,\t\\'e\t{\t}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1\n",
      "models\tmodel\tlanguage\t(\t)\t.\t,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2\n",
      "tutorial\tincluding\tal.\te.g.\tet\t;\t,\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3\n",
      "%\t1\t\\\t:\t;\t(\t)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4\n",
      "116\t105\t110\t97\t111\t101\t32\n",
      "----------------------------------------------------------------------------------------------------\n",
      "5\n",
      "''\t{\t}\t%\tmodels\t\\\tmodel\n",
      "----------------------------------------------------------------------------------------------------\n",
      "6\n",
      "\\\t'\t''\t{\t}\tsystem\tcorpus\n",
      "----------------------------------------------------------------------------------------------------\n",
      "7\n",
      "training\tmachine\tlanguages\tmodels\tdata\ttranslation\tlanguage\n",
      "----------------------------------------------------------------------------------------------------\n",
      "8\n",
      "neural\tnmt\tmt\tsystems\tsystem\tmachine\ttranslation\n",
      "----------------------------------------------------------------------------------------------------\n",
      "9\n",
      "information\tsemantic\tembeddings\twords\ttranslation\tlanguage\tword\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10\n",
      "semantic\tlanguages\tmethod\twords\tembeddings\tdata\tword\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11\n",
      "de\tinformation\tcorpus\t''\tl\tmodels\t'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "12\n",
      "speech\tannotation\t%\t\\\tdata\tcorpus\tmodel\n",
      "----------------------------------------------------------------------------------------------------\n",
      "13\n",
      "de\tmodel\tword\tdata\tsystem\t'\ttask\n",
      "----------------------------------------------------------------------------------------------------\n",
      "14\n",
      "propose\tlearning\tmethod\tdata\tlanguage\tknowledge\tinformation\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for dim in range(15):\n",
    "  dim_vecs = word_vectors.T[dim]\n",
    "  dim_vecs_named = dict(zip(vectorizer.get_feature_names(),\n",
    "                            dim_vecs.tolist()))\n",
    "  print(dim)\n",
    "  print('\\t'.join([x[0] for x in sorted(dim_vecs_named.items(), key=lambda item: item[1])[-7:]]))\n",
    "  print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd9B6YCRPLDL"
   },
   "source": [
    "# <font color=\"red\">NOTE: We did not get to anything below on 10/1/2021! We will cover this when we return to semantics after next week</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46nOMj7pa4QT"
   },
   "source": [
    "## Obtaining document representations with LSA\n",
    "\n",
    "In general, latent semantic analysis (LSA) is a great place to start to explore your data. You can use LSA word vectors in a wide variety of tasks. \n",
    "\n",
    "But, because of the way PCA works, we can also create a _document_ representation that lives in the same size space. Basically, we do matrix multiplication between our word embeddings (`word_vectors`) and our original bag-of-words matrix (`bow_abstracts`). \n",
    "\n",
    "`bow_abstracts * word_vectors`\n",
    "\n",
    "In this example, we would obtain a lower-dimensional document matrix that is 100 dimensions instead of 80,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKnKN8IQbWvn",
    "outputId": "5641fe48-1e49-4a92-f8fe-235109e9a93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.13830514e+00  1.11154095e+01 -3.03283940e-01 ... -2.98927800e-01\n",
      "  -8.14621672e-02 -1.11000604e-01]\n",
      " [ 3.21887023e+00  8.60508405e+00  1.73180506e+00 ...  9.02496108e-02\n",
      "  -1.82919169e-02  3.24698321e-01]\n",
      " [ 3.10511647e+00  8.49275678e+00  5.39884682e-01 ...  1.69585186e-01\n",
      "  -1.86813782e-02 -1.04224344e-01]\n",
      " ...\n",
      " [ 3.04857387e+01  1.25141350e+00  4.79350365e+00 ...  1.65147718e-01\n",
      "   6.43308758e-01 -7.18689582e-01]\n",
      " [ 7.30122405e+00  1.31476322e+00  1.47368206e+00 ...  2.88248065e-02\n",
      "   3.23041172e-01 -2.52280566e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "(27471, 100)\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = pca.transform(bow_abstracts)\n",
    "print(document_embeddings)\n",
    "print(document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4lUIqaNkbAS",
    "outputId": "34b49a74-a465-4434-cfee-0973a008f805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting document embeddings is a multiplication problem\n",
    "a = bow_abstracts * word_vectors\n",
    "a==document_embeddings # test for equivalence in methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSLT_LvZdLFa"
   },
   "source": [
    "# Exploring the document embeddings\n",
    "\n",
    "### Sort by each dimension to find the \"top\" match along that dimension.\n",
    "\n",
    "This document scores the highest on the 0th/first dimension because it is incredibly French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "W86EEqo5cn9m",
    "outputId": "4171d903-2e90-4d98-e3e7-176fdea1a678"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"La quasi-totalit{\\\\'e} des {\\\\'e}tiqueteurs grammaticaux mettent en oeuvre des r{\\\\`e}gles qui portent sur les successions ou collocations permises de deux ou trois cat{\\\\'e}gories grammaticales. Leurs performances s{'}{\\\\'e}tablissent {\\\\`a} hauteur de 96{\\\\%} de mots correctement {\\\\'e}tiquet{\\\\'e}s, et {\\\\`a} moins de 57{\\\\%} de phrases correctement {\\\\'e}tiquet{\\\\'e}es. Ces r{\\\\`e}gles binaires et ternaires ne repr{\\\\'e}sentent qu{'}une fraction du total des r{\\\\`e}gles de succession que l{'}on peut extraire {\\\\`a} partir des phrases d{'}un corpus d{'}apprentissage, alors m{\\\\^e}me que la majeure partie des phrases (plus de 98{\\\\%} d{'}entre elles) ont une taille sup{\\\\'e}rieure {\\\\`a} 3 mots. Cela signifie que la plupart des phrases sont analys{\\\\'e}es au moyen de r{\\\\`e}gles reconstitu{\\\\'e}es ou simul{\\\\'e}es {\\\\`a} partir de r{\\\\`e}gles plus courtes, ternaires en l{'}occurrence dans le meilleur des cas. Nous montrons que ces r{\\\\`e}gles simul{\\\\'e}es sont majoritairement agrammaticales, et que l{'}avantage inf{\\\\'e}rentiel qu{'}apporte le cha{\\\\^\\\\i}nage de r{\\\\`e}gles courtes pour parer au manque d{'}apprentissage, plus marqu{\\\\'e} pour les r{\\\\`e}gles plus longues, est largement neutralis{\\\\'e} par la permissivit{\\\\'e} de ce processus dont toutes sortes de poids, scores ou probabilit{\\\\'e}s ne r{\\\\'e}ussissent pas {\\\\`a} en hi{\\\\'e}rarchiser la production afin d{'}y distinguer le grammatical de l{'}agrammatical. Force est donc de reconsid{\\\\'e}rer les r{\\\\`e}gles de taille sup{\\\\'e}rieure {\\\\`a} 3, lesquelles, il y a une trentaine d{'}ann{\\\\'e}es, avaient {\\\\'e}t{\\\\'e} d{'}embl{\\\\'e}e {\\\\'e}cart{\\\\'e}es pour des raisons essentiellement li{\\\\'e}es {\\\\`a} la puissance des machines d{'}alors, et {\\\\`a} l{'}insuffisance des corpus d{'}apprentissage. Mais si l{'}on admet qu{'}il faille d{\\\\'e}sormais {\\\\'e}tendre la taille des r{\\\\`e}gles de succession, la question se pose de savoir jusqu{'}{\\\\`a} quelle limite, et pour quel b{\\\\'e}n{\\\\'e}fice. Car l{'}on ne saurait non plus plaider pour une port{\\\\'e}e des r{\\\\`e}gles aussi longue que les plus longues phrases auxquelles elles sont susceptibles d{'}{\\\\^e}tre appliqu{\\\\'e}es. Autrement dit, y a-t-il une taille optimale des r{\\\\`e}gles qui soit suffisamment petite pour que leur apprentissage puisse converger, mais suffisamment longue pour que tout cha{\\\\^\\\\i}nage de telles r{\\\\`e}gles pour embrasser les phrases de taille sup{\\\\'e}rieure soit grammatical. La cons{\\\\'e}quence heureuse {\\\\'e}tant que poids, scores et probabilit{\\\\'e}s ne seraient plus invoqu{\\\\'e}s que pour choisir entre successions d{'}{\\\\'e}tiquettes toutes {\\\\'e}galement grammaticales, et non pour {\\\\'e}liminer en outre les successions agrammaticales. Cette taille semble exister. Nous montrons qu{'}au moyen d{'}algorithmes relativement simples l{'}on peut assez pr{\\\\'e}cis{\\\\'e}ment la d{\\\\'e}terminer. Qu{'}elle se situe, compte tenu de nos corpus, aux alentours de 12 pour le fran{\\\\c{c}}ais, de 10 pour l{'}arabe, et de 10 pour l{'}anglais. Qu{'}elle est donc en particulier inf{\\\\'e}rieure {\\\\`a} la taille moyenne des phrases, quelle que soit la langue consid{\\\\'e}r{\\\\'e}e.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[document_embeddings_df.sort_values('0', ascending=False).iloc[0].name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ok8aNkY1dXSC"
   },
   "source": [
    "Likewise, this one is clearly a machine translation paper (the 8th dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "BjwEMmKtc9PX",
    "outputId": "c1009b11-b9f0-495c-b962-cebde4933806"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"We wrote this report in Japanese and translated it by NEC's machine translation system PIVOT/JE.) IBS (International Business Service) is the company which does the documentation service which contains translation business. We introduced a machine translation system into translation business in earnest last year. The introduction of a machine translation system changed the form of our translation work. The translation work was divided into some steps and the person who isn't experienced became able to take it of the work of each of translation steps. As a result, a total translation cost reduced. In this paper, first, we report on the usage of our machine translation system. Next, we report on translation quality and the translation cost with a machine translation system. Lastly, we report on the merit which was gotten by introducing machine translation.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[document_embeddings_df.sort_values('8', ascending=False).iloc[0].name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naQEAFxqdcSp"
   },
   "source": [
    "But the dimensions are not particularly interpretable -- consider if we look at the *worst* matches. In what way is this paper the least similar to that dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "0jtpH9IGdSQu",
    "outputId": "afdf8ebe-4f28-4dfb-bf47-0dcb6fd7a0af"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The infrastructure Global Open Resources and Information for Language and Linguistic Analysis (GORILLA) was created as a resource that provides a bridge between disciplines such as documentary, theoretical, and corpus linguistics, speech and language technologies, and digital language archiving services. GORILLA is designed as an interface between digital language archive services and language data producers. It addresses various problems of common digital language archive infrastructures. At the same time it serves the speech and language technology communities by providing a platform to create and share speech and language data from low-resourced and endangered languages. It hosts an initial collection of language models for speech and natural language processing (NLP), and technologies or software tools for corpus creation and annotation. GORILLA is designed to address the Transcription Bottleneck in language documentation, and, at the same time to provide solutions to the general Language Resource Bottleneck in speech and language technologies. It does so by facilitating the cooperation between documentary and theoretical linguistics, and speech and language technologies research and development, in particular for low-resourced and endangered languages.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[document_embeddings_df.sort_values('8', ascending=True).iloc[0].name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCWpWcDldofi"
   },
   "source": [
    "What does the least \"French\" document look like, then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "GC6aGToWdgYz",
    "outputId": "75d9aefc-6f95-4e6b-e964-92d2fa665844"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[document_embeddings_df.sort_values('0', ascending=True).iloc[0].name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmP2ZMDjd0Mj"
   },
   "source": [
    "Oh. (Well that explains it. A matrix of 0s will give you 0s everywhere.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2q7OjdhlSz8"
   },
   "source": [
    "# Closing notes for this week\n",
    "\n",
    "Thanks for sticking with it! Next week will be very hands on. Please try to come to class in any modality so you can get a running start on the final paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAftcm_WmJ9r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "latent_semantic_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

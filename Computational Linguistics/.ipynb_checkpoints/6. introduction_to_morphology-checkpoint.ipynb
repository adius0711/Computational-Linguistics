{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAoRDkZFHbHb"
   },
   "source": [
    "## Clarification, HW2:\n",
    "\n",
    "* Question 8 (mutual information question) says you need to pick an abstract from Question 5. Please pick an abstract from Question 6 (where you compare tokenizations).\n",
    "* Compute bigram statistics over the entire corpus (otherwise the numbers will be unreliable) -- pointwise mutual information values can be computed on the fly as long as you know $p(AB)$, $p(A)$ and $p(B)$.\n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuFxV5fY9Vx2"
   },
   "source": [
    "# What is a word?\n",
    "\n",
    "We have been working with the `nltk.word_tokenize` function for the last two weeks. Here is what we know:\n",
    "\n",
    "* It breaks up words like `\"don't\"` into two words: `[\"do\", \"n't\"]`\n",
    "* It does not break up complex words like `\"reading\"` into two pieces (i.e., it just says the word is `[\"reading\"]`).\n",
    "* With this tokenizer, we have no way to link similarly-spelled words to each other\n",
    "* Not all languages are like English\n",
    "\n",
    "## Morphology\n",
    "\n",
    "**Morphemes** are normally defined as the smallest \"meaningful\" parts of language. When they are combined, they form more complex meanings, from single words to whole texts. The system of morphemes of a whole language is called its **morphology**.\n",
    "\n",
    "There are many different kinds of morphemes and they can sometimes have different behavior. Every language uses morphology slightly differently. Here are just a few:\n",
    "\n",
    "* English\n",
    "  * **Inflectional morphology**: Keeps the base mostly the same\n",
    "    * Noun morphology\n",
    "      * Singular (_cat_) to plural (_cat**s**_)\n",
    "    * Verb morphology\n",
    "      * Progressive (dance to danc**ing**)\n",
    "      * Past (danced to danc**ed**) \n",
    "  * **Derivational morphology**: Changes the word to a completely different meaning\n",
    "    * American\n",
    "    * --> American**ize**\n",
    "    * ---> Americaniz**ation**\n",
    "    * ---> Americaniz**ability**\n",
    "    * ---> Americanization**ing**\n",
    "  * **Compounds**: Puts two morphemes together to create a new whole, usually with an _implicit_ grammatical relation\n",
    "    * Dollhouse (a house for dolls)\n",
    "    * Armchair (a chair with arms)\n",
    "    * Boxcar (a train car that is box-shaped)\n",
    "\n",
    "Many languages have much more complex systems than English. \n",
    "\n",
    "* For example, French has a rich system for conjugating verbs:\n",
    "  * je danse (I dance)\n",
    "  * je dans**ais** (I used to dance)\n",
    "  * je dans**erais** (I would dance)\n",
    "  * j'ai dans**Ã©** (I danced)\n",
    "* And Estonian has lots of ways of changing nouns depending on the meaning\n",
    "  * kohvik (cafÃ©)\n",
    "  * kohvik**u** (cafÃ©'s)\n",
    "  * kohviku**sse** (into the cafÃ©)\n",
    "  * kohviku**st** (from the cafÃ©)\n",
    "\n",
    "Among many other alternatives. In many indigenous languages throughout the world, more rich morphology systems exist. Other languages can even have \"words\" that express something like a whole sentence. For example, in Inuit languages, we can express the idea, \"Even though it's not snowing a great deal\" with a single word: [\"Qanniluanngikkaluaqtuq\"](https://en.wikipedia.org/wiki/Inuit_grammar#Verb_modifiers) that contains five morphemes (qanniq-, -luaq-, -nngit-, -galuaq-, -tuq). \n",
    "\n",
    "In some languages like Turkish, there is a system where the sounds within the word have to \"agree\" with the sounds of the previous morpheme. So, for example, the morpheme \"-dE\" has two possible forms: \"-de\" and \"-da\" depending on what precedes in. The phrase \"in Germany\" would be translated as \"Almanya**da**\" but \"in Turkey\" would be translated as \"TÃ¼rkiyede\" because the preceding vowel (/a/ and /e/, respectively) has control over how the morpheme sounds. The multiple ways that the \"same\" morpheme can be realized in pronunciation is known as an **allomorph**. Not all allomorphs are written differently. For example, compare these two words:\n",
    "\n",
    "* cat --> cats\n",
    "* dog --> dogs\n",
    "\n",
    "Do you hear a difference in how the \"-s\" sounds at the end of the word?\n",
    "\n",
    "<details>\n",
    "<summary>The difference is...</summary>\n",
    "that the -s in \"cats\" is said like the s in \"hiss\" ([s]) and the -s in \"dogs\" is said like the /z/ sound at the end of the word \"fries.\"\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiQtXyG_JOhj"
   },
   "source": [
    "## Types of morphemes\n",
    "\n",
    "Different morphemes will play different roles in a word. For example, the \"sleep\" in \"sleeping\" is clearly more important to the meaning than the \"-ing\" at the end. This is because, in a language like English, \"sleep\" can more or less stand on its own and be understood.\n",
    "\n",
    "The morpheme \"sleep\" in \"sleeping\" is usually called the **root** of the word. The **root morpheme** of any word is usually the most important morpheme.\n",
    "\n",
    "Other morphemes that are less central to the meaning are **prefixes**, which come *before* other morphemes (the **pre-** in \"prefixes\"), **suffixes** (the **-es** in \"suffixes\"), which come *after* other morphemes, **infixes** (rare in English but things like Homer Simpson's saxo-**ma**-phone.)\n",
    "\n",
    "Morphemes that cannot occur on their own, without being attached to anything else, are known as **bound** morphemes. For example, we cannot normally say \"pre-\" without some additional context (e.g., \"Did you pre-date or post-date this file?\" \"Pre-.\"). \\[Probably we should use another metaphor for this soon.\\]\n",
    "\n",
    "Morphemes that can stand on their own are known as **free** morphemes because they can occur in many contexts and positions. Some good examples of free morphemes in English are words that occur in compounds -- armchair, bookcase, etc. All the words \"arm\", \"chair\", \"book\", and \"case\" can occur on their own and be added to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaeRtFYG56Go"
   },
   "source": [
    "## Traditional approaches to morphology: Finite State Machines\n",
    "\n",
    "Traditionally, we handled morphology with **finite state machines** and generated novel wordforms using **finite state transducers**. Finite state methods are foundational in our understanding of natural language processing approaches, but they are present in many of the systems we see every day, such as a classic gumball machine:\n",
    "\n",
    "<center><img src=\"http://www.openfst.org/twiki/pub/GRM/PyniniDocs/gumball.png\" alt=\"Finite state tranducer representing a gumball machine\" /></center>\n",
    "\n",
    "We expect that if we turn the nob and give the machine a coin, that we will get a gumball out. Getting a gumball will reset the knob.\n",
    "\n",
    "**Weighted finite state transducers** are like regular finite state machines, except they assign a weight -- which can be converted to a probability -- to each transition. For the above, this could be a more complex gumball machine that 30% of the time gave you a gumball, and 70% of the time gave you one of those plastic slappy hands.\n",
    "\n",
    "<center><img src=\"https://m.media-amazon.com/images/I/71agbbZgQ7L._AC_SL1452_.jpg\" width=250 alt=\"Six multicolored ticky hands toys\"/></center>\n",
    "\n",
    "## How is this like morphology?\n",
    "\n",
    "Morphology can also be thought of as **transitions** between sounds or morphemes. However, instead of knobs and coins and gumballs, we have sequences of morphemes, e.g., the Estonian \\<kohvik-u-sse\\> that we saw earlier.\n",
    "\n",
    "In general, finite state approaches at the highest level are appropriate characterizations of many NLP problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c4O6AtzNs-m",
    "outputId": "432bcc21-cd5b-41c4-943b-0957153d41b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "drive.mount('/content/drive/', force_remount=True)\n",
    "\n",
    "abstracts = open(\"/content/drive/MyDrive/Teaching/Fall2021/\"\n",
    "                 \"Computational Linguistics/Lectures/\"\n",
    "                 \"supplementary_files/abstracts.tsv\").readlines()\n",
    "\n",
    "# load in spacy model so we can lemmatize\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtcpEd5OId7b"
   },
   "source": [
    "## Stemming\n",
    "\n",
    "One cheap way to link words with related wordforms -- and probably meanings -- together is to use the [Porter Stemmer (\"Suffix stripping\") algorithm](https://tartarus.org/martin/PorterStemmer/python.txt). It is rarely used today, because it does not generally do a sophisticated enough job at breaking words into their constituent morphemes. But, it is a fast and efficient algorithm that simplies word forms. For example, it will take a word like \"stopping\" and produce \"stop\" because it simplifies doubled consonants (\"pp\") and strips off specific word endings (\"-ing\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkcWM7ypUITv",
    "outputId": "2bde7add-d645-40c2-8bb3-a5be7d26229d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('offens languag detect ( old ) ha receiv increas attent due to it societ '\n",
      " 'impact . recent work show that bidirect transform base method obtain impress '\n",
      " 'perform on old . howev , such method usual reli on larg - scale well - label '\n",
      " 'old dataset for model train . To address the issu of data / label scarciti '\n",
      " 'in old , in thi paper , we propos a simpl yet effect domain adapt approach '\n",
      " 'to train bidirect transform . our approach introduc domain adapt ( DA ) '\n",
      " 'train procedur to albert , such that it can effect exploit auxiliari data '\n",
      " 'from sourc domain to improv the old perform in a target domain . experiment '\n",
      " 'result on benchmark dataset show that our approach , albert ( DA ) , obtain '\n",
      " 'the state - of - the - art perform in most case . particularli , our '\n",
      " 'approach significantli benefit underrepres and under - perform class , with '\n",
      " 'a signific improv over albert . \\n')\n"
     ]
    }
   ],
   "source": [
    "# using the porter stemmer\n",
    "import nltk\n",
    "from nltk.stem import porter\n",
    "from pprint import pprint\n",
    "\n",
    "stemmed_abstract = []\n",
    "stemmer = porter.PorterStemmer()\n",
    "nlped_abstract = nlp(abstracts[0])\n",
    "for w in nlped_abstract:\n",
    "  stemmed_word = stemmer.stem(w.text)\n",
    "  stemmed_abstract.append(stemmed_word)\n",
    "\n",
    "pprint(' '.join(stemmed_abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLuktDG8XThX"
   },
   "source": [
    "## Some issues with stemming\n",
    "\n",
    "One of the main reasons stemming is dispreferred these days is that it increases ambiguity. By throwing away (stripping) final information, you lose clues to:\n",
    "* What kind of word it was (noun or verb)\n",
    "* What grammatical role that word played -- e.g., past versus present tense\n",
    "\n",
    "But there are other disadvantages, too:\n",
    "* Generally not nice for people to read\n",
    "* Not sensitive to context -- only uses word form itself\n",
    "* Not robust to some types of string errors (e.g., \"identif**ies**inconsistencies\")\n",
    "\n",
    "The system does have some advantages though:\n",
    "* \"Unlexicalized\" -- Porter stemmer's behavior does not depend on what word is being stemmed\n",
    "* Able to generalize to unseen words\n",
    "* Much faster than statistically optimized methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-Gas4kx507C"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "* Reverting a word to its base (root) form\n",
    "  * sleeps --> sleep\n",
    "  * bears --> bear\n",
    "  * is --> be\n",
    "  * it/they/he/she --> -PRON- (short for pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cKpDxnBrra1E",
    "outputId": "f1f1f83c-7a36-4833-9ea7-eb767c33b815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('offensive language detection ( OLD ) have receive increase attention due to '\n",
      " '-PRON- societal impact . recent work show that bidirectional transformer '\n",
      " 'base method obtain impressive performance on OLD . however , such method '\n",
      " 'usually rely on large - scale well - label old dataset for model training . '\n",
      " 'to address the issue of datum / label scarcity in OLD , in this paper , '\n",
      " '-PRON- propose a simple yet effective domain adaptation approach to train '\n",
      " 'bidirectional transformer . -PRON- approach introduce domain adaptation ( DA '\n",
      " ') training procedure to ALBERT , such that -PRON- can effectively exploit '\n",
      " 'auxiliary datum from source domain to improve the old performance in a '\n",
      " 'target domain . experimental result on benchmark dataset show that -PRON- '\n",
      " 'approach , ALBERT ( DA ) , obtain the state - of - the - art performance in '\n",
      " 'most case . particularly , -PRON- approach significantly benefit '\n",
      " 'underrepresented and under - perform class , with a significant improvement '\n",
      " 'over ALBERT . \\n')\n"
     ]
    }
   ],
   "source": [
    "# spacy's lemmatization of the abstract\n",
    "pprint(' '.join([x.lemma_ for x in nlp(abstracts[0])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ypPT-JT_JlK"
   },
   "source": [
    "Problems associated with lemmatization:\n",
    "  * Datasets and transformations must be curated (time consuming)\n",
    "  * Lossy -- removes morphological information\n",
    "    - Not directly possible to retrieve it from context\n",
    "  * List of lemmas not update automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqNq-2R7sDoz",
    "outputId": "a931a8cd-9521-4719-8cd3-3001f4a8f706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Offensive', 'offensive'), 'offens'),\n",
       " (('language', 'language'), 'languag'),\n",
       " (('detection', 'detection'), 'detect'),\n",
       " (('(', '('), '('),\n",
       " (('OLD', 'OLD'), 'old'),\n",
       " ((')', ')'), ')'),\n",
       " (('has', 'have'), 'ha'),\n",
       " (('received', 'receive'), 'receiv'),\n",
       " (('increasing', 'increase'), 'increas'),\n",
       " (('attention', 'attention'), 'attent'),\n",
       " (('due', 'due'), 'due'),\n",
       " (('to', 'to'), 'to'),\n",
       " (('its', '-PRON-'), 'it'),\n",
       " (('societal', 'societal'), 'societ'),\n",
       " (('impact', 'impact'), 'impact'),\n",
       " (('.', '.'), '.'),\n",
       " (('Recent', 'recent'), 'recent'),\n",
       " (('work', 'work'), 'work'),\n",
       " (('shows', 'show'), 'show'),\n",
       " (('that', 'that'), 'that')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([(x.text, x.lemma_) for x in nlped_abstract], stemmed_abstract))[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PRA-JmmTr06l"
   },
   "outputs": [],
   "source": [
    "# compute basically the same thing as for n-grams\n",
    "# conditional probability of lemma given wordform\n",
    "\n",
    "lemma_to_wordforms = {}\n",
    "wordforms_to_lemmas = {}\n",
    "# look at the first 1000 abstracts\n",
    "for abstract in abstracts[0:1000]:\n",
    "  tokenized_abstract = nlp(abstract)\n",
    "  # one direction\n",
    "  lemmas = [token.lemma_ for token in tokenized_abstract]\n",
    "  raw_strings = [token.text for token in tokenized_abstract]\n",
    "  for i, lemma in enumerate(lemmas):\n",
    "    raw_string = raw_strings[i]\n",
    "    if lemma not in lemma_to_wordforms:\n",
    "      lemma_to_wordforms[lemma] = {raw_string: 1}\n",
    "    else:\n",
    "      if raw_string not in lemma_to_wordforms[lemma]:\n",
    "        lemma_to_wordforms[lemma][raw_string] = 1\n",
    "      else:\n",
    "        lemma_to_wordforms[lemma][raw_string] += 1\n",
    "  # the other direction\n",
    "  for i, raw_string in enumerate(raw_strings):\n",
    "    lemma = lemmas[i]\n",
    "    if raw_string not in wordforms_to_lemmas:\n",
    "      wordforms_to_lemmas[raw_string] = {lemma: 1}\n",
    "    else:\n",
    "      if lemma not in wordforms_to_lemmas[raw_string]:\n",
    "        wordforms_to_lemmas[raw_string][lemma] = 1\n",
    "      else:\n",
    "        wordforms_to_lemmas[raw_string][lemma] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TS4Colalv3BY",
    "outputId": "d7b4a30b-c991-47f1-c6b6-adf87a400093"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Being': 3,\n",
       " 'are': 644,\n",
       " 'be': 398,\n",
       " 'been': 187,\n",
       " 'being': 31,\n",
       " 'is': 1260,\n",
       " 'was': 127,\n",
       " 'were': 89}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_to_wordforms['be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfIkn8mT1WsN",
    "outputId": "5c83c88d-5491-49d6-b6c7-61072f70435c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'check': 9, 'checked': 2, 'checking': 16}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_to_wordforms['check']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc6TPVF91ZcQ",
    "outputId": "60e9f1e0-b04a-4825-a56e-49fd7c26ba2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Verifying': 1, 'verified': 2, 'verify': 14, 'verifying': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_to_wordforms['verify']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apFM0tdu1a3A",
    "outputId": "218b1e6a-11a7-422d-b2e3-aed2129bf9fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interpret': 9, 'interpreted': 5, 'interpreting': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_to_wordforms['interpret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfX86ubx1cLA",
    "outputId": "679ecf6f-6417-4000-a4a8-5f80d78ba61e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interpretation': 11, 'interpretations': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_to_wordforms['interpretation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uo3qbaaT_9G8",
    "outputId": "2f88c9f8-6f5d-43f5-9721-41ba7dd9a373"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interpretation': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordforms_to_lemmas['interpretations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GovibAS2HcZP",
    "outputId": "da8952b3-62e6-4c8b-8741-3b7f21e6c69d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interpretation': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordforms_to_lemmas['interpretation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttLArjSJA57t"
   },
   "source": [
    "# A modern proposal: Byte Pair Encoding (BPE) subwords\n",
    "\n",
    "We will cover the algorithm for BPE on Friday -- it is the current standard approach related to _representing wordforms_ but not necessarily about _representing morphology_. It produces very different output from the lemmatization presented above, while retaining all aspects of the words' written forms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WkxUxByENFG",
    "outputId": "05758e6f-7c4e-4ec0-9341-45c95ef9a34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@title Download transformers if necessary { vertical-output: true, display-mode: \"form\" }\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXBKxzngGjEk"
   },
   "source": [
    "# ðŸ¤— Transformers by Huggingface\n",
    "\n",
    "Lots of implementations of modern neural network models along with their tokenizers -- which are subword-centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E_fTb9tqHf1z"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "m = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#m = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rA5-m8vbEMKF",
    "outputId": "cf691928-07e0-4141-cc0c-b6e90e9856d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Offensive language detection ( O ##LD ) has received increasing attention '\n",
      " 'due to its societal impact . Recent work shows that bid ##ire ##ctional '\n",
      " 'transform ##er based methods obtain impressive performance on O ##LD . '\n",
      " 'However , such methods usually rely on large - scale well - labeled O ##LD '\n",
      " 'data ##sets for model training . To address the issue of data / label scar '\n",
      " '##city in O ##LD , in this paper , we propose a simple yet effective domain '\n",
      " 'adaptation approach to train bid ##ire ##ctional transform ##ers . Our '\n",
      " 'approach introduces domain adaptation ( D ##A ) training procedures to AL '\n",
      " '##BE ##RT , such that it can effectively exploit auxiliary data from source '\n",
      " 'domains to improve the O ##LD performance in a target domain . Experimental '\n",
      " 'results on bench ##mark data ##sets show that our approach , AL ##BE ##RT ( '\n",
      " 'D ##A ) , obtain ##s the state - of - the - art performance in most cases . '\n",
      " 'Part ##icular ##ly , our approach significantly benefits under ##re ##p '\n",
      " '##res ##ented and under - performing classes , with a significant '\n",
      " 'improvement over AL ##BE ##RT .')\n"
     ]
    }
   ],
   "source": [
    "# compare BERT tokenization to the ones above\n",
    "pprint(' '.join(m.tokenize(abstracts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKnDzBSJOWpS"
   },
   "source": [
    "# Unsupervised morphology induction\n",
    "\n",
    "Algorithms like Morfessor (Creutz & Lagus, 2004) use probabilities to learn what morphemes are in a language. The introduction of the paper is quite good -- read as much of it as you understand. We will talk about unsupervised morphology induction (as a class of problem) on Wednesday, as well as discuss some resources for morphological information (e.g., [Unimorph](https://unimorph.github.io/))."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "introduction_to_morphology.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

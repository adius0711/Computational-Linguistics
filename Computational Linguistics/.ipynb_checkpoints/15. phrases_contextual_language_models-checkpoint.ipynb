{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ihT7HqYLDgJ"
   },
   "source": [
    "# The goals of good language modeling\n",
    "\n",
    "We want to be able to account for the statistical structure of language as much as possible. The better we can build a statistical model, the more easily we can handle:\n",
    "\n",
    "1. Unseen, novel language\n",
    "2. Complex linguistic dependencies\n",
    "3. Extract the important parts out of a text or conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZE9KQV4aZdN"
   },
   "source": [
    "# Challenges for n-gram language models and static embeddings\n",
    "\n",
    "## Long-distance dependencies\n",
    "\n",
    "We need models that can remember words from **far** outside the immediate words to make good predictions about what the next words will be. Consider for example, the following sentences:\n",
    "\n",
    "###Example 1: Coreference resolution\n",
    "1. Last week, I took **my cat** to the vet and the doctor told me **he** was doing very well\n",
    "2. Last week, I took **my cat** to the vet and the doctor told me **my boyfriend** was doing very well\n",
    "\n",
    "A good language model will be able to tell you that (1) is generally a better sentence than (2). But, any model that does not know that the pronoun **he** is a better word to follow **my cat** than the noun phrase **my boyfriend** will get this wrong.\n",
    "\n",
    "This is especially clear in languages with **flexible word order**. For example, languages like Bulgarian, Turkish, classical Latin, Medieval French, etc., all have highly flexible word order. This means that estimating immediate sequences of n-grams is _even harder_ than for languages like English which have relatively **fixed word order**. Even more challenging, languages with flexible word order often have **complex morphology** which increases **sparsity** in co-occurrence data. Effectively:\n",
    "\n",
    "###<center>Our counts become unreliable.</center>\n",
    "\n",
    "###Example 2: Verb-particle constructions\n",
    "* Do you know which doctor I **sent** a letter **to**?\n",
    "* I had a hard time **carrying** the suitcase full of heavy equipment **up** the stairs.\n",
    "\n",
    "These problems are very difficult and finding a good solution has been challenging. For coreference resolution, knowing that **my cat** and **he** are the same referent requires remembering what other things I have mentioned. For verb-particle constructions, I need to be able to learn that **(send, to)** and **(carry, up)** are pairs of words that go together.\n",
    "\n",
    "**Long-distance dependencies** make the probabilities of words very hard to estimate. That is, we cannot usually build very good language models. In order to reliably estimate upcoming words in a language like English, companies like Google would store 6-, 7-, and even 8- or 10-gram sequences of words for applications like **machine translation**.\n",
    "\n",
    "This leads to these hard questions\n",
    "\n",
    "1. How far out do we need to look?\n",
    "2. What length of n-gram do we need to store?\n",
    "\n",
    "## Ambiguity\n",
    "\n",
    "Liz did a great job on Monday discussing how ambiguity plays a role in understanding the meanings conveyed by a sentence. Ambiguity can range from **polysemy** (e.g., the material versus the research meanings of _paper_) to **homonymy** (e.g., _bass_ the fish versus _bass_ the guitar).\n",
    "\n",
    "## Phrases\n",
    "\n",
    "We have previously created **document embeddings** using LSA, `word2vec`, and LDA. But, we do not have a good way to represent combinations of words. Consider, for example, the following types of data:\n",
    "\n",
    "* strong coffee vs. powerful coffee\n",
    "  - \"powerful coffee\" sounds strange to us -- but why? If you were developing a dialogue agent, you would want to make sure it says the more natural phrase in English.\n",
    "* the white house (a place) vs. the White House (the US government entity)\n",
    "  - A good system should know that _The White House_ only has a tiny bit of the same kind of \"white\" in it as _the white house_\n",
    "* Green Bay Packers (football) vs. Green Bay (place)\n",
    "* Xe **is not** in El Paso vs. Xe **is** in El Paso\n",
    "\n",
    "Ultimately, we are faced with not just an issue of **ambiguity** but rather **compositionality**.\n",
    "\n",
    "## How do we compute the meanings of higher-order combinations?\n",
    "\n",
    "For today and Friday, we can think about this in an n-gram or embedding frameworks. But, next week, we will spend more time thinking about meanings in terms of first order logic and sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pFeJ_9cz5RF"
   },
   "source": [
    "## A quick-and-easy way to learn phrase representations\n",
    "\n",
    "N-grams are the easiest way to approximate the meaning of a phrase. We simply make the assumption that two words that co-occur are potentially meaningful, and phrases that co-occur a lot are more meaningful. Importantly, if two words in a phrase almost always occur in the same \n",
    "\n",
    "We can easily extend the bag-of-words model code from before to n-grams of arbitrary sizes, creating a **bag-of-n-grams** representation. We will start with bigrams to be simple, and do stop word removal.\n",
    "\n",
    "In order to create \"n-grams\" in our vocabulary, we just have to treat phrases as unanalyzed strings! Let's try LSA with frequent n-grams and test out how similar phrases are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYc9nnf83zHP",
    "outputId": "22fea292-9c79-4fde-e6b4-42310ac4dcb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4Rqfcb91rOk",
    "outputId": "cd0cc111-f402-4bc6-fa79-67ade729e4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# load in standard stuff\n",
    "from google.colab import files, drive\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD # PCA but for sparse matrices\n",
    "nltk_stops = stopwords.words(\"english\")\n",
    "missing_stops = [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could',\n",
    "                 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would']\n",
    "stop_words = nltk_stops + missing_stops\n",
    "\n",
    "drive.mount(\"/content/drive/\")\n",
    "abstracts = open(\n",
    "    (\"/content/drive/MyDrive/Fall 2021 Computational\"\n",
    "     \" Linguistics Notebooks/files/abstracts.tsv\"), 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_AqyueV5Gz-",
    "outputId": "22f8d502-2da9-490e-868b-afde3acb0bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |  \n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be a sequence of items that\n",
      " |      can be of type string or byte.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer is not callable``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |  \n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  fixed_vocabulary_: boolean\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> print(vectorizer2.get_feature_names())\n",
      " |  ['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |  'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |   'this document', 'this is', 'this the']\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list\n",
      " |          A list of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing, tokenization\n",
      " |      and n-grams generation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : str\n",
      " |          The string to decode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HAj4tg8m3J81"
   },
   "outputs": [],
   "source": [
    "# turn our documents into a bag-of-words plus a bag-of-bigrams\n",
    "vectorizer = CountVectorizer( \n",
    "    tokenizer=word_tokenize,\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 2), # learn bigrams\n",
    "    lowercase=True)\n",
    "# basically just one line to get a giant matrix\n",
    "bow_abstracts = vectorizer.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsMW9WJA8ebs",
    "outputId": "d2eed526-41f7-45b6-8c09-678edf9d2e66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27470, 1100487)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_abstracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ju_MCmR4UN_c",
    "outputId": "9490d796-1a03-45c0-b3b9-00642e7a8088"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do PCA\n",
    "N_COMPONENTS = 100\n",
    "\n",
    "pca = TruncatedSVD(n_components=N_COMPONENTS)\n",
    "pca.fit(bow_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_wG_Lb99cEm",
    "outputId": "ed6f0be7-433e-4be4-e583-dc43e613e539"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3692086 , 0.05771472, 0.01183093, 0.01050857, 0.00931065,\n",
       "       0.00878017, 0.0056165 , 0.00428921, 0.0039652 , 0.0038232 ,\n",
       "       0.0033733 , 0.00294632, 0.00256014, 0.0023847 , 0.00229067,\n",
       "       0.00212326, 0.00202841, 0.0017337 , 0.0016834 , 0.00161826,\n",
       "       0.00156556, 0.00151432, 0.00145235, 0.00144049, 0.00135888,\n",
       "       0.00131564, 0.00129326, 0.00125716, 0.00122282, 0.00119186,\n",
       "       0.00117083, 0.00113469, 0.00111088, 0.00110497, 0.00108503,\n",
       "       0.00106873, 0.00104472, 0.00102723, 0.00100087, 0.00098857,\n",
       "       0.00097882, 0.00097021, 0.00095404, 0.00093225, 0.00092549,\n",
       "       0.00091923, 0.0009044 , 0.00088168, 0.00087176, 0.00086272,\n",
       "       0.00084663, 0.00084001, 0.00083729, 0.00082749, 0.00081828,\n",
       "       0.00081738, 0.00080247, 0.00078626, 0.00077412, 0.00076023,\n",
       "       0.00075079, 0.00074161, 0.00073603, 0.00072274, 0.00071703,\n",
       "       0.0007134 , 0.00070361, 0.00069429, 0.00068583, 0.00067729,\n",
       "       0.00067318, 0.00066818, 0.0006623 , 0.00065274, 0.00064744,\n",
       "       0.00064229, 0.00064026, 0.00063524, 0.00063013, 0.00062049,\n",
       "       0.00061584, 0.00060681, 0.00060335, 0.00058923, 0.00058181,\n",
       "       0.00057323, 0.00056663, 0.00056376, 0.00055546, 0.0005492 ,\n",
       "       0.00054324, 0.00053913, 0.00052996, 0.00052772, 0.0005198 ,\n",
       "       0.00051063, 0.00050326, 0.00049639, 0.00048937, 0.00048417])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVVdM95-3CW5",
    "outputId": "81eb13aa-4368-4f98-fe00-65cd46dc0231"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'. parsing': 0.9314807905080164,\n",
       " '1-endpoint-crossing': 0.9015153297051915,\n",
       " 'algebras .': 0.8931686691990012,\n",
       " 'chart parsing': 0.8928353448486199,\n",
       " 'constituent parsing': 0.9099672574812746,\n",
       " 'dependency parsing': 0.8927422486203865,\n",
       " 'parsing': 0.9302649052178742,\n",
       " 'parsing ,': 0.9139515080136837,\n",
       " 'parsing .': 0.9238950963351666,\n",
       " 'parsing algorithm': 0.9118931403500804,\n",
       " 'parsing algorithms': 1.0,\n",
       " 'parsing schema': 0.9341754124624457,\n",
       " 'parsing schemata': 0.9218372055211215,\n",
       " 'parsing strategies': 0.8932287250109212,\n",
       " 'transition-based': 0.908928470006244}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try again with \"parsing algorithms\"\n",
    "word_vectors = pca.components_.T\n",
    "_index = vectorizer.vocabulary_['parsing algorithms']\n",
    "word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                      word_vectors)\n",
    "_to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                            word_similarities[0].tolist()))\n",
    "dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbPaJ6ic6TQK",
    "outputId": "491f1642-31f4-4a2f-a07b-e34f15d97017"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'( smt': 0.7053785221912773,\n",
       " 'machine': 0.9027890466965998,\n",
       " 'machine translation': 1.0,\n",
       " 'phrase-based machine': 0.7788280894106764,\n",
       " 'smt )': 0.7021311492810304,\n",
       " 'statistical machine': 0.7882546492292113,\n",
       " 'translation .': 0.7467885645523396,\n",
       " 'translation neural': 0.7240753774811233,\n",
       " 'vigor practical': 0.6986410150754385,\n",
       " 'way recognition': 0.6986410150754385,\n",
       " 'white house': 0.6986410150754385,\n",
       " 'win/win': 0.6986410150754385,\n",
       " 'win/win outcomes': 0.6986410150754385,\n",
       " 'witnessed accelerated': 0.6986410150754385,\n",
       " 'years truly': 0.6986410150754385}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_index = vectorizer.vocabulary_['machine translation']\n",
    "word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                      word_vectors)\n",
    "_to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                            word_similarities[0].tolist()))\n",
    "dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--11CBwoUWZS",
    "outputId": "597b5e6d-de1e-447e-f61c-55e45ccdb060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{', competitive': 0.5718417337818319,\n",
       " 'art': 0.8907364912757962,\n",
       " 'art .': 0.6134248243313557,\n",
       " 'art results': 0.5159260987754503,\n",
       " 'backpropagating': 0.5136848164839294,\n",
       " 'competitive state-of-the-art': 0.5108603450616938,\n",
       " 'current state': 0.6263172908546091,\n",
       " 'new state': 0.7099132084691453,\n",
       " 'outperforming': 0.5274225684992738,\n",
       " 'previous state': 0.6363981684187728,\n",
       " 'reduce size': 0.5280202647050907,\n",
       " 'state': 0.6372971352265207,\n",
       " 'state art': 1.0,\n",
       " 'uses large': 0.5061153100760793,\n",
       " 'wmt romanian-english': 0.5008015392156285}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try again with \"state of the art\", removing the stop words\n",
    "_index = vectorizer.vocabulary_['state art']\n",
    "word_similarities = cosine_similarity(word_vectors[_index].reshape(1, -1),\n",
    "                                      word_vectors)\n",
    "_to_similarities = dict(zip(vectorizer.get_feature_names(),\n",
    "                            word_similarities[0].tolist()))\n",
    "dict(sorted(_to_similarities.items(), key=lambda item: item[1])[-15:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoAMkU8pXW1P"
   },
   "source": [
    "## Assessing the \"parsing\" in \"parsing algorithms\"\n",
    "\n",
    "We can use standard vector math -- addition, substraction, cosine similarity -- to understand how different \"parsing algorithms\" is from \"parsing.\" This allows us to know -- somewhat -- how much parsing matters. To do this, we can simply subtract \"algorithms\" from \"parsing algorithms\" and compute the cosine similarity between _that new vector_ and the vector for \"parsing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "pCuogGSVXuYL"
   },
   "outputs": [],
   "source": [
    "# find out which word vector to get\n",
    "parsing_index = vectorizer.vocabulary_['parsing']\n",
    "algorithms_index = vectorizer.vocabulary_['algorithms']\n",
    "parsing_algorithms_index = vectorizer.vocabulary_['parsing algorithms']\n",
    "\n",
    "# store these for easy reuse\n",
    "parsing_vector = word_vectors[parsing_index].reshape(1, -1)\n",
    "algorithms_vector = word_vectors[algorithms_index].reshape(1, -1)\n",
    "parsing_algorithms_vector = word_vectors[parsing_algorithms_index].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWP4wZZxYLH-",
    "outputId": "64b0f478-e42b-493a-c16e-c886b8b56eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25381316]]\n"
     ]
    }
   ],
   "source": [
    "parsing_only = parsing_algorithms_vector - algorithms_vector\n",
    "print(cosine_similarity(parsing_only, parsing_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "G7O-bw_x-9FG"
   },
   "outputs": [],
   "source": [
    "schema_vector = word_vectors[vectorizer.vocabulary_['schema']].reshape(1, -1)\n",
    "parsing_schema = parsing_algorithms_vector - algorithms_vector + schema_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iAQaZ4m_O40",
    "outputId": "a97ee728-6420-4d6e-fc81-da22eeddc36a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20535282]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(parsing_schema,\n",
    "                  word_vectors[vectorizer.vocabulary_['parsing schema']].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQtHzOQ-Y36E"
   },
   "source": [
    "### Vector addition, subtraction not necessarily well-behaved\n",
    "\n",
    "It is not clear that vectors should *add* to form the word vectors for more complex linguistic categories. Linguistic relationships are often _non-additive_ in a statistical sense. That is, in most things about language, a combination represents something distinct from the components. There is, effectively, an **interaction** between representations. Consider, for example, the following:\n",
    "\n",
    "* the doctor 's **office**\n",
    "* brain **doctor**\n",
    "\n",
    "Most of us would intuitively say the word \"doctor\" means something _roughly_ equivalent across all three examples.\n",
    "\n",
    "But, the _core meaning_ of the combination depends on both component words, and often one slightly more than another (the bolded words). But, it is also not clear that the meaning should be a **weighted average** of static word vectors, either.\n",
    "\n",
    "So, we probably want another solution -- for which we can use more sophisticated math. This includes **convolutional** methods, **neural network** representatations, or **learned composition functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8rGKd3SmcSj"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Elman (1990): Finding structure in time. https://doi.org/10.1207/s15516709cog1402_1\n",
    "\n",
    "Recurrent Neural Networks, or RNNs, are like the n-gram language models we learned about before. RNNs are very similar to `word2vec` and n-gram models in that their job is to **predict what the next word is going to be** as well as possible. The particular insight in creating recurrent neural networks was that we can represent the **context** as something like a word vector.\n",
    "\n",
    "The earliest instantiation of Elman's RNN was a **character language model**, or a model that learned to predict the next letter in a sequence. Also known as a **simple recurrent network**, his model had a vocabulary of about 40 characters (26 English letters + 10 numbers). This type of model took weeks to run on machines even though its task would take 30 seconds on your own laptop.\n",
    "\n",
    "But, as we gained computing power, it was feasible to extend his small RNN to complex vocabularies from whole corpora. Starting in the early 2010s, we were able to finally compute sophisticated RNN representations rather than work with toy examples.\n",
    "\n",
    "## Four critical components in RNNs\n",
    "\n",
    "<!-- Here is some python pseudocode where we have a hidden layer (just like in `word2vec`) of $k$ dimensions: -->\n",
    "\n",
    "<!-- ```python\n",
    "for i, word in enumerate(sentence):\n",
    "  one_hot_word_vector = make_bag_of_words([word])\n",
    "  one_hot_next_word_vector = make_bag_of_words([sentence[i + 1])\n",
    "  if i==0:\n",
    "    hidden_units = randomly_initialize(dimensionality=k)\n",
    "    recurrent_units = zero_initialize(dimensionality=k)\n",
    "  else:\n",
    "    recurrent_units = hidden_units # set to previous hidden state\n",
    "    hidden_units = recompute_hidden_from(one_hot_vector)\n",
    "  concatenated_hidden = concat(hidden_units, recurrent_units)\n",
    "  prediction = predict(concatenated_hidden)\n",
    "  error = compute_loss(prediction, one_hot_next_word_vector)\n",
    "``` -->\n",
    "\n",
    "<center><img src=\"https://www.oreilly.com/library/view/keras-2x-projects/9781789536645/assets/8bf6fccb-4bdf-4542-b095-1791a7e2ca88.png\" width=550 /></center>\n",
    "\n",
    "\n",
    "1. Input representations (a one-hot bag-of-words representation of the current word)\n",
    "2. Hidden units (just like in `word2vec`)\n",
    "3. Recurrent units (which hold a \"copy\" of (2) from the previous cycle\n",
    "4. Output representations (a one-hot bag-of-words representation of the next word)\n",
    "\n",
    "On the surface, this is very different from n-gram language modeling. But, the outcome is similar.\n",
    "\n",
    "The major contribution of the RNN is that it predicts the next output (just like an n-gram language model) using a **latent representation** of the context. That is, both the _current word_ and _all the prior words_ contribute to the prediction.\n",
    "\n",
    "Cool historical page on SRNs from Jay McClelland's research group: https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIAhWcFXs8Tj"
   },
   "source": [
    "## Some caveats about Simple Recurrent Networks (SRNs)\n",
    "\n",
    "Elman's original models work well over simplified inputs and outputs, such as small corpora. But, **hidden states can slowly get corrupted**, so the model is not guaranteed to work very well for long sequences. That is, it might forget some of what it has read.\n",
    "\n",
    "So, while SRNs are better than n-gram language models for prediction, they still need more tricks to better remember the past. For that, researchers implemented Long Short-Term Memory ([Hochreiter & Schmidhuber, 1997](https://doi.org/10.1162/neco.1997.9.8.1735)) and Attention ([Bahdanau, Cho, & Bengio, 2014](https://arxiv.org/abs/1409.0473)). These help models to better remember what they have just seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0tD3Usyo0qd"
   },
   "source": [
    "## Word representations to date\n",
    "\n",
    "To date, we have talked about the following algorithms we have covered that learn word vectors are:\n",
    "\n",
    "* Latent Semantic Analysis (LSA) via Principal Component Analysis (PCA)\n",
    "* word2vec (the continuous bag-of-words version)\n",
    "* Latent Dirichlet Allocation (LDA) for topic model representations of words\n",
    "\n",
    "What are the features of each of the dimensions learned by these methods? What is the input and the output?\n",
    "\n",
    "**LSA (Latent Semantic Analysis)**\n",
    "<details>\n",
    "<summary>Each dimension corresponds to </summary>\n",
    "  Its distance along an axis defined by learning the best characterization of the subspace\n",
    "</details>\n",
    "<details>\n",
    "<summary>The input to LSA is </summary>\n",
    "  A set of document representations in bag-of-words format -- a document matrix\n",
    "</details>\n",
    "<details>\n",
    "<summary>The result of LSA is </summary>\n",
    "  A subspace that projects all words $w$ in a vocabulary $V$ into a lower-dimensional space ($|V| \\times k$)\n",
    "</details>\n",
    "\n",
    "**word2vec (continuous bag-of-words)**\n",
    "<details>\n",
    "<summary>Each dimension corresponds to</summary>\n",
    "A compressed representation learned by predicting \"held-out\" words from a context\n",
    "</details>\n",
    "<details>\n",
    "<summary>The input to word2vec is</summary>\n",
    "Document representations presented one at a time until the model converges on a solution. These document representations are like bag-of-words representations, except the model predicts a held-out word that is not included in the counts\n",
    "</details>\n",
    "<details>\n",
    "<summary>The result of word2vec is </summary>\n",
    "  Two matrices that (1) turn one-hot bag-of-words representations into word vectors and (2) turn a context vector [from the document representation] into predictions (probabilities) of held-out words\n",
    "</details>\n",
    "\n",
    "**LDA (Latent Dirichlet Allocation)**\n",
    "Unlike `word2vec` and LSA, LDA does not return a word vector representation exactly. Like the vectors learned by PCA...\n",
    "<details>\n",
    "<summary>Each dimension of a \"word vector\" corresponds to</summary>\n",
    "A \"weight\" in the form of a probability that a given word $w$ belongs to a given topic $t$\n",
    "</details>\n",
    "\n",
    "But, unlike the vectors we learn in LSA:\n",
    "\n",
    "<details>\n",
    "<summary>Each dimension corresponds to</summary>\n",
    "The dimensions are arbitrary -- PCA gives us vectors ordered by their importance. LDA treats each topic separately.\n",
    "</details>\n",
    "\n",
    "And, because the values learned by LDA for a given word are **probabilities** this means some similarity math (e.g., cosine similarity or dot products) are challenging. This motivates contextual methods that can learn word representations that still behave nicely in standard geometric spaces.\n",
    "\n",
    "**RNN (Recurrent Neural Network)**\n",
    "Using a more complex neural structure that holds onto prior states in memory, we can learn contextual word representations.\n",
    "<details>\n",
    "<summary>Each dimension of a \"word vector\" corresponds to</summary>\n",
    "The hidden state (a float vector) prior to the output layer (a vector of probabilities of the next word).\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6i0c4Yxa-Zu"
   },
   "source": [
    "## Preview of Friday!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I36o93Q5KZnV"
   },
   "source": [
    "## ELMo\n",
    "\n",
    "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer\n",
    "\n",
    "https://aclanthology.org/N18-1202.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaZuELwzJQ5d",
    "outputId": "e755edbf-ff8e-47b3-f71a-89758bee2127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simple-elmo\n",
      "  Downloading simple_elmo-0.8.0-py3-none-any.whl (45 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |███████▏                        | 10 kB 21.0 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 20 kB 28.0 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 30 kB 26.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 40 kB 20.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 45 kB 2.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (3.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (1.19.5)\n",
      "Requirement already satisfied: smart-open>1.8.1 in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (5.2.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (1.1.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simple-elmo) (1.4.1)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->simple-elmo) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simple-elmo) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->simple-elmo) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->simple-elmo) (1.15.0)\n",
      "Installing collected packages: simple-elmo\n",
      "Successfully installed simple-elmo-0.8.0\n"
     ]
    }
   ],
   "source": [
    "# doing this instead of trying to use Allen Institute's package\n",
    "!pip install simple-elmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRjDCZ2ALAhG"
   },
   "source": [
    "## BERT\n",
    "\n",
    "https://aclanthology.org/N19-1423.pdf\n",
    "\n",
    "## RoBERTa\n",
    "\n",
    "https://arxiv.org/pdf/1907.11692.pdf\n",
    "\n",
    "## GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbAx3xIuVwJi",
    "outputId": "79091cbf-7c37-4bf0-de95-e3633c3b1957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 8.8 MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 43.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Collecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 23.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 52.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "# using the HuggingFace implementation\n",
    "from transformers import BertModel, RobertaModel, GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmq-xqwxLfkY"
   },
   "source": [
    "# A comparison of contextual representations\n",
    "\n",
    "https://aclanthology.org/D19-1006.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "phrases_contextual_language_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

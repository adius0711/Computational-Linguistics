{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDPbx_Rl7yXI"
   },
   "source": [
    "# The Stanford Natural Language Inference Dataset\n",
    "\n",
    "SNLI attempts to turn the question of asking whether a dachshund is a dog or not into a machine learning task. The goal of SNLI is to be able to handle all kinds of questions like the **hypernym** relationships, but also other types of entailment that encode \"common sense\" knowledge about the world. This dataset has become a staple test dataset in natural language understanding research.\n",
    "\n",
    "This is a three-way classification task. The model is going to use the word embeddings to estimate the probability that each `Hypothesis` either is `neutral` with respect to the original `Text`, or whether there is an `entailment` or `contradiction` relation. If the `Hypothesis` is **entailed** by the `Text`, then this means that the `Hypothesis` MUST be true. If the `Hypothesis` **contradicts** the `Text`, then it must _not_ be the case -- that is, the `Hypothesis` MUST be false.\n",
    "\n",
    "There are approximately 550,000 **training** items, and 10,000 **test** items. We use the test items to evaluate the training items -- we don't want our models to just memorize the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JxAX7WAqDMQ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './snli_1.0.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9414159f363b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./snli_1.0.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './snli_1.0.zip'"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import json\n",
    "\n",
    "data = ZipFile('./snli_1.0.zip')\n",
    "data = {name: data.read(name) for name in data.namelist()}\n",
    "\n",
    "# break training data down into lines\n",
    "training_file_contents = data['snli_1.0/snli_1.0_train.jsonl'].decode('utf-8')\n",
    "\n",
    "# store as jsons\n",
    "training_jsons = []\n",
    "for x in training_file_contents.split(\"\\n\"):\n",
    "  if x!='':\n",
    "    training_jsons.append(json.loads(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe0ZSaUx6iEH"
   },
   "source": [
    "## Getting a model ready for our NLI prediction task\n",
    "\n",
    "Goal: Predict one of three labels in SNLI given a preamble (`Text`) and a sentence that is potentially implied by that text (`Hypothesis`).\n",
    "\n",
    "To do this the modern way, we are going to need to extract word or sentence embedding representations from a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQDSVrZ_Hbg8"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3Npif2zGcAF"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer \n",
    "import numpy as np\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\n",
    "    'distilbert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "608pu38wHaGB"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, neural_model, neural_tokenizer,\n",
    "                           cls_token, layer_number):\n",
    "  tokenized = neural_tokenizer(sentence, return_tensors='pt')\n",
    "  embeddings = neural_model(**tokenized)['hidden_states']\n",
    "  layerwise_embeddings = embeddings[layer_number]\n",
    "  if cls_token:\n",
    "    # then grab very first index\n",
    "    out_embedding = layerwise_embeddings[0].detach().numpy().mean(axis=0).flatten()\n",
    "  else:\n",
    "    # then average all indices\n",
    "    out_embedding = layerwise_embeddings.detach().numpy().mean(axis=1).flatten()\n",
    "  \n",
    "  return out_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWanX7ooPSMv"
   },
   "outputs": [],
   "source": [
    "def process_snli_distilbert(jsons: list, neural_model, neural_tokenizer,\n",
    "                            cls_token=True, layer_number=-1):\n",
    "  \"\"\"\n",
    "  This function will convert all our jsons into a dataset for sklearn\n",
    "  jsons: list[dict] of SNLI data\n",
    "  neural_model: e.g., DistilBertModel from huggingface\n",
    "  neural_tokenizer: e.g., DistilBertTokenizer from huggingface\n",
    "  cls_token: Which word embedding(s) do you want (defaults to CLS/first)\n",
    "  layer_number: What layer do you want (defaults to last)\n",
    "  \"\"\"\n",
    "  Xs, ys = [], []\n",
    "  for record in jsons:\n",
    "    sent1, sent2 = record['sentence1'], record['sentence2']\n",
    "    label = record['gold_label']\n",
    "    s1_embedding = get_sentence_embedding(\n",
    "        sent1, neural_model, neural_tokenizer,\n",
    "        cls_token, layer_number)\n",
    "    s2_embedding = get_sentence_embedding(\n",
    "        sent2, neural_model, neural_tokenizer,\n",
    "        cls_token, layer_number)\n",
    "    # convolution\n",
    "    convolved = np.convolve(s1_embedding, s2_embedding)\n",
    "    # concatenate into one long vector\n",
    "    embedding = np.hstack([s1_embedding, s2_embedding, convolved])\n",
    "    Xs.append(embedding)\n",
    "    ys.append(label)\n",
    "  return Xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TGlUdsrY6DP",
    "outputId": "ef02fda3-635f-47ab-9502-eb5e9326b773"
   },
   "outputs": [],
   "source": [
    "process_snli_distilbert([{\"sentence1\": \"My sentence\", \"sentence2\": \"Another sentence\", \"gold_label\": \"neutral\"}],\n",
    "                        model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvvzPgbEmDUy"
   },
   "source": [
    "### What other representations could we try to use to characterize the relationships between two sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bZmFy8nf5dy"
   },
   "source": [
    "## Building our classifier\n",
    "\n",
    "We are going to create a standard classifier -- the goal of this model is to find the best way to use the word embeddings to carve up the space of categories. When we build a **linear classifier** we are trying to find **lines** that separate our categories (**classes**). The property of our model that we are aiming for is **linear separability**. In the SNLI context, we have 3 classes (entailment, contradiction, neutral), and up to 3071 **slopes** we can learn on our lines. (This is just like $y = mx + b$ in school!)\n",
    "\n",
    "<center><img src=\"https://machinelearningmastery.com/wp-content/uploads/2020/01/Scatter-Plot-of-Multi-Class-Classification-Dataset.png\" width=500 /></center>\n",
    "\n",
    "If we can draw lines that separate the blue blob from the orange and green, and a line that separates the orange from the green, we have a nice lil classifier that knows where to put every new data point. While this plot is in two dimensions, our data are much bigger than that. The math is thankfully still the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YyOa4PYe8RT"
   },
   "outputs": [],
   "source": [
    "# downsample because nobody has time to embed 550,000 sentences lol\n",
    "sub_training_jsons = [x for i, x in enumerate(training_jsons) if i%100==0]\n",
    "\n",
    "# now create our training dataset\n",
    "train_X, train_y = process_snli_distilbert(sub_training_jsons, model, tokenizer,\n",
    "                                           cls_token=True, layer_number=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GnK3SFMfq3_"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(np.vstack(train_X), train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4I5RH_cHjRSL"
   },
   "source": [
    "## Evaluating our classifier\n",
    "\n",
    "In order to know if our models are any good, we first want to see if our model is learning anything from our training data. To do this, we have to compute some kind of accuracy measure. For this, we will need to think about how to count up incorrect guesses.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1024/1*u8PgZ_84no_swpLnuMf-PQ.png\" width=500 /></center>\n",
    "\n",
    "In our **multiclass** classification task for SNLI, we have three labels. But, let's pretend we only have two for now: **entailment** and **contradiction**. Our models will make guesses about **entailment** and **contradiction** -- so we just need to count up all four of these different combinations.\n",
    "\n",
    "| Gold Label      | Predicted==entailment | Predicted==contradiction|\n",
    "| ----------- | ----------- | -----------|\n",
    "| entailment      | Correct       | Incorrect |\n",
    "| contradiction   | Incorrect        | Correct |\n",
    "\n",
    "\n",
    "When we evaluate our models, we are often using the proportions of correct responses relative to different kinds of errors. This is what the two estimates of $Precision$ and $Recall$ are for.\n",
    "\n",
    "<center>\n",
    "$\\text{Precision} = p(\\text{gold label}==\\text{entailment} | \\text{predicted}==\\text{entailment})$\n",
    "</center>\n",
    "\n",
    "> Pokemon analogy: How many of the Pokemon that you caught were Pikachus?\n",
    "\n",
    "<center>\n",
    "$\\text{Recall} = p(\\text{predicted}==\\text{entailment} | \\text{gold label}==\\text{entailment})$\n",
    "</center>\n",
    "\n",
    "> Pokemon analogy: How many of all of the Pikachus in the world did you catch?\n",
    "\n",
    "<center>\n",
    "And then, if we want to combine these two things together, we compute the F score, which allows us to weigh Precision and Recall. The resulting score is a **harmonic mean**. If we weight them equally, then we compute what is called $F_1$. \n",
    "\n",
    "$F_1 = 2 \\cdot \\large \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlPdNQRVm5qe",
    "outputId": "5811e687-4600-45d1-ccdb-75f4689ad918"
   },
   "outputs": [],
   "source": [
    "### Scoring our model on guesses for the training data\n",
    "from sklearn.metrics import f1_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "f1_score(train_y,\n",
    "         classifier.predict(np.vstack(train_X)),\n",
    "         average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iZ2_hGwjrjj"
   },
   "source": [
    "## Micro vs. Macro F1\n",
    "\n",
    "From the scikit-learn documentation:\n",
    "\n",
    "'micro':\n",
    "\n",
    "    Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "'macro':\n",
    "\n",
    "    Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdXiDWWmv0Z_",
    "outputId": "6faf59d6-f508-4d2e-a30e-6d2b6eb57883"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(train_y,\n",
    "         # our predicted training labels\n",
    "         classifier.predict(np.vstack(train_X)),\n",
    "         average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjuQyeviwP34",
    "outputId": "c79333e1-cfd8-4add-c216-1913f956703b"
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "sum(train_y == classifier.predict(np.vstack(train_X))) / len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DcSLcXQJw4R0",
    "outputId": "350d4058-56e4-4e62-ff23-6bdc3c6802fa"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(classifier.predict(np.vstack(train_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jLJhWJfnJ0-"
   },
   "source": [
    "At least our model isn't just predicting one class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRNdjzJSf0Ur"
   },
   "outputs": [],
   "source": [
    "# break test data down into lines\n",
    "test_file_contents = data['snli_1.0/snli_1.0_test.jsonl'].decode('utf-8')\n",
    "\n",
    "# store as jsons\n",
    "test_jsons = []\n",
    "for x in test_file_contents.split(\"\\n\"):\n",
    "  if x!='':\n",
    "    test_jsons.append(json.loads(x))\n",
    "\n",
    "# create our test dataset\n",
    "test_X, test_y = process_snli_distilbert(test_jsons, model, tokenizer, cls_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZyr7mJuzhdf",
    "outputId": "82b9d0ad-03df-42ad-930f-ac566ff50ea3"
   },
   "outputs": [],
   "source": [
    "# assess performance on our test data\n",
    "f1_score(test_y,\n",
    "         classifier.predict(np.vstack(test_X)),\n",
    "         average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6QzEh-8jpXw",
    "outputId": "f7767278-8672-42b9-c524-3bcf34340024"
   },
   "outputs": [],
   "source": [
    "# assess performance on our test data\n",
    "f1_score(test_y,\n",
    "         classifier.predict(np.vstack(test_X)),\n",
    "         average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd_9bnc8nSDx"
   },
   "source": [
    "Yikes! That's not great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFFsM18wzikH",
    "outputId": "e65d298a-ddbc-45ec-9f1a-96cfd4850373"
   },
   "outputs": [],
   "source": [
    "# raw accuracy is just micro f1\n",
    "sum(test_y == classifier.predict(np.vstack(test_X))) / len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "793Qn7b4X-4B"
   },
   "source": [
    "# Semantic role labeling\n",
    "\n",
    "Semantic roles are ways of characterizing the elements that make sentences similar. When we say these two sentences:\n",
    "\n",
    "1. Bollywood Bistro serves vegetarian food\n",
    "2. They serve vegetarian dishes at Bollywood Bistro\n",
    "\n",
    "We know that these two things \"mean\" roughly the same thing. That is, they both express the same underlying truth about the world -- there exists some restaurant Bollywood Bistro, and they serve food, which is appropriate for vegetarians to eat. That is, they have the same **canonical form**. We can express both of these statements with something like these **predicates**:\n",
    "\n",
    "`Serves(BollywoodBistro, VegetarianFood)`. Stated another way:\n",
    "\n",
    "* There exists some food item on Bollywood Bistro's menu that vegetarians can eat\n",
    "\n",
    "If we want to know whether some restaurant ($x$) serves some particular kind of food (FoodType), we can make use of **variables** and create an even more general formula:\n",
    "\n",
    "`Serves(Restaurant(x), FoodType(y))`\n",
    "\n",
    "When we look at the structure of predictates like `Serves` above, we might want to break down the **arguments** of those predicates. Often, we want to **label** those arguments so we can evaluate whether two sentences have the same canonical form.\n",
    "\n",
    "Let's start with a more \"human readable\" framework before we move to a more abstract one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjA0A_-YoEmK"
   },
   "source": [
    "## FrameNet\n",
    "\n",
    "https://framenet.icsi.berkeley.edu/fndrupal/\n",
    "\n",
    "FrameNet is an **annotation schema** that has been used in **semantic role labeling**. \n",
    "\n",
    "Like our `Serves(Restaurant,FoodType)` example before, a restaurant serving food is similar to other types of entities offering services or products, so FrameNet lumps these two things together into an `Offering` frame. That is, similar types of events correspond roughly to the same **semantic frame** (hence the name).\n",
    "\n",
    "All Frames have **seed** sentences or **exemplars** that highlight good examples of the use of a Frame.\n",
    "\n",
    "FrameNet's documentation points out a tendency among data annotation types to either **lump** or **split** categories. An older version of FrameNet contained about 300 frames and under 500 semantic roles. The newer FrameNet lists 877 frames and 1,068 role types. This greatly increases **sparsity** but produces much more semantically informative labels.\n",
    "\n",
    "<center><h4>lumping (few labels; PropBank) <--------------------------------------------> splitting (lots of labels; FrameNet)</h4></center>\n",
    "\n",
    "* The FrameNet entry for \"serve\": https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu17140.xml?mode=lexentry\n",
    "* Let's take a look at the Offering page in FrameNet. https://framenet2.icsi.berkeley.edu/fnReports/data/frameIndex.xml?frame=Offering\n",
    "* For example, one in-depth annotation of the `Offering` frame can be found here: https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu15659.xml?mode=annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "enRHRpXrvJGo",
    "outputId": "931740b4-792d-4b12-ff0d-be7502f16bd4"
   },
   "outputs": [],
   "source": [
    "#@title Example annotation for preference schema\n",
    "from IPython.display import HTML\n",
    "\n",
    "schema='An <font style=\"color: #FFFFFF; background-color: #FF0000;\">Experiencer</font>' \\\n",
    "       ' has a greater desire to participate in some <font style=\"color: #FFFFFF;' \\\n",
    "       ' background-color: #00008B;\">Event</font>, as against another (contextually' \\\n",
    "       ' recoverable) event which exhibits a specific <font style=\"color: #FFFFFF;' \\\n",
    "       ' background-color: #A52A2A;\">Contrast</font> with the <font style=\"color: #FFFFFF;' \\\n",
    "       ' background-color: #00008B;\">Event</font>. Alternatively, the <font style=\"color: #FFFFFF;' \\\n",
    "       ' background-color: #FF0000;\">Experiencer</font> may have a greater desire that some' \\\n",
    "       ' <font style=\"color: #FFFFFF; background-color: #0000FF;\">Focal_participant</font>' \\\n",
    "       ' participate in the <font style=\"color: #FFFFFF; background-color: #00008B;\">Event</font>.' \\\n",
    "       ' The <font style=\"color: #FFFFFF; background-color: #00BFFF;\">Location_of_event</font> may also be mentioned.' \\\n",
    "       '<br><font style=\"color: #FFFFFF; background-color: #9F79EE;\">Why</font> do <font style=\"color: #FFFFFF; background-color: #FF0000;\">women</font> <font style=\"color: rgb(255, 255, 255); background-color: rgb(0, 0, 0); text-transform:uppercase;\">prefer</font> <font style=\"color: #FFFFFF; background-color: #0000FF;\">manly faces</font>?<br>' \\\n",
    "       '<br><font style=\"color: #FFFFFF; background-color: #FF0000;\">I</font> <font style=\"color: rgb(255, 255, 255); background-color: rgb(0, 0, 0); text-transform:uppercase;\">prefer</font> <font style=\"color: #FFFFFF; background-color: #0000FF;\">open source programs</font> <font style=\"color: #FFFFFF; background-color: #A52A2A;\">over proprietary ones</font>.<br>' \\\n",
    "       '<br><font style=\"color: #FFFFFF; background-color: #FF0000;\">Other customers</font> <font style=\"color: rgb(255, 255, 255); background-color: rgb(0, 0, 0); text-transform:uppercase;\">prefer</font> <font style=\"color: #FFFFFF; background-color: #00008B;\">to send us an order together with a cheque</font>.<br>' \\\n",
    "       '<br><font style=\"color: #FFFFFF; background-color: #FF0000;\">I</font> <font style=\"color: rgb(255, 255, 255); background-color: rgb(0, 0, 0); text-transform:uppercase;\">prefer</font> <font style=\"color: #FFFFFF; background-color: #0000FF;\">my tartar sauce</font> <font style=\"color: #FFFFFF; background-color: #00BFFF;\">on fish</font>.<br>'\n",
    "HTML(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRoH4cG-8r2t"
   },
   "source": [
    "## PropBank\n",
    "\n",
    "From Das, Chen, Martins, Schneider, and Smith (2014).\n",
    "> PropBank defines **core roles** ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional)\n",
    "\n",
    "> The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning.\n",
    "\n",
    "The goal of PropBank is to give a sufficiently abstract, dense number of labels. This means that data sparsity will affect learning and/or overfitting.\n",
    "\n",
    "#### Plain sentence:\n",
    "    But I will never learn that.\n",
    "\n",
    "#### Leaves:\n",
    "\n",
    "    0   But\n",
    "    1   I\n",
    "           coref: IDENT        m_0   1-1    I\n",
    "    2   will\n",
    "    3   never\n",
    "    4   learn\n",
    "           sense: learn-v.1\n",
    "           prop:  learn.01\n",
    "            v          * -> 4:0,  learn\n",
    "            ARGM-DIS   * -> 0:0,  But\n",
    "            ARG0       * -> 1:1,  I\n",
    "            ARGM-MOD   * -> 2:0,  will\n",
    "            ARGM-TMP   * -> 3:1,  never\n",
    "            ARG1       * -> 5:1,  that\n",
    "    5   that\n",
    "           coref: IDENT        284   5-5    that\n",
    "    6   /.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsjNjUXID15-"
   },
   "source": [
    "### PropBank predicates are associated with \"frames\" as well\n",
    "\n",
    "But these frames are quite different from the ones in FrameNet. They are numbered instead! Check out the `n` properties of each `<role>` object corresponding to each of the roles in the `prefer` schema or frame from `PropBank` below:\n",
    "\n",
    "```xml\n",
    "<roles>\n",
    "    <role descr=\"chooser, agent\" f=\"\" n=\"0\">\n",
    "        <vnrole vncls=\"31.2\" vntheta=\"experiencer\"/>\n",
    "        <vnrole vncls=\"32.1-1-1\" vntheta=\"experiencer\"/>\n",
    "    </role>\n",
    "    <role descr=\"entity chosen\" f=\"\" n=\"1\">\n",
    "        <vnrole vncls=\"31.2\" vntheta=\"theme\"/>\n",
    "        <vnrole vncls=\"32.1-1-1\" vntheta=\"theme\"/>\n",
    "    </role>\n",
    "    <role descr=\"entity compared to\" f=\"\" n=\"2\"/>\n",
    "    \n",
    "    \n",
    "    <note/>\n",
    "</roles>\n",
    "```\n",
    "\n",
    "Each role in a PropBank frame has a natural language description (e.g., \"entity chosen\") but also some labels from formal linguistics (\"experiencer\", \"theme\"). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcIgR21vrwb1"
   },
   "source": [
    "## Automatic semantic role labeling\n",
    "\n",
    "The task is typically broken down into two phases:\n",
    "\n",
    "1. Identifying which chunks correspond to different aspects of an event schema or frame (e.g., identifying the **span** of the argument) -- **target identification**\n",
    "2. **Frame identification**\n",
    "3. Identifying what role the identified chunk occupies in a given predicate, or **argument identification**\n",
    "\n",
    "Getting anything wrong earlier in these stages will lead to worse performance downstream. You need a good parser (1), a good understanding of semantic or event frames (2), and an ability to map contents onto predicates (3). Typically, the first is done using **shallow parsers**. \n",
    "\n",
    "The second and third points are usually accomplished using **analogy** or **clustering**. Statistical models attempt to assign sentences to a cluster based on how similar that sentence is to the _exemplars_ defined earlier. Once a Frame has been identified, then it is just a matter of finding the arguments.\n",
    "\n",
    "#### Can you think of a reason that we might want to know the arguments of a frame before we know exactly which frame it is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB_0TiwfE20a"
   },
   "source": [
    "# Classifiers for semantic role labeling\n",
    "\n",
    "While it would take too long to run this in class, I'd like to write down our ideas for how to incorporate features such as **propositions** to represent our utterances, so we can learn that two sentences are similar. \n",
    "\n",
    "What tools do we have already for creating **proposition representations** that we can use in a classifier?\n",
    "\n",
    "* \n",
    "* \n",
    "* \n",
    "\n",
    "How might we represent any one of these things mathematically? What is hard about doing this?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classifier_eval_and_semantic_roles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

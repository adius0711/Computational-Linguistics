{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"intro_to_document_embeddings_and_word2vec.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP2U0n4WUCSb7SF7rrRYN9U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qkLy78VTborf"},"source":["# Happy Indigenous People's Day"]},{"cell_type":"markdown","metadata":{"id":"LqO9s8eh4RBY"},"source":["# A return to distributional semantics\n","\n","Today's topics\n","* A brief review of Latent Semantic Analysis\n","* Discussion of document embeddings\n","* word2vec\n","\n","Announcements:\n","* Homework 4 will be released this afternoon. Per the updated syllabus, it will be due October 29, 2021. Even if you do not start the homework yet, open it up and follow along closely with lectures to find what you need to understand.\n","* Homework 3 grades are still in progress. If you asked for an extension, there will be some delays in getting grades back. Expect HW3 grades by the end of the week."]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":77},"id":"r-Imhh6NjrF3","executionInfo":{"status":"ok","timestamp":1633958121368,"user_tz":240,"elapsed":116906,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"9f53a809-3936-438b-ca6d-db3b7a73f164"},"source":["from google.colab import drive, files\n","import pandas as pd\n","\n","abstract_file = files.upload()\n","\n","abstracts = abstract_file['abstracts.tsv'].decode(\"utf-8\").split('\\n')"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-dc2fb92e-021d-4a3c-953d-6490d18e021e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dc2fb92e-021d-4a3c-953d-6490d18e021e\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving abstracts.tsv to abstracts.tsv\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75uRqDQcriG0","executionInfo":{"status":"ok","timestamp":1633958155706,"user_tz":240,"elapsed":33787,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"7650c548-64dd-4fbf-c481-4a8fc2636dc6"},"source":["import nltk\n","nltk.download(\"punkt\")\n","nltk.download('stopwords')\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","nltk_stops = stopwords.words(\"english\")\n","missing_stops = [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would']\n","stop_words = nltk_stops + missing_stops\n","\n","vectorizer = CountVectorizer( # instantiate sparse matrix-creator\n","    tokenizer=word_tokenize, # with our tokenization algorithm\n","    stop_words=stop_words, # typically remove stop words\n","    lowercase=True) # optionally lowercase words\n","# basically just one line to get a giant matrix\n","bow_abstracts = vectorizer.fit_transform(abstracts)\n","\n","bow_abstracts"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["<27471x74170 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 1946891 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"0f4Rvgd2rtZS"},"source":["# Interpreting the word vector dimensions\n","\n","Finally, we can try to interpret the top words for each dimension, let's just try the first 7 dimensions.\n","\n","What we'll find is that some of them are interpretable, and others are less interpretable. For example, the 0th component appears to be frequent non-English words (e.g., \"de\", \"é\" from French) and symbols (\\{, \\}). The fourth component (=3) includes $LaTeX$ formatting symbols and other simple non-alphabetic letters. The third (=2) looks to be academic code words, and so on. \n","\n","The degree to which your space is interpretable depends on a few factors:\n","\n","1. How many vocabulary terms you are using at the beginning and how sparse they are\n","2. How many dimensions you want to learn\n","3. What your learning algorithm is to generate word vectors (e.g., PCA vs. co-occurrence/mutual information vs. word2vec)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWBvJYDIr3kl","executionInfo":{"status":"ok","timestamp":1633958195615,"user_tz":240,"elapsed":5598,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"d9b11ede-a960-4aa9-ce31-2924ec81989c"},"source":["from sklearn.decomposition import TruncatedSVD\n","pca = TruncatedSVD(n_components=25)\n","pca.fit(bow_abstracts)\n","word_vectors = pca.components_.T\n","\n","for dim in range(15):\n","  dim_vecs = word_vectors.T[dim]\n","  dim_vecs_named = dict(zip(vectorizer.get_feature_names(),\n","                            dim_vecs.tolist()))\n","  print(dim)\n","  print('\\t'.join([x[0] for x in sorted(dim_vecs_named.items(), key=lambda item: item[1])[-7:]]))\n","  print(\"-\" * 100)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","'\tde\t.\t,\t\\'e\t{\t}\n","----------------------------------------------------------------------------------------------------\n","1\n","models\tmodel\tlanguage\t(\t)\t.\t,\n","----------------------------------------------------------------------------------------------------\n","2\n","tutorial\tincluding\tal.\te.g.\tet\t;\t,\n","----------------------------------------------------------------------------------------------------\n","3\n","%\t1\t\\\t:\t;\t(\t)\n","----------------------------------------------------------------------------------------------------\n","4\n","116\t105\t110\t97\t111\t101\t32\n","----------------------------------------------------------------------------------------------------\n","5\n","''\t{\t}\t%\tmodels\t\\\tmodel\n","----------------------------------------------------------------------------------------------------\n","6\n","\\\t'\t''\t{\t}\tsystem\tcorpus\n","----------------------------------------------------------------------------------------------------\n","7\n","training\tmachine\tlanguages\tmodels\tdata\ttranslation\tlanguage\n","----------------------------------------------------------------------------------------------------\n","8\n","neural\tnmt\tmt\tsystems\tsystem\tmachine\ttranslation\n","----------------------------------------------------------------------------------------------------\n","9\n","information\tsemantic\tembeddings\twords\ttranslation\tlanguage\tword\n","----------------------------------------------------------------------------------------------------\n","10\n","semantic\tlanguages\tmethod\twords\tembeddings\tdata\tword\n","----------------------------------------------------------------------------------------------------\n","11\n","translation\tinformation\tcorpus\t''\tl\tmodels\t'\n","----------------------------------------------------------------------------------------------------\n","12\n","speech\tannotation\t%\t\\\tdata\tcorpus\tmodel\n","----------------------------------------------------------------------------------------------------\n","13\n","de\tmodel\tword\tdata\tsystem\t'\ttask\n","----------------------------------------------------------------------------------------------------\n","14\n","propose\tlearning\tmethod\tlanguage\tdata\tknowledge\tinformation\n","----------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"XWywdOzwr7na"},"source":["## Obtaining document representations with LSA\n","\n","In general, latent semantic analysis (LSA) is a great place to start to explore your data. You can use LSA word vectors in a wide variety of tasks. \n","\n","But, because of the way PCA works, we can also create a _document_ representation that lives in the same size space. Basically, we do matrix multiplication between our word embeddings (`word_vectors`) and our original bag-of-words matrix (`bow_abstracts`). \n","\n","`bow_abstracts * word_vectors`\n","\n","In this example, we would obtain a lower-dimensional document matrix that is 100 dimensions instead of 80,000."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NNhGcf5r7M4","executionInfo":{"status":"ok","timestamp":1633958398033,"user_tz":240,"elapsed":204,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"d667c426-ea7f-4f95-b6f2-df368547df24"},"source":["document_embeddings = pca.transform(bow_abstracts)\n","print(document_embeddings)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 4.13815811 11.11587934 -0.30377429 ...  0.44884097 -0.2514207\n","  -0.80736889]\n"," [ 3.21875736  8.60545633  1.73131588 ... -0.51021759 -0.06641183\n","   0.28601048]\n"," [ 3.10500404  8.4931142   0.53965843 ... -0.69344295  0.38111936\n","  -0.1489152 ]\n"," ...\n"," [30.48578383  1.25238543  4.7919043  ...  3.6036478   5.45698146\n","  -0.33974106]\n"," [ 7.30122149  1.31503462  1.47304996 ...  0.91350222  1.18078048\n","   0.15332986]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkWQunhh5F_T","executionInfo":{"status":"ok","timestamp":1633958432914,"user_tz":240,"elapsed":196,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"215c87f7-93b5-41ad-a23d-c465dd851df0"},"source":["print(document_embeddings.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(27471, 25)\n"]}]},{"cell_type":"markdown","metadata":{"id":"v97WWTrssCxs"},"source":["Document embeddings are a multiplication problem!\n","\n","If we have a dataset with 10 documents $D$ and a vocabulary $V$ of 100,000 words, then we have a $[10 x 100000]$ matrix for our dataset. To turn 100,000 words into 100 dimensions, we need a projection matrix $P$ with dimensionality $[100000, 100]$. We then multiply $D x P$ to get a matrix of $[10 x 100]$."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WE1t78xasCBS","executionInfo":{"status":"ok","timestamp":1633958489895,"user_tz":240,"elapsed":186,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"f00fc0b1-c39c-45d7-be9c-ee00793db779"},"source":["# getting document embeddings is a multiplication problem\n","a = bow_abstracts * word_vectors\n","# test for equivalence in methods\n","a==document_embeddings"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ True,  True,  True, ...,  True,  True,  True],\n","       [ True,  True,  True, ...,  True,  True,  True],\n","       [ True,  True,  True, ...,  True,  True,  True],\n","       ...,\n","       [ True,  True,  True, ...,  True,  True,  True],\n","       [ True,  True,  True, ...,  True,  True,  True],\n","       [ True,  True,  True, ...,  True,  True,  True]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"OXVeeF8wzJsf","executionInfo":{"status":"ok","timestamp":1633958507837,"user_tz":240,"elapsed":167,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}}},"source":["document_embeddings_df = pd.DataFrame(document_embeddings)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lGFQ27CvQyu"},"source":["# Exploring the document embeddings\n","\n","### Sort by each dimension to find the \"top\" match along that dimension.\n","\n","This document scores the highest on the 0th/first dimension because it is incredibly French."]},{"cell_type":"markdown","metadata":{"id":"aZV2_TIpvZhQ"},"source":["What is the most French document?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"zQ0lcXKzvQY2","executionInfo":{"status":"ok","timestamp":1633958566100,"user_tz":240,"elapsed":201,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"4267c14c-eb20-43c4-9d99-b53c1903b43c"},"source":["french_topic_id: int = 0\n","abstracts[document_embeddings_df.sort_values(french_topic_id, ascending=False).iloc[0].name]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"La quasi-totalit{\\\\'e} des {\\\\'e}tiqueteurs grammaticaux mettent en oeuvre des r{\\\\`e}gles qui portent sur les successions ou collocations permises de deux ou trois cat{\\\\'e}gories grammaticales. Leurs performances s{'}{\\\\'e}tablissent {\\\\`a} hauteur de 96{\\\\%} de mots correctement {\\\\'e}tiquet{\\\\'e}s, et {\\\\`a} moins de 57{\\\\%} de phrases correctement {\\\\'e}tiquet{\\\\'e}es. Ces r{\\\\`e}gles binaires et ternaires ne repr{\\\\'e}sentent qu{'}une fraction du total des r{\\\\`e}gles de succession que l{'}on peut extraire {\\\\`a} partir des phrases d{'}un corpus d{'}apprentissage, alors m{\\\\^e}me que la majeure partie des phrases (plus de 98{\\\\%} d{'}entre elles) ont une taille sup{\\\\'e}rieure {\\\\`a} 3 mots. Cela signifie que la plupart des phrases sont analys{\\\\'e}es au moyen de r{\\\\`e}gles reconstitu{\\\\'e}es ou simul{\\\\'e}es {\\\\`a} partir de r{\\\\`e}gles plus courtes, ternaires en l{'}occurrence dans le meilleur des cas. Nous montrons que ces r{\\\\`e}gles simul{\\\\'e}es sont majoritairement agrammaticales, et que l{'}avantage inf{\\\\'e}rentiel qu{'}apporte le cha{\\\\^\\\\i}nage de r{\\\\`e}gles courtes pour parer au manque d{'}apprentissage, plus marqu{\\\\'e} pour les r{\\\\`e}gles plus longues, est largement neutralis{\\\\'e} par la permissivit{\\\\'e} de ce processus dont toutes sortes de poids, scores ou probabilit{\\\\'e}s ne r{\\\\'e}ussissent pas {\\\\`a} en hi{\\\\'e}rarchiser la production afin d{'}y distinguer le grammatical de l{'}agrammatical. Force est donc de reconsid{\\\\'e}rer les r{\\\\`e}gles de taille sup{\\\\'e}rieure {\\\\`a} 3, lesquelles, il y a une trentaine d{'}ann{\\\\'e}es, avaient {\\\\'e}t{\\\\'e} d{'}embl{\\\\'e}e {\\\\'e}cart{\\\\'e}es pour des raisons essentiellement li{\\\\'e}es {\\\\`a} la puissance des machines d{'}alors, et {\\\\`a} l{'}insuffisance des corpus d{'}apprentissage. Mais si l{'}on admet qu{'}il faille d{\\\\'e}sormais {\\\\'e}tendre la taille des r{\\\\`e}gles de succession, la question se pose de savoir jusqu{'}{\\\\`a} quelle limite, et pour quel b{\\\\'e}n{\\\\'e}fice. Car l{'}on ne saurait non plus plaider pour une port{\\\\'e}e des r{\\\\`e}gles aussi longue que les plus longues phrases auxquelles elles sont susceptibles d{'}{\\\\^e}tre appliqu{\\\\'e}es. Autrement dit, y a-t-il une taille optimale des r{\\\\`e}gles qui soit suffisamment petite pour que leur apprentissage puisse converger, mais suffisamment longue pour que tout cha{\\\\^\\\\i}nage de telles r{\\\\`e}gles pour embrasser les phrases de taille sup{\\\\'e}rieure soit grammatical. La cons{\\\\'e}quence heureuse {\\\\'e}tant que poids, scores et probabilit{\\\\'e}s ne seraient plus invoqu{\\\\'e}s que pour choisir entre successions d{'}{\\\\'e}tiquettes toutes {\\\\'e}galement grammaticales, et non pour {\\\\'e}liminer en outre les successions agrammaticales. Cette taille semble exister. Nous montrons qu{'}au moyen d{'}algorithmes relativement simples l{'}on peut assez pr{\\\\'e}cis{\\\\'e}ment la d{\\\\'e}terminer. Qu{'}elle se situe, compte tenu de nos corpus, aux alentours de 12 pour le fran{\\\\c{c}}ais, de 10 pour l{'}arabe, et de 10 pour l{'}anglais. Qu{'}elle est donc en particulier inf{\\\\'e}rieure {\\\\`a} la taille moyenne des phrases, quelle que soit la langue consid{\\\\'e}r{\\\\'e}e.\""]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"XFWUlhFBvXth","executionInfo":{"status":"ok","timestamp":1633958590837,"user_tz":240,"elapsed":348,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"c6131db8-9c78-4941-b833-508f49d536d0"},"source":["machine_translation_id: int = 8\n","abstracts[document_embeddings_df.sort_values(machine_translation_id, ascending=False).iloc[0].name]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"We wrote this report in Japanese and translated it by NEC's machine translation system PIVOT/JE.) IBS (International Business Service) is the company which does the documentation service which contains translation business. We introduced a machine translation system into translation business in earnest last year. The introduction of a machine translation system changed the form of our translation work. The translation work was divided into some steps and the person who isn't experienced became able to take it of the work of each of translation steps. As a result, a total translation cost reduced. In this paper, first, we report on the usage of our machine translation system. Next, we report on translation quality and the translation cost with a machine translation system. Lastly, we report on the merit which was gotten by introducing machine translation.\""]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"wSk3atkXvyi1"},"source":["What is the least French abstract?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"NtIzbBECv1Ha","executionInfo":{"status":"ok","timestamp":1633958621502,"user_tz":240,"elapsed":158,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"aae1eb76-a8c5-4198-c486-895299e0e8db"},"source":["french_topic_id: int = 0\n","abstracts[document_embeddings_df.sort_values(french_topic_id, ascending=True).iloc[0].name]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["''"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"CIx-0r6Mv3UT"},"source":["Oh. (Well that explains it. A matrix of 0s will give you 0s everywhere.)"]},{"cell_type":"markdown","metadata":{"id":"dY0QwCKBzUHd"},"source":["# Extending the distributional hypothesis further: `word2vec`\n","\n","The **distributional hypothesis** that has driven our analyses so far says that we can use context to learn the meanings of words. PCA learns embeddings from bag-of-word representations by trying to find the $k$ most informative orthogonal dimensions in some lower-dimensional space. It tries to carve up increasingly small subsets of the data.\n","\n","`word2vec` makes the idea that context can _teach_ us an explicit algorithmic assumption. For this, the model leverages a simple **neural network** architecture. \n","\n","## What is a neural network?\n","\n","A neural network is a computational model that connects different nodes to other nodes in the form of connections, weights, or edges. Some of the simplest neural networks are no different from a linear regression:\n","\n","![400px-Linear_regression.svg.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"GMUqjah-59bA"},"source":["Linear models can be written in the general form from high school algebra: \n","\n","$y = mx + b$\n","\n","in which $y$ represents some outcome variable (e.g., ice cream sales), $x$ represents some input variable (e.g., temperature), $m$ represents a transformation of that input, which is the _slope_ of the line. $b$ represents the intercept of that line. In practical terms, we would describe the slope of line ($m$) that says how much more (or less) ice cream is bought than average ($b$) depending on what temperature it is outside."]},{"cell_type":"markdown","metadata":{"id":"L5nrGxb76pFa"},"source":["## Neural networks are just fancy linear models\n","\n","$y$ in the above example is usually an outcome variable that we know. When we make linear models, we are doing **supervised learning**. That is, we have a known **outcome** ($y$) that we want to **predict** from some set of variables ($X$ to denote several $x$'s), so we need to learn slopes for every $x \\in X$. \n","\n","Neural networks contain additional architecture that can make more sophisticated computations. Neural networks often have **hidden layers**, which allow us to capture complex relationships between an outcome ($y$) and the input ($X$). When we have a relationship that is not captured by a simple slope, we want to learn **nonlinear** $m$s. \n","\n","![Gaussian_kernel_regression.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"GenjpNJI7-UK"},"source":["## `word2vec` uses a simple neural network architecture\n","\n","`word2vec` is a three-layer neural network composed of two matrices stacked on top of each other. Like `LSA`, `word2vec` is basically matrix multiplication. When we obtained document embeddings from our trained PCA model for LSA, we multiplied the original bag-of-words representation with the projection matrix of word vector representations (`bow_abstracts * word_vectors`). We basically do the same for `word2vec`.\n","\n","<img src=\"https://i.stack.imgur.com/sAvR9.png\" width=700/>\n","\n","Both of the matrices of CBOW can be thought of as \"projection\" matrices. The first one is of size $|V| \\times k$ and the second one is of size $k \\times |V|$. Like our PCA example, `word2vec` also does **dimensionality reduction** so $k$ corresponds to whatever size we want our **hidden units** to be. This is just like when we pick `n_components_` for PCA. Each element in the two matrices can be thought of as a **weight** so when we multiply a bag-of-words representation (dimensionality $1 \\times |V|$) by the first of these matrices ($|V| \\times k$) by we get a $k$-dimensional vector back (the $|V|$s cancel out).\n","\n","The very first matrix (of size $|V| \\times k$) is usually what people refer to when they mean the word embeddings that `word2vec` makes.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QeeFzbrbEShF"},"source":["## Prediction for word2vec to train word embeddings\n","\n","The input and the desired output of a `word2vec` model are both the bag-of-words representation. `word2vec2` learns from data, one vector at a time. At each step, a `word2vec` model will get a single bag-of-words representation and be asked to **predict**, just like a linear model. \n","\n","`word2vec` learns by **prediction error**. The job of CBOW is to predict a single held-out word from all of the surrounding words **in a sliding window**. Consider the sentence, \"My cat Vector is very old.\", which we can tokenize as `[\"My\", \"cat\", \"Vector\", \"is\", \"very\", \"old\", \".\"]`.\n","\n","| Input      | Desired output |\n","| ----------- | ----------- |\n","| My, cat, is, very, old, .      | Vector       |\n","| My, cat, Vector, very, old, .   | is        |\n","| ...   | ...        |\n","| My, cat, Vector, is, very, old   | .        |\n","\n","Each word gets predicted by words that surround it. This is the **contextual** part of the `word2vec` model. `word2vec` learns that two words like \"dog\" and \"cat\" are similar when they typically have the same **context words** predicting them.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTzE5iork1NX","executionInfo":{"status":"ok","timestamp":1633959848911,"user_tz":240,"elapsed":172,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"62a594ad-dc29-4f6d-c5b7-b4e31d5c7ac8"},"source":["sentence = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n","\n","sliding_window = 2\n","for i, w in enumerate(sentence):\n","  left_context = sentence[i - sliding_window:i]\n","  right_context = sentence[i + 1 :i + sliding_window + 1]\n","  print(w, left_context, right_context)\n","  context = left_context + right_context"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["a [] ['b', 'c']\n","b [] ['c', 'd']\n","c ['a', 'b'] ['d', 'e']\n","d ['b', 'c'] ['e']\n","e ['c', 'd'] []\n"]}]},{"cell_type":"markdown","metadata":{"id":"CtCffJnOBzQp"},"source":["## Steps of word2vec (CBOW)\n","\n","1. Determine how big the window should be\n","  * Three back, three forward?\n","  * A whole sentence?\n","  * Symmetrical or asymmetrical windows?\n","2. Transform the context into a bag-of-words representation by extracting those words in the window\n","3. Transform the target word into a bag-of-words representation (=exactly 1 dimension will have a 1, all else zeroes)\n","3. Give the bag-of-words representation to CBOW\n","  * Average the current word embeddings for the bag-of-words in the input\n","4. Multiply $k$-dimensional vector by projection matrix (of size $k \\times |V|$)\n","5. Transform output using [**softmax** function](https://en.wikipedia.org/wiki/Softmax_function). Produces a vector of probabilities.\n","6. Compute the **error** between the output (a vector of probabilities) and the actual expectation (in 3)\n","7. **Backpropagate** the error. This adjusts the matrices to be better at prediction."]},{"cell_type":"markdown","metadata":{"id":"0uK7SsOGntzQ"},"source":["# Syntagmatic similarity\n","\n","* Co-occur = Similar\n","* Cat, pet, chase\n","* Common from LSA\n","\n","# Paradigmatic similarity\n","\n","* Co-occur with the same things = Similar\n","* Dog, cat, mouse, hamsters\n","* Common from word2vec"]},{"cell_type":"markdown","metadata":{"id":"8XC8NZw6ByRi"},"source":["Note: There are technically two `word2vec` models -- the \"**Continuous Bag-of-Words**\" (CBOW) model and \"**skip-gram**.\" Skip-gram is significantly more complex than CBOW so we will not cover this here. But, there is a good discussion of it in the SLP3 book! I will refer to CBOW as `word2vec` here. Always be sure to check which algorithm the authors mean in any papers you read."]},{"cell_type":"code","metadata":{"id":"Up3W5UIgotDM","executionInfo":{"status":"ok","timestamp":1633960562519,"user_tz":240,"elapsed":29320,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}}},"source":["tokenized_abstracts = [word_tokenize(x) for x in abstracts]"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRvL9Xzpo1J2","executionInfo":{"status":"ok","timestamp":1633960563069,"user_tz":240,"elapsed":16,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"561d08a6-e776-4895-fa2d-6370fe07d416"},"source":["help(Word2Vec)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class Word2Vec in module gensim.models.word2vec:\n","\n","class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n"," |  Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n"," |  \n"," |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n"," |  \n"," |  Once you're finished training a model (=no more updates, only querying)\n"," |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n"," |  \n"," |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n"," |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n"," |  \n"," |  The trained word vectors can also be stored/loaded from a format compatible with the\n"," |  original word2vec implementation via `self.wv.save_word2vec_format`\n"," |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n"," |  \n"," |  Some important attributes are the following:\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n"," |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n"," |      directly to query those embeddings in various ways. See the module level docstring for examples.\n"," |  \n"," |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n"," |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n"," |      Besides keeping track of all unique words, this object provides extra functionality, such as\n"," |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n"," |  \n"," |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n"," |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n"," |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n"," |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n"," |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n"," |  \n"," |  Method resolution order:\n"," |      Word2Vec\n"," |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n"," |      gensim.models.base_any2vec.BaseAny2VecModel\n"," |      gensim.utils.SaveLoad\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __contains__(self, word)\n"," |      Deprecated. Use `self.wv.__contains__` instead.\n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n"," |  \n"," |  __getitem__(self, words)\n"," |      Deprecated. Use `self.wv.__getitem__` instead.\n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n"," |  \n"," |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n"," |      Parameters\n"," |      ----------\n"," |      sentences : iterable of iterables, optional\n"," |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n"," |          See also the `tutorial on data streaming in Python\n"," |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n"," |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n"," |          in some other way.\n"," |      corpus_file : str, optional\n"," |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n"," |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n"," |          `corpus_file` arguments need to be passed (or none of them).\n"," |      size : int, optional\n"," |          Dimensionality of the word vectors.\n"," |      window : int, optional\n"," |          Maximum distance between the current and predicted word within a sentence.\n"," |      min_count : int, optional\n"," |          Ignores all words with total frequency lower than this.\n"," |      workers : int, optional\n"," |          Use these many worker threads to train the model (=faster training with multicore machines).\n"," |      sg : {0, 1}, optional\n"," |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n"," |      hs : {0, 1}, optional\n"," |          If 1, hierarchical softmax will be used for model training.\n"," |          If 0, and `negative` is non-zero, negative sampling will be used.\n"," |      negative : int, optional\n"," |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n"," |          should be drawn (usually between 5-20).\n"," |          If set to 0, no negative sampling is used.\n"," |      ns_exponent : float, optional\n"," |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n"," |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n"," |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n"," |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n"," |          other values may perform better for recommendation applications.\n"," |      cbow_mean : {0, 1}, optional\n"," |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n"," |      alpha : float, optional\n"," |          The initial learning rate.\n"," |      min_alpha : float, optional\n"," |          Learning rate will linearly drop to `min_alpha` as training progresses.\n"," |      seed : int, optional\n"," |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n"," |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n"," |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n"," |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n"," |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n"," |      max_vocab_size : int, optional\n"," |          Limits the RAM during vocabulary building; if there are more unique\n"," |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n"," |          Set to `None` for no limit.\n"," |      max_final_vocab : int, optional\n"," |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n"," |          min_count is more than the calculated min_count, the specified min_count will be used.\n"," |          Set to `None` if not required.\n"," |      sample : float, optional\n"," |          The threshold for configuring which higher-frequency words are randomly downsampled,\n"," |          useful range is (0, 1e-5).\n"," |      hashfxn : function, optional\n"," |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n"," |      iter : int, optional\n"," |          Number of iterations (epochs) over the corpus.\n"," |      trim_rule : function, optional\n"," |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n"," |          be trimmed away, or handled using the default (discard if word count < min_count).\n"," |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n"," |          or a callable that accepts parameters (word, count, min_count) and returns either\n"," |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n"," |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n"," |          model.\n"," |      \n"," |          The input parameters are of the following types:\n"," |              * `word` (str) - the word we are examining\n"," |              * `count` (int) - the word's frequency count in the corpus\n"," |              * `min_count` (int) - the minimum count threshold.\n"," |      sorted_vocab : {0, 1}, optional\n"," |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n"," |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n"," |      batch_words : int, optional\n"," |          Target size (in words) for batches of examples passed to worker threads (and\n"," |          thus cython routines).(Larger batches will be passed if individual\n"," |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n"," |      compute_loss: bool, optional\n"," |          If True, computes and stores loss value which can be retrieved using\n"," |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n"," |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n"," |          Sequence of callbacks to be executed at specific stages during training.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n"," |      \n"," |      >>> from gensim.models import Word2Vec\n"," |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n"," |      >>> model = Word2Vec(sentences, min_count=1)\n"," |  \n"," |  __str__(self)\n"," |      Human readable representation of the model's state.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      str\n"," |          Human readable representation of the model's state, including the vocabulary size, vector size\n"," |          and learning rate.\n"," |  \n"," |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n"," |      Deprecated. Use `self.wv.accuracy` instead.\n"," |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n"," |  \n"," |  clear_sims(self)\n"," |      Remove all L2-normalized word vectors from the model, to free up memory.\n"," |      \n"," |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n"," |  \n"," |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n"," |      Discard parameters that are used in training and scoring, to save memory.\n"," |      \n"," |      Warnings\n"," |      --------\n"," |      Use only if you're sure you're done training a model.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      replace_word_vectors_with_normalized : bool, optional\n"," |          If True, forget the original (not normalized) word vectors and only keep\n"," |          the L2-normalized word vectors, to save even more memory.\n"," |  \n"," |  get_latest_training_loss(self)\n"," |      Get current value of the training loss.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      float\n"," |          Current training loss.\n"," |  \n"," |  init_sims(self, replace=False)\n"," |      Deprecated. Use `self.wv.init_sims` instead.\n"," |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n"," |  \n"," |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n"," |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n"," |      where it intersects with the current vocabulary.\n"," |      \n"," |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n"," |      non-intersecting words are left alone.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname : str\n"," |          The file path to load the vectors from.\n"," |      lockf : float, optional\n"," |          Lock-factor value to be set for any imported word-vectors; the\n"," |          default value of 0.0 prevents further updating of the vector during subsequent\n"," |          training. Use 1.0 to allow further training updates of merged vectors.\n"," |      binary : bool, optional\n"," |          If True, `fname` is in the binary word2vec C format.\n"," |      encoding : str, optional\n"," |          Encoding of `text` for `unicode` function (python2 only).\n"," |      unicode_errors : str, optional\n"," |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n"," |  \n"," |  predict_output_word(self, context_words_list, topn=10)\n"," |      Get the probability distribution of the center word given context words.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      context_words_list : list of str\n"," |          List of context words.\n"," |      topn : int, optional\n"," |          Return `topn` words and their probabilities.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      list of (str, float)\n"," |          `topn` length list of tuples of (word, probability).\n"," |  \n"," |  reset_from(self, other_model)\n"," |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n"," |      \n"," |      Structures copied are:\n"," |          * Vocabulary\n"," |          * Index to word mapping\n"," |          * Cumulative frequency table (used for negative sampling)\n"," |          * Cached corpus length\n"," |      \n"," |      Useful when testing multiple models on the same corpus in parallel.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n"," |          Another model to copy the internal structures from.\n"," |  \n"," |  save(self, *args, **kwargs)\n"," |      Save the model.\n"," |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n"," |      online training and getting vectors for vocabulary words.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname : str\n"," |          Path to the file.\n"," |  \n"," |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n"," |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n"," |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n"," |  \n"," |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n"," |      Score the log probability for a sequence of sentences.\n"," |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n"," |      \n"," |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n"," |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n"," |      \n"," |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n"," |      score more than this number of sentences but it is inefficient to set the value too high.\n"," |      \n"," |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n"," |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n"," |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n"," |      how to use such scores in document classification.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sentences : iterable of list of str\n"," |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n"," |      total_sentences : int, optional\n"," |          Count of sentences.\n"," |      chunksize : int, optional\n"," |          Chunksize of jobs\n"," |      queue_factor : int, optional\n"," |          Multiplier for size of queue (number of workers * queue_factor).\n"," |      report_delay : float, optional\n"," |          Seconds to wait before reporting progress.\n"," |  \n"," |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n"," |      Update the model's neural weights from a sequence of sentences.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n"," |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n"," |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n"," |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n"," |      you can simply use `total_examples=self.corpus_count`.\n"," |      \n"," |      Warnings\n"," |      --------\n"," |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n"," |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n"," |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sentences : iterable of list of str\n"," |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n"," |          See also the `tutorial on data streaming in Python\n"," |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n"," |      corpus_file : str, optional\n"," |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n"," |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n"," |          `corpus_file` arguments need to be passed (not both of them).\n"," |      total_examples : int\n"," |          Count of sentences.\n"," |      total_words : int\n"," |          Count of raw words in sentences.\n"," |      epochs : int\n"," |          Number of iterations (epochs) over the corpus.\n"," |      start_alpha : float, optional\n"," |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n"," |          for this one call to`train()`.\n"," |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n"," |          (not recommended).\n"," |      end_alpha : float, optional\n"," |          Final learning rate. Drops linearly from `start_alpha`.\n"," |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n"," |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n"," |          (not recommended).\n"," |      word_count : int, optional\n"," |          Count of words already trained. Set this to 0 for the usual\n"," |          case of training on all words in sentences.\n"," |      queue_factor : int, optional\n"," |          Multiplier for size of queue (number of workers * queue_factor).\n"," |      report_delay : float, optional\n"," |          Seconds to wait before reporting progress.\n"," |      compute_loss: bool, optional\n"," |          If True, computes and stores loss value which can be retrieved using\n"," |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n"," |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n"," |          Sequence of callbacks to be executed at specific stages during training.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> from gensim.models import Word2Vec\n"," |      >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n"," |      >>>\n"," |      >>> model = Word2Vec(min_count=1)\n"," |      >>> model.build_vocab(sentences)  # prepare the model vocabulary\n"," |      >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n"," |      (1, 30)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods defined here:\n"," |  \n"," |  load(*args, **kwargs) from builtins.type\n"," |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n"," |      \n"," |      See Also\n"," |      --------\n"," |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n"," |          Save model.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname : str\n"," |          Path to the saved file.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      :class:`~gensim.models.word2vec.Word2Vec`\n"," |          Loaded model.\n"," |  \n"," |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n"," |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods defined here:\n"," |  \n"," |  log_accuracy(section)\n"," |      Deprecated. Use `self.wv.log_accuracy` instead.\n"," |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n"," |  \n"," |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n"," |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sentences : iterable of list of str\n"," |          Can be simply a list of lists of tokens, but for larger corpora,\n"," |          consider an iterable that streams the sentences directly from disk/network.\n"," |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n"," |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n"," |      corpus_file : str, optional\n"," |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n"," |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n"," |          `corpus_file` arguments need to be passed (not both of them).\n"," |      update : bool\n"," |          If true, the new words in `sentences` will be added to model's vocab.\n"," |      progress_per : int, optional\n"," |          Indicates how many words to process before showing/updating the progress.\n"," |      keep_raw_vocab : bool, optional\n"," |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n"," |      trim_rule : function, optional\n"," |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n"," |          be trimmed away, or handled using the default (discard if word count < min_count).\n"," |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n"," |          or a callable that accepts parameters (word, count, min_count) and returns either\n"," |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n"," |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n"," |          of the model.\n"," |      \n"," |          The input parameters are of the following types:\n"," |              * `word` (str) - the word we are examining\n"," |              * `count` (int) - the word's frequency count in the corpus\n"," |              * `min_count` (int) - the minimum count threshold.\n"," |      \n"," |      **kwargs : object\n"," |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n"," |  \n"," |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n"," |      Build vocabulary from a dictionary of word frequencies.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      word_freq : dict of (str, int)\n"," |          A mapping from a word in the vocabulary to its frequency count.\n"," |      keep_raw_vocab : bool, optional\n"," |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n"," |      corpus_count : int, optional\n"," |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n"," |      trim_rule : function, optional\n"," |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n"," |          be trimmed away, or handled using the default (discard if word count < min_count).\n"," |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n"," |          or a callable that accepts parameters (word, count, min_count) and returns either\n"," |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n"," |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n"," |          of the model.\n"," |      \n"," |          The input parameters are of the following types:\n"," |              * `word` (str) - the word we are examining\n"," |              * `count` (int) - the word's frequency count in the corpus\n"," |              * `min_count` (int) - the minimum count threshold.\n"," |      \n"," |      update : bool, optional\n"," |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n"," |  \n"," |  doesnt_match(self, words)\n"," |      Deprecated, use self.wv.doesnt_match() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n"," |  \n"," |  estimate_memory(self, vocab_size=None, report=None)\n"," |      Estimate required memory for a model using current settings and provided vocabulary size.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      vocab_size : int, optional\n"," |          Number of unique tokens in the vocabulary\n"," |      report : dict of (str, int), optional\n"," |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      dict of (str, int)\n"," |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n"," |  \n"," |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n"," |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n"," |      \n"," |      Refer to the documentation for\n"," |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n"," |  \n"," |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n"," |      Deprecated, use self.wv.most_similar() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n"," |  \n"," |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n"," |      Deprecated, use self.wv.most_similar_cosmul() instead.\n"," |      \n"," |      Refer to the documentation for\n"," |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n"," |  \n"," |  n_similarity(self, ws1, ws2)\n"," |      Deprecated, use self.wv.n_similarity() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n"," |  \n"," |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n"," |      Deprecated, use self.wv.similar_by_vector() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n"," |  \n"," |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n"," |      Deprecated, use self.wv.similar_by_word() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n"," |  \n"," |  similarity(self, w1, w2)\n"," |      Deprecated, use self.wv.similarity() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n"," |  \n"," |  wmdistance(self, document1, document2)\n"," |      Deprecated, use self.wv.wmdistance() instead.\n"," |      \n"," |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n"," |  \n"," |  cum_table\n"," |  \n"," |  hashfxn\n"," |  \n"," |  iter\n"," |  \n"," |  layer1_size\n"," |  \n"," |  min_count\n"," |  \n"," |  sample\n"," |  \n"," |  syn0_lockf\n"," |  \n"," |  syn1\n"," |  \n"," |  syn1neg\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from gensim.utils.SaveLoad:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"4Mi8aAGzoHbP","executionInfo":{"status":"ok","timestamp":1633960658897,"user_tz":240,"elapsed":47564,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}}},"source":["from gensim.models import Word2Vec\n","\n","model = Word2Vec(sentences=tokenized_abstracts,\n","                 size=100, window=5, min_count=1, workers=4)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daw0_Hneofpp","executionInfo":{"status":"ok","timestamp":1633960674465,"user_tz":240,"elapsed":207,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"fa02d849-b909-4bf8-d1f1-fe0f2380e741"},"source":["model.most_similar(\"annotation\")"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["[('annotations', 0.6894370317459106),\n"," ('2+1', 0.605654239654541),\n"," ('coding', 0.6048095226287842),\n"," ('construction', 0.5795214176177979),\n"," ('enrichment', 0.5551689267158508),\n"," ('annotating', 0.5536321401596069),\n"," ('creation', 0.5446563959121704),\n"," ('annotated', 0.5411858558654785),\n"," ('evaluation', 0.5304673314094543),\n"," ('analysis', 0.5106405019760132)]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEFMjIPqpUKQ","executionInfo":{"status":"ok","timestamp":1633960697039,"user_tz":240,"elapsed":173,"user":{"displayName":"Cassandra Jacobs","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12934675697410030326"}},"outputId":"28cd0dc0-9e23-41c9-e69f-2d8f0beda42f"},"source":["model.most_similar(\"parsing\")"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["[('parsers', 0.6895569562911987),\n"," ('tagging', 0.6513684988021851),\n"," ('parser', 0.6427717208862305),\n"," ('labeling', 0.5811421275138855),\n"," ('trees', 0.5802563428878784),\n"," ('dependency', 0.5662418603897095),\n"," ('scaffolds', 0.546824038028717),\n"," ('SRL', 0.5444870591163635),\n"," ('table-column', 0.5389926433563232),\n"," ('heterogeneity/gap', 0.5268558859825134)]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"_3veSz_spZrf"},"source":[""],"execution_count":null,"outputs":[]}]}
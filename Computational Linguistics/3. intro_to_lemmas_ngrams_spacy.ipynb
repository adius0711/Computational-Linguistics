{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QKzJldte1LY"
   },
   "source": [
    "# Expanding on basic text processing\n",
    "\n",
    "Basic text processing involves taking a corpus and producing useful representations of that text. Today, we will be talking about how we can look at corpora using basic tools. Now that we know how to load files into Colab, we can use a variety of built-in Python tools as well as natural language processing packages that are freely available as open-source software.\n",
    "\n",
    "This lecture will cover the following general steps to understanding your data.\n",
    "\n",
    "* \"lumping\" versus \"splitting\" in natural language processing and **lemmatization**\n",
    "* Computing basic statistics\n",
    "  * Counts of words\n",
    "  * Counts of n-grams\n",
    "  * Transition probabilities\n",
    "  * Term frequency / inverse document frequency (tf-idf)\n",
    "* Basic statistics in languages other than English\n",
    "  * The need for linguistic specialization\n",
    "  * Challenges posed by different writing systems (Hebrew, Chinese, Japanese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hj0lAfBI1ehQ",
    "outputId": "ad334ffc-85ea-445b-e505-1a0dbaadf691"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0daa617989db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# colab-specific\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# nlp software we used last time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "## load in all of our imports from last time\n",
    "# python built-ins\n",
    "from collections import Counter\n",
    "import os\n",
    "from pprint import pprint\n",
    "# colab-specific\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "# nlp software we used last time\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-2CxCkrnyeD",
    "outputId": "0954869b-cc25-40f0-bb93-ee1e90044a97"
   },
   "outputs": [],
   "source": [
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "## alternately: upload a file from your local machine:\n",
    "## uncomment the two lines below if uploading abstracts.tsv is easier\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# abstracts = uploaded['abstracts.tsv'].decode('utf-8')\n",
    "\n",
    "location_of_my_abstracts = ('/content/drive/MyDrive/Teaching/'\n",
    "                            'Fall2021/Computational Linguistics/'\n",
    "                            'Lectures/supplementary_files')\n",
    "## your location will probably be:\n",
    "# location_of_my_abstracts = ('/content/drive/Shared/'\n",
    "#                             'Computational Linguistics/Lectures/'\n",
    "#                             'supplementary_files')\n",
    "abstracts = open(os.path.join(location_of_my_abstracts,\n",
    "                              'abstracts.tsv'), 'r').read().split(\"\\n\")\n",
    "abstracts[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBc6dzyBx_zY"
   },
   "source": [
    "To build a dictionary ourselves, we basically want to loop through all of the words and add words it if they are not already in the dictionary, and increment by one if they are already in the dictionary. The code for that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjXPQdtMyPnU",
    "outputId": "74c1f547-dfe5-4300-a964-5fbbbd43b5bb"
   },
   "outputs": [],
   "source": [
    "abstract_counts_dict = {}\n",
    "\n",
    "for abstract in abstracts:\n",
    "  abstract_tokenized_into_words = word_tokenize(abstract)\n",
    "  for word in abstract_tokenized_into_words:\n",
    "    if word not in abstract_counts_dict:\n",
    "      abstract_counts_dict[word] = 1\n",
    "    else:\n",
    "      abstract_counts_dict[word] += 1\n",
    "\n",
    "print(abstract_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvttzvLXlk7p"
   },
   "source": [
    "## Sentence formatting can matter\n",
    "\n",
    "Sometimes a text is short and we may want to \"lump\" together different instances of the same word but which appears in slightly different ways. For example:\n",
    "\n",
    "> Doctor Marshall spent three weeks at a ski resort with their best doctor friends.\n",
    "\n",
    "We might want to count both instances of \"doctor\" as the same. That is, the word \"doctor\" appears twice in this text. \n",
    "\n",
    "If we want to lump both instances of \"doctor\" together, then we can edit the string that our tokenization algorithm gets ahead of time. The most common way to do this is to get rid of the contribution of case, or whether a word is UPPER or lower case.\n",
    "\n",
    "To change the case of a string, we can use the `.lower()` or `.upper()` methods. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTW82ZvFpteA",
    "outputId": "0a850450-8389-4001-c6de-5e0a4d809991"
   },
   "outputs": [],
   "source": [
    "# Convert a string to lowercase with the .lower() method\n",
    "print('Doctor Marshall spent three weeks at a ski resort with their best doctor friends.'.lower())\n",
    "\n",
    "# Now try UPPERCASE with .upper()\n",
    "print('Doctor Marshall spent three weeks at a ski resort with their best doctor friends.'.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "im4AB2cUqPqC",
    "outputId": "9a6126e1-396a-46ba-ba1f-724c82e174c4"
   },
   "outputs": [],
   "source": [
    "Counter(('Doctor Marshall spend three weeks at a ski '\n",
    "         'resort with their best doctor friends.').lower().split(\" \")).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7q3xjAdgqMxC",
    "outputId": "67f5e11a-534d-4561-c7b5-e6bc2d5ea1b2"
   },
   "outputs": [],
   "source": [
    "Counter(('Doctor Marshall spend three weeks at a ski '\n",
    "         'resort with their best doctor friends.').upper().split(\" \")).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU5rGmFVqnyp"
   },
   "source": [
    "### Can you think of a situation when we might want to preserve case information in a string?\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "</summary>\n",
    "  Case information is useful for telling what kind of word something is. For example, if we are trying to find all the corporations in a document (e.g., as part of a named entity recognition (NER) task), it will matter whether it is spelled \"The Dow Chemical Company\" versus \"The Dow Chemical company.\" \n",
    "  Case information can also tell us about register -- if we see a lowercase i, it might be $i$ written in a mathematics paper, or it could be informal (e.g., a tweet).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_l-8poYrkwm"
   },
   "source": [
    "## What other challenges are there for tokenization? Languages other than English\n",
    "\n",
    "* Writing systems for Chinese languages\n",
    "  * Tens of thousands of characters\n",
    "  * Many, many more words -- \"word\" in Mandarin is often compared to English language compounds (e.g., \"houseboat\")\n",
    "  * Words are not separated by spaces\n",
    "  * = Boundaries between words are highly ambiguous\n",
    "* Japanese\n",
    "  * Three writing systems (kanji, hirigana, katakana)\n",
    "  * No spaces are used\n",
    "  * Three scripts are used for complex linguistic reasons\n",
    "\n",
    "Tl;dr: Many written languages use scripts that do not easily translate into tokens in the English sense. We will delve more into this during the morphology section of the course \n",
    "üëÄ\n",
    "\n",
    "<!-- * Other scripts: E.g., Hebrew and Arabic\n",
    "  * These writing systems do not mark vowels\n",
    "    * Primarily available only for second language learners\n",
    "  * Vowels can (usually) be inferred from context\n",
    "  * Challenges for segmentation and tokenization because the vowels can completely change the kind of word (e.g., noun to verb; noun to another kind of noun, etc.) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzlp8Z47qakK",
    "outputId": "c238278e-60fe-4459-c9fb-5b1733459211"
   },
   "outputs": [],
   "source": [
    "# Google Translate of part of second paragraph of \n",
    "# https://zh.wikipedia.org/wiki/%E6%B1%89%E5%AD%97 accessed 9/9/2021: \n",
    "# Not only used by China, but for a long period of time,\n",
    "# it has also served as the only internationally-used script \n",
    "# in East Asia. Before the 20th century, it was the written \n",
    "# standard script of the Korean Peninsula, Vietnam, Ryukyu, and Japan. \n",
    "# In addition to Chinese, the ancient East Asian countries all created Chinese \n",
    "# characters on their own to a certain extent. \n",
    "\n",
    "chinese_script_from_wikipedia = (\n",
    "    \"‰∏çÂñÆ‰∏≠Âúã‰ΩøÁî®ÔºåÂú®ÂæàÈï∑ÊôÇÊúüÂÖßÈÇÑÂÖÖÁï∂Êù±‰∫ûÂú∞ÂçÄÂîØ‰∏ÄÁöÑÂúãÈöõÈÄöÁî®ÊñáÂ≠óÔºå\"\n",
    "    \"Âú®20‰∏ñÁ¥ÄÂâçÈÉΩÊòØÊúùÈÆÆÂçäÂ≥∂„ÄÅË∂äÂçó„ÄÅÁêâÁêÉÂíåÊó•Êú¨Á≠âÂúãÂÆ∂ÁöÑÊõ∏Èù¢Ë¶èÁØÑÊñáÂ≠ó„ÄÇ\"\n",
    "    \"Èô§‰∫ÜÊº¢Ë™û‰πãÂ§ñÔºåÂè§‰ª£Êù±‰∫ûË´∏ÂúãÈÉΩÊúâ‰∏ÄÂÆöÁ®ãÂ∫¶Âú∞Ëá™Ë°åÂâµË£ΩÊº¢Â≠ó„ÄÇ \")\n",
    "\n",
    "pprint(chinese_script_from_wikipedia.split(\"Ôºå\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtO4bOkL0oSx"
   },
   "source": [
    "It is clear from the above that splitting on `\"Ôºå\"` only breaks this sentence into clauses along these commas. This is a great case where we will want to use a regular expression to split the sentence on either `\"Ôºå\"` or `\"„ÄÇ\"`\n",
    "\n",
    "For this, we need language-specific knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYeyQBhU1DNq",
    "outputId": "19668a6b-3bb2-4fd4-c7a9-1afdd5981ffa"
   },
   "outputs": [],
   "source": [
    "# split the sentence above on either the comma or the period characters\n",
    "# here we import the regular expression built-in module from Python\n",
    "import re\n",
    "\n",
    "re.split('Ôºå|„ÄÇ', chinese_script_from_wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRBVgm-Uyxxr"
   },
   "source": [
    "# Lemmatization and tokenization with spaCy\n",
    "\n",
    "Sometimes we want a more sophisticated, linguistically-informed way to process our sentences. The open source Python package `spaCy` uses algorithms and pre-trained models from some of the most recent advances in NLP to make its tokenization and processing decisions. `spaCy` typically returns intuitive tokenizations. So, we will check out the output of `spaCy`'s model for English, which we need to download and load into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qT0T8gTPB981",
    "outputId": "e8f4e386-1b4f-4a11-db37-ad57b2e43378"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBqJy2KdNhD2",
    "outputId": "c44fd687-810e-4493-c6d5-3384c323ec8a"
   },
   "outputs": [],
   "source": [
    "# Begin analysis with spaCy -- looking at the structure of the output\n",
    "import spacy\n",
    "\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# a quick example on one of the abstracts from earlier\n",
    "spacy_abstract = spacy_model(abstracts[17])\n",
    "pprint(spacy_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXfpd_tKRfSO"
   },
   "source": [
    "Note that the output of the list comprehension above does not return a normal-looking array. Normally, we would expect to see quotation marks -- but because spaCy does not render it like this for us, we can tell that this is special behavior.\n",
    "\n",
    "In order to look inside, we have to actually inspect each of the objects within the `spacy_abstract` itself.\n",
    "\n",
    "Each object that spaCy returns is a `Doc`, which contains within it sentences (`sent`s) and tokens (`Token` objects). You can iterate through all the tokens with a simple `for` loop. \n",
    "\n",
    "spaCy does a lot of things on the backend to give you reasonably high-quality results. You can inspect all of the things it gives you for a Token object by typing `.` for a given token.\n",
    "\n",
    "```python\n",
    "for t in spacy_abstract[0:10]:\n",
    "  print(t.dep_, t.pos_, t.tag_)\n",
    "```\n",
    "\n",
    "will give you the output\n",
    "\n",
    "```\n",
    "amod ADJ JJ\n",
    "compound NOUN NN\n",
    "nsubj NOUN NN\n",
    "punct PUNCT -LRB-\n",
    "appos PROPN NNP\n",
    "punct PUNCT -RRB-\n",
    "aux AUX VBZ\n",
    "ROOT VERB VBN\n",
    "amod VERB VBG\n",
    "dobj NOUN NN\n",
    "```\n",
    "\n",
    "which correspond to values for the token's syntactic dependency (Week 12), its broad part-of-speech category (Week 11), and its classic syntactic part-of-speech \"tag\" (Week 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlnZ9d8NvO8"
   },
   "source": [
    "## From tokenization to lemmatization\n",
    "\n",
    "Our next goal with this is to extract **lemmas** from the spaCy tokens that it automatically processes.\n",
    "\n",
    "## What is a lemma?\n",
    "\n",
    "A lemma is something like the \"base form\" of a word. So for the words \"cats\" and \"cat\" or \"sleeping\" and \"sleeps\", we might want to collapse into just \"cat\" and \"sleep\" respectively. This is related to the UPPERCASE/lowercase problem earlier. \n",
    "\n",
    "The reason we often want to do this is _data sparsity_, which can make it hard to use language data for other applications. That is, sometimes we do not care what particular form of a word appeared in a text because that form might be so rare as to not be useful. So, we can combine it with another (related!) word for analysis.\n",
    "\n",
    "## What is lemmatization?\n",
    "\n",
    "**Lemmatization** turns more complex words into their base forms. This is useful for languages like English, which has relatively simple wordforms (singular/plural, very few ways to change verb forms), and very useful for languages like French or Spanish. For example, in French and Spanish, verbs are much more varied than in a language like English. \n",
    "\n",
    "<center><img src=\"https://leconjugueur.lefigaro.fr/images/ipadlcecr3.png\" width = 700 alt=\"A screenshot of a conjugation table from Le Conjuguer.com. It illustrates the conjugation of the verb _avoir_.\"/></center>\n",
    "\n",
    "So, in many cases, we want to turn potentially hundreds of forms into one single form. This is will help us avoid the problem of data sparsity. We can turn many rare words into more common ones -- which effectively allows us to learn even from events made of rare words. Otherwise, without clever models, we would basically be throwing that data away.\n",
    "\n",
    "## Getting lemmas from `spaCy`\n",
    "\n",
    "The lemmas, or the lemmatized forms of words that are the output of natural language processing models, are one of the many linguistic features that `spaCy` will give you. Most of the time, these lemmas are obtained through a complex dictionary lookup. That is, a word (`key`) like \"sleeps\" will have a lemma `value` of \"sleep.\" Other times, sophisticated algorithms are used to try to guess the lemma (a topic we will cover in the Morphology lectures in Week ).\n",
    "\n",
    "To get the lemma form of a word, simply ask for the `.lemma_` property of a `Token` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYcFDkJglv6Y",
    "outputId": "63dae48d-d604-4bff-f2e1-4ed7158c9301"
   },
   "outputs": [],
   "source": [
    "# get the lemmas from spacy\n",
    "for t in spacy_abstract[0:10]:\n",
    "  print(t.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTJ-PHJOqe0g"
   },
   "source": [
    "## Aggregate all lemmas to make lemma-nade (better count estimates for rare words)\n",
    "\n",
    "We can combine our techniques above and create a dictionary of word frequencies using the _lemmatized strings_ rather than the raw strings, as in the below example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JQO0kt7qr9_",
    "outputId": "d4eae9af-447a-49f6-e68c-84cc1471798c"
   },
   "outputs": [],
   "source": [
    "# this will take a while, running locally is not recommended\n",
    "lemmatized_abstract_counts_dict = {}\n",
    "\n",
    "for abstract in abstracts:\n",
    "  abstract_tokenized_into_words = spacy_model(abstract)\n",
    "  for word in abstract_tokenized_into_words:\n",
    "    # we have to change the previous examples\n",
    "    # from *word* to *lemma*\n",
    "    lemma = word.lemma_\n",
    "    print('lemma', lemma)\n",
    "    if lemma not in lemmatized_abstract_counts_dict:\n",
    "      lemmatized_abstract_counts_dict[lemma] = 1\n",
    "    else:\n",
    "      lemmatized_abstract_counts_dict[lemma] += 1\n",
    "\n",
    "print(lemmatized_abstract_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnAbR2ch08EW"
   },
   "source": [
    "There are many fewer lemmas than individual words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpZde52CzfBz",
    "outputId": "06e31cca-08ec-425d-8ac4-2dc59c5cab53"
   },
   "outputs": [],
   "source": [
    "len(lemmatized_abstract_counts_dict), len(abstract_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEPRV6-8sMNK"
   },
   "source": [
    "### Lemmatization can have huge consequences for your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQcfQdo2rN7q",
    "outputId": "b8da0ef2-8306-45d1-f0fb-a9ae6ea091e8"
   },
   "outputs": [],
   "source": [
    "pprint(sorted(abstract_counts_dict.items(),\n",
    "              key=lambda item: item[1], reverse=True)[0:5])\n",
    "pprint(sorted(lemmatized_abstract_counts_dict.items(),\n",
    "              key=lambda item: item[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOAP0qywyEpB"
   },
   "source": [
    "Or, we can check out familiar terms. Any idea what types of words will change most? When do you think lemmatization can hurt what we learn?\n",
    "\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "Mnp8HVxp8IIb",
    "outputId": "52a8f7ae-95dc-4a73-9f52-5bf227a6379d"
   },
   "outputs": [],
   "source": [
    "# suggest something for us to search for!\n",
    "\n",
    "print(abstract_counts_dict['priority'], lemmatized_abstract_counts_dict['priority'])\n",
    "print(abstract_counts_dict['organization'], lemmatized_abstract_counts_dict['organization'])\n",
    "print(abstract_counts_dict['get'], lemmatized_abstract_counts_dict['get'])\n",
    "print(abstract_counts_dict['go'], lemmatized_abstract_counts_dict['go'])\n",
    "print(lemmatized_abstract_counts_dict['went'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsU_mBsy4ghA"
   },
   "source": [
    "### Debugging a question that came up in class: Why would the counts of something like \"the\" be different across the two lists?\n",
    "\n",
    "Some possibilities:\n",
    "\n",
    "* There is something wrong with the code\n",
    "* There are differences between `spaCy` and `nltk` and how they tokenize. For example, it looks like `spaCy` keeps words linked to their parentheses. This is a puzzle but it is worth looking into because this behavior is a bit unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xf1P4Ymu4_ls",
    "outputId": "259d8316-d371-4d71-a497-783e71d67ddd"
   },
   "outputs": [],
   "source": [
    "[x for x in lemmatized_abstract_counts_dict if \"the\" in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g-UyVsKvPSC"
   },
   "source": [
    "# Computing more than count statistics for single tokens\n",
    "\n",
    "* Multi-word sequences (n-grams)\n",
    "* Conditional probabilities (transition probabilities)\n",
    "\n",
    "For Wednesday and Friday:\n",
    "\n",
    "* Bag-of-words representations (vector representations)\n",
    "* tf-idf (term frequency / inverse document frequency)\n",
    "* Smoothing and overcoming sparsity\n",
    "* Text normalization (e.g., spelling correction, edit distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3bEHn1mzqAs"
   },
   "source": [
    "# N-gram frequencies\n",
    "\n",
    "**N-grams** are defined as multi-word combinations of length $n$. Most of the time, for a language like English, these are immediately adjacent tokens in a sentence. **Bigrams** typically mean two-word sequences, **trigrams** sequences of three, and **4-grams** and **5-grams**...and so on. \n",
    "\n",
    "Computing bigram frequencies is very simple -- the easiest thing to do is to create a dictionary of dictionaries with the first word ($w_1$) as the key to another dictionary that contains the second words ($w_2$) as keys, which are then mapped onto the value (frequency) of that bigram.\n",
    "\n",
    "For this code, as with the unigram frequency code, note that the `not` **must** come first because you will run into issues building the dictionary. We can't add things to the dictionary if an entry doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iua9NnFUTp_D"
   },
   "outputs": [],
   "source": [
    "abstract_bigrams_dict = {}\n",
    "\n",
    "for abstract in abstracts:\n",
    "  abstract_tokenized_into_words = word_tokenize(abstract)\n",
    "  # note: enumerate allows us to get indices\n",
    "  for i, word in enumerate(abstract_tokenized_into_words):\n",
    "    word_string = word\n",
    "    # if we are not yet at the end of the sentence\n",
    "    if i < (len(abstract_tokenized_into_words) - 1):\n",
    "      next_word_string = abstract_tokenized_into_words[i + 1]\n",
    "    else:\n",
    "      next_word_string = \"<<EOS>>\"\n",
    "    # have we ever seen this word as the first one in a bigram?\n",
    "    if word_string not in abstract_bigrams_dict:\n",
    "      abstract_bigrams_dict[word_string] = {next_word: 1}\n",
    "    else:\n",
    "      # have we ever seen the second word with the first word?\n",
    "      if next_word_string not in abstract_bigrams_dict[word_string]:\n",
    "        abstract_bigrams_dict[word_string][next_word_string] = 1\n",
    "      else:\n",
    "        abstract_bigrams_dict[word_string][next_word_string] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7dUmAE8bAEp",
    "outputId": "651fe84f-94dc-4eb2-f96d-482d066ddb48"
   },
   "outputs": [],
   "source": [
    "sum(abstract_bigrams_dict['parsing'].values()), abstract_counts_dict['parsing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEocoq-712Zk",
    "outputId": "5fdc09bb-670c-4d12-dcdc-7a4052ef400a"
   },
   "outputs": [],
   "source": [
    "# briefly inspect one example to make sure it is saving counts properly\n",
    "pprint(sorted(abstract_bigrams_dict['parsing'].items(),\n",
    "              key=lambda item: item[1], reverse=True)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzT3qkTx3OLA"
   },
   "source": [
    "## Computing conditional probabilities\n",
    "\n",
    "Now that you have a dictionary of frequencies that has the structure `{w1: {w2: count(w1, w2)}}`, you can compute conditional probabilities. Formally, for a pair of words that occur in sequence in a sentence $w_1$ and $w_2$, these are defined as:\n",
    "\n",
    "<center>$\\large p(w_2|w_1) = \\frac{p(w_1 \\cap w_2)}{p(w_1)}$</center>\n",
    "\n",
    "We already know $p(w_1)$. To compute the probability of a _single_ word, all we have to do is find the frequency of A and divide it by the sum of the counts all of the words.\n",
    "\n",
    "We already know $p(A \\cap B)$ because we computed the frequencies of the words occurring together. For example, the frequency of \"parsing algorithms\" is 21 (from the above). In order to compute the _unconditional probability of a word_, all we have to do is divide by the count of all of the pairs of words in our dataset (i.e., the total number we have seen).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khdeYw923NxS",
    "outputId": "c8c40336-ceb5-4bd0-cceb-2e2d7a85b21a"
   },
   "outputs": [],
   "source": [
    "total_pairs = 0\n",
    "for w1 in abstract_bigrams_dict:\n",
    "  total_pairs += sum(abstract_bigrams_dict[w1].values())\n",
    "\n",
    "total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_Yq78_85Fw7",
    "outputId": "0c7e5ce3-d298-4709-effe-95fda1b5df6b"
   },
   "outputs": [],
   "source": [
    "freq_parsing = abstract_counts_dict['parsing']\n",
    "total_count_of_completions = sum(abstract_counts_dict.values())\n",
    "p_parsing = freq_parsing / total_count_of_completions\n",
    "p_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G47G8nI728mj",
    "outputId": "fe1cfc4b-db7b-4214-b5c1-309ff5fe4488"
   },
   "outputs": [],
   "source": [
    "# if \"parsing algorithms\" occurs 21 times, then it makes up:\n",
    "p_parsing_and_algorithms = 21 / 3559578\n",
    "p_parsing_and_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkhcQFrr4Tya",
    "outputId": "926f8396-a1aa-43f3-e474-1807c8ca32e5"
   },
   "outputs": [],
   "source": [
    "# therefore the probability of algorithms _given_ parsing is:\n",
    "p_parsing_and_algorithms / p_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovm11gYJ5lb6",
    "outputId": "28f7dc99-5c0a-4c01-da9d-d38aeb9c266c"
   },
   "outputs": [],
   "source": [
    "# now we can compare another alternative -- dividing by known frequencies\n",
    "abstract_bigrams_dict['parsing']['algorithms'] / sum(abstract_bigrams_dict['parsing'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvqA0T9N9bSR"
   },
   "source": [
    "## Bayes' rule\n",
    "\n",
    "Bayes' rule allows us to change between two different conditional probabilities. Bayes' rule comes up in many places when we have one known probability but we do not know others. It can take a lot of practice to be familiar with. \n",
    "\n",
    "But, here is an example for the previous:\n",
    "\n",
    "$p(w_2=\\text{algorithms} | w_1=\\text{parsing})$ = $\\large \\frac{p(w_1=\\text{parsing} | w_2=\\text{algorithms}) \\cdot p(w_2=\\text{algorithms})}{p(w_1=\\text{parsing})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB_qIwRJ9HC9"
   },
   "source": [
    "## For Wednesday\n",
    "\n",
    "Take a look at the assignment to be released by 5pm on Monday September 12, 2021. Please come to class with questions or post on the Discord server.\n",
    "\n",
    "Be sure to keep up with the readings. This week is:\n",
    "\n",
    "* Jurafsky and Martin (Speech and Language Processing 3rd Ed.) Ch. 3 (pdf pages 37-54)\n",
    "* Eisenstein (2019) section 2.1 ‚Äì Bag of Words (pdf pages 31-34)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_to_lemmas_ngrams_spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

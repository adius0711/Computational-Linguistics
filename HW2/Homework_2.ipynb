{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework_2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ccD5Kq24WAN3"},"source":["# Question 1 (5 points)\n","Load in all of the packages you will need for this assignment in the cell below. \n","\n","If you load in other packages later in the notebook, be sure to bring them up here. This is good coding practice and will look cleaner for everyone when reading your code.\n","\n","You will need the following:\n","\n","* To load a plain text file (`abstracts.tsv`) in with the colab interface (either local to your drive or by uploading the file to the notebook)\n","* The NLTK tokenizer for English\n","* The spaCy word tokenizer for English"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uTReaS3PbGC","executionInfo":{"status":"ok","timestamp":1652982062638,"user_tz":240,"elapsed":8728,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"1c7af134-a23b-4d61-c2cc-d268ad88194d"},"source":["!pip3 install spacy nltk \n","!python -m spacy download en_core_web_sm"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 26.9 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.64.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfOQBQztFnqW","executionInfo":{"status":"ok","timestamp":1652982062638,"user_tz":240,"elapsed":5,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"be3d1a57-93f7-462c-e35c-184b68909882"},"source":["# Load in packages that you will use in this notebook\n","from pprint import pprint\n","from google.colab import drive\n","from google.colab import files\n","\n","import nltk\n","from nltk import word_tokenize, sent_tokenize\n","nltk.download('punkt')\n","\n","import spacy\n","import os\n","# put other packages you will use below this line\n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"SZSPSo-TWTxL"},"source":["# Question 2 (1 point)\n","\n","Load in the file called `abstracts.tsv` in the `data/` subdirectory of this folder into this notebook.\n","\n","Uncomment one of the two blocks below.\n","\n","Then, edit the line that you uncommented to load in abstracts.tsv.\n","\n","Note that using the `files` command requires you to do a bit more work to load the file in in Question 3. Be sure to check previous notebooks."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bl225PcgV61a","executionInfo":{"status":"ok","timestamp":1652982084822,"user_tz":240,"elapsed":22188,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"5254ee3f-a07f-4baf-8b86-a9a140973ff8"},"source":["# Block 1: File stored on your google drive\n","drive.mount('/content/drive', force_remount=True)\n","!ls '/content/drive/MyDrive/Fall 2021 Computational Linguistics Notebooks/HW2/data'\n","data_folder_path = input(\"Enter the data folder path \")\n","\n","# Block 2: File stored on your computer that you upload to the notebook directly\n","# uploaded = files.upload()"],"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","ls: cannot access '/content/drive/MyDrive/HW2/Fall 2021 Computational Linguistics Notebooks/data': No such file or directory\n","Enter the data folder path /content/drive/MyDrive/Fall 2021 Computational Linguistics Notebooks/HW2/data\n"]}]},{"cell_type":"markdown","metadata":{"id":"LRL5OkT4Wn-4"},"source":["# Question 3: 3 points\n","\n","In this section, we will be comparing different preprocessing strategies. For this question, you should first preview the data by looking at the first 5 lines. Use [a slice](https://stackoverflow.com/questions/509211/understanding-slice-notation) to print the first five elements from the array.\n","\n","Then, separate all of the abstracts on all whitespace. Store this in an array of string arrays called `split_abstracts`."]},{"cell_type":"code","metadata":{"id":"nq6-78b1IGPm","executionInfo":{"status":"ok","timestamp":1652982086367,"user_tz":240,"elapsed":1550,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}}},"source":["# preview data (print the first five lines)\n","abstracts = open(os.path.join(data_folder_path, 'abstracts.tsv'), 'r').read().split(\"\\n\")\n","abstracts[0:5]\n","\n","# split every sentence on whitespace and save array\n","split_abstracts = []\n","for abstract in abstracts:\n","  split_abstracts.append(abstract.split())"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UqHOFeVIGzB"},"source":["# Question 4: 4 points\n","\n","Now, we are going to use the `nltk` `word_tokenize` function. You should have loaded this above in the very first block. Use `word_tokenize` on all the abstracts and store this in an array of string arrays called `nltk_tokenized_abstracts`. Use a slice to print the fifth to the tenth elements of the array."]},{"cell_type":"code","metadata":{"id":"z8oOzoHiIzdh","executionInfo":{"status":"ok","timestamp":1652982153876,"user_tz":240,"elapsed":67513,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}}},"source":["# use nltk's word_tokenize function over all of the abstracts\n","def word_tokenization(abstract):\n","    return word_tokenize(abstract)\n","\n","# save the output into a variable\n","nltk_tokenized_abstracts = []\n","abstract_counts_dict = {}\n","for abstract in abstracts:\n","    nltk_tokenized_abstracts.append(word_tokenize(abstract))\n","    for word in word_tokenization(abstract):\n","        if word not in abstract_counts_dict:\n","            abstract_counts_dict[word] = 1\n","        else:\n","            abstract_counts_dict[word] += 1"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9PpfhdWSI0Me"},"source":["# Question 5: 5 points\n","\n","Now, we are going to use the `spacy` tokenization function. The output that spacy gives you is more complicated than the output of `nltk`'s `word_tokenize` function, because the `spacy` API takes a string (e.g., \"I like cheese\") and returns a `Doc` object. Within the `Doc` object there are `Token`s, and each `Token` has a `text` object. \n","\n","For this question, what you need to do is implement another loop through all of the abstracts, and store a list (array) of all of the token _strings_ from each `Token` object. If you were paying attention during the tokenization lecture this should be easy.\n","\n","Store all of these tokenizations into an array of string arrays called `spacy_tokenized_abstracts`."]},{"cell_type":"code","metadata":{"id":"mTwCwQ7nM8dX","executionInfo":{"status":"ok","timestamp":1652982890957,"user_tz":240,"elapsed":737091,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}}},"source":["# use spacy's tokenization features\n","spacy_model = spacy.load('en_core_web_sm')\n","def spacy_tokenization(abstract):\n","    return spacy_model(abstract)\n","\n","# save the output into a variable\n","spacy_tokenized_abstracts = []\n","for abstract in abstracts:\n","    spacy_tokenized_abstracts.append(spacy_tokenization(abstract))"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLC2daI-Myn1"},"source":["# Question 6: Compare tokenizations (8 points)\n","\n","Now that we have three tokenizations (`split_abstracts`, `nltk_tokenized_abstracts`, and `spacy_tokenized_abstracts`), we want to compare how similar the tokenizations are. Pick a slice of 5 abstracts with any start and end indices. Demonstrate that the total number of abstracts that you selected is 5 by printing the length of that subset of abstracts.\n","\n","Tokenize each of the 5 abstracts according to each of the three approach above, and print their output in the code cell below. Then, in the cell below that, explain how these tokenizations differ. What are the strengths and weaknesses of each tokenization approach? Do you think one of the tokenizations is better than another? Can you think of a way you would test which one is better? Refer to justification from the readings where appropriate."]},{"cell_type":"markdown","metadata":{"id":"4sZ7km6oRYZs"},"source":["### Question 6A: Code (3/8)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ld_dCKjLQaqf","executionInfo":{"status":"ok","timestamp":1652982891160,"user_tz":240,"elapsed":212,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"b5b1d538-2e28-48d2-dcc4-cbd85c97b42c"},"source":["# select a slice of 5 abstracts from the documents\n","print(abstracts[0:5])\n","# print the length of this slice to show that it is five abstracts\n","print(len(abstracts[0:5]))\n","\n","# print the outputs of each of these 3 tokenizations for all 5 abstracts\n","print(\"Split Abstracts:\")\n","pprint(split_abstracts[0:5])\n","\n","print('\\n')\n","\n","print(\"Word Tokenize:\")\n","pprint(nltk_tokenized_abstracts[0:5])\n","\n","print('\\n')\n","\n","print(\"Spacy Tokenize:\")\n","pprint(spacy_tokenized_abstracts[0:5])"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.', 'Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.', 'We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.', \"Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.\", 'Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.']\n","5\n","Split Abstracts:\n","[['Offensive',\n","  'language',\n","  'detection',\n","  '(OLD)',\n","  'has',\n","  'received',\n","  'increasing',\n","  'attention',\n","  'due',\n","  'to',\n","  'its',\n","  'societal',\n","  'impact.',\n","  'Recent',\n","  'work',\n","  'shows',\n","  'that',\n","  'bidirectional',\n","  'transformer',\n","  'based',\n","  'methods',\n","  'obtain',\n","  'impressive',\n","  'performance',\n","  'on',\n","  'OLD.',\n","  'However,',\n","  'such',\n","  'methods',\n","  'usually',\n","  'rely',\n","  'on',\n","  'large-scale',\n","  'well-labeled',\n","  'OLD',\n","  'datasets',\n","  'for',\n","  'model',\n","  'training.',\n","  'To',\n","  'address',\n","  'the',\n","  'issue',\n","  'of',\n","  'data/label',\n","  'scarcity',\n","  'in',\n","  'OLD,',\n","  'in',\n","  'this',\n","  'paper,',\n","  'we',\n","  'propose',\n","  'a',\n","  'simple',\n","  'yet',\n","  'effective',\n","  'domain',\n","  'adaptation',\n","  'approach',\n","  'to',\n","  'train',\n","  'bidirectional',\n","  'transformers.',\n","  'Our',\n","  'approach',\n","  'introduces',\n","  'domain',\n","  'adaptation',\n","  '(DA)',\n","  'training',\n","  'procedures',\n","  'to',\n","  'ALBERT,',\n","  'such',\n","  'that',\n","  'it',\n","  'can',\n","  'effectively',\n","  'exploit',\n","  'auxiliary',\n","  'data',\n","  'from',\n","  'source',\n","  'domains',\n","  'to',\n","  'improve',\n","  'the',\n","  'OLD',\n","  'performance',\n","  'in',\n","  'a',\n","  'target',\n","  'domain.',\n","  'Experimental',\n","  'results',\n","  'on',\n","  'benchmark',\n","  'datasets',\n","  'show',\n","  'that',\n","  'our',\n","  'approach,',\n","  'ALBERT',\n","  '(DA),',\n","  'obtains',\n","  'the',\n","  'state-of-the-art',\n","  'performance',\n","  'in',\n","  'most',\n","  'cases.',\n","  'Particularly,',\n","  'our',\n","  'approach',\n","  'significantly',\n","  'benefits',\n","  'underrepresented',\n","  'and',\n","  'under-performing',\n","  'classes,',\n","  'with',\n","  'a',\n","  'significant',\n","  'improvement',\n","  'over',\n","  'ALBERT.'],\n"," ['Hate',\n","  'speech',\n","  'and',\n","  'profanity',\n","  'detection',\n","  'suffer',\n","  'from',\n","  'data',\n","  'sparsity,',\n","  'especially',\n","  'for',\n","  'languages',\n","  'other',\n","  'than',\n","  'English,',\n","  'due',\n","  'to',\n","  'the',\n","  'subjective',\n","  'nature',\n","  'of',\n","  'the',\n","  'tasks',\n","  'and',\n","  'the',\n","  'resulting',\n","  'annotation',\n","  'incompatibility',\n","  'of',\n","  'existing',\n","  'corpora.',\n","  'In',\n","  'this',\n","  'study,',\n","  'we',\n","  'identify',\n","  'profane',\n","  'subspaces',\n","  'in',\n","  'word',\n","  'and',\n","  'sentence',\n","  'representations',\n","  'and',\n","  'explore',\n","  'their',\n","  'generalization',\n","  'capability',\n","  'on',\n","  'a',\n","  'variety',\n","  'of',\n","  'similar',\n","  'and',\n","  'distant',\n","  'target',\n","  'tasks',\n","  'in',\n","  'a',\n","  'zero-shot',\n","  'setting.',\n","  'This',\n","  'is',\n","  'done',\n","  'monolingually',\n","  '(German)',\n","  'and',\n","  'cross-lingually',\n","  'to',\n","  'closely-related',\n","  '(English),',\n","  'distantly-related',\n","  '(French)',\n","  'and',\n","  'non-related',\n","  '(Arabic)',\n","  'tasks.',\n","  'We',\n","  'observe',\n","  'that,',\n","  'on',\n","  'both',\n","  'similar',\n","  'and',\n","  'distant',\n","  'target',\n","  'tasks',\n","  'and',\n","  'across',\n","  'all',\n","  'languages,',\n","  'the',\n","  'subspace-based',\n","  'representations',\n","  'transfer',\n","  'more',\n","  'effectively',\n","  'than',\n","  'standard',\n","  'BERT',\n","  'representations',\n","  'in',\n","  'the',\n","  'zero-shot',\n","  'setting,',\n","  'with',\n","  'improvements',\n","  'between',\n","  'F1',\n","  '+10.9',\n","  'and',\n","  'F1',\n","  '+42.9',\n","  'over',\n","  'the',\n","  'baselines',\n","  'across',\n","  'all',\n","  'tested',\n","  'monolingual',\n","  'and',\n","  'cross-lingual',\n","  'scenarios.'],\n"," ['We',\n","  'introduce',\n","  'HateBERT,',\n","  'a',\n","  're-trained',\n","  'BERT',\n","  'model',\n","  'for',\n","  'abusive',\n","  'language',\n","  'detection',\n","  'in',\n","  'English.',\n","  'The',\n","  'model',\n","  'was',\n","  'trained',\n","  'on',\n","  'RAL-E,',\n","  'a',\n","  'large-scale',\n","  'dataset',\n","  'of',\n","  'Reddit',\n","  'comments',\n","  'in',\n","  'English',\n","  'from',\n","  'communities',\n","  'banned',\n","  'for',\n","  'being',\n","  'offensive,',\n","  'abusive,',\n","  'or',\n","  'hateful',\n","  'that',\n","  'we',\n","  'have',\n","  'curated',\n","  'and',\n","  'made',\n","  'available',\n","  'to',\n","  'the',\n","  'public.',\n","  'We',\n","  'present',\n","  'the',\n","  'results',\n","  'of',\n","  'a',\n","  'detailed',\n","  'comparison',\n","  'between',\n","  'a',\n","  'general',\n","  'pre-trained',\n","  'language',\n","  'model',\n","  'and',\n","  'the',\n","  'retrained',\n","  'version',\n","  'on',\n","  'three',\n","  'English',\n","  'datasets',\n","  'for',\n","  'offensive,',\n","  'abusive',\n","  'language',\n","  'and',\n","  'hate',\n","  'speech',\n","  'detection',\n","  'tasks.',\n","  'In',\n","  'all',\n","  'datasets,',\n","  'HateBERT',\n","  'outperforms',\n","  'the',\n","  'corresponding',\n","  'general',\n","  'BERT',\n","  'model.',\n","  'We',\n","  'also',\n","  'discuss',\n","  'a',\n","  'battery',\n","  'of',\n","  'experiments',\n","  'comparing',\n","  'the',\n","  'portability',\n","  'of',\n","  'the',\n","  'fine-tuned',\n","  'models',\n","  'across',\n","  'the',\n","  'datasets,',\n","  'suggesting',\n","  'that',\n","  'portability',\n","  'is',\n","  'affected',\n","  'by',\n","  'compatibility',\n","  'of',\n","  'the',\n","  'annotated',\n","  'phenomena.'],\n"," ['Hateful',\n","  'memes',\n","  'pose',\n","  'a',\n","  'unique',\n","  'challenge',\n","  'for',\n","  'current',\n","  'machine',\n","  'learning',\n","  'systems',\n","  'because',\n","  'their',\n","  'message',\n","  'is',\n","  'derived',\n","  'from',\n","  'both',\n","  'text-',\n","  'and',\n","  'visual-modalities.',\n","  'To',\n","  'this',\n","  'effect,',\n","  'Facebook',\n","  'released',\n","  'the',\n","  'Hateful',\n","  'Memes',\n","  'Challenge,',\n","  'a',\n","  'dataset',\n","  'of',\n","  'memes',\n","  'with',\n","  'pre-extracted',\n","  'text',\n","  'captions,',\n","  'but',\n","  'it',\n","  'is',\n","  'unclear',\n","  'whether',\n","  'these',\n","  'synthetic',\n","  'examples',\n","  'generalize',\n","  'to',\n","  '{`}memes',\n","  'in',\n","  'the',\n","  \"wild{'}.\",\n","  'In',\n","  'this',\n","  'paper,',\n","  'we',\n","  'collect',\n","  'hateful',\n","  'and',\n","  'non-hateful',\n","  'memes',\n","  'from',\n","  'Pinterest',\n","  'to',\n","  'evaluate',\n","  'out-of-sample',\n","  'performance',\n","  'on',\n","  'models',\n","  'pre-trained',\n","  'on',\n","  'the',\n","  'Facebook',\n","  'dataset.',\n","  'We',\n","  'find',\n","  'that',\n","  '{`}memes',\n","  'in',\n","  'the',\n","  \"wild{'}\",\n","  'differ',\n","  'in',\n","  'two',\n","  'key',\n","  'aspects:',\n","  '1)',\n","  'Captions',\n","  'must',\n","  'be',\n","  'extracted',\n","  'via',\n","  'OCR,',\n","  'injecting',\n","  'noise',\n","  'and',\n","  'diminishing',\n","  'performance',\n","  'of',\n","  'multimodal',\n","  'models,',\n","  'and',\n","  '2)',\n","  'Memes',\n","  'are',\n","  'more',\n","  'diverse',\n","  'than',\n","  '{`}traditional',\n","  \"memes{'},\",\n","  'including',\n","  'screenshots',\n","  'of',\n","  'conversations',\n","  'or',\n","  'text',\n","  'on',\n","  'a',\n","  'plain',\n","  'background.',\n","  'This',\n","  'paper',\n","  'thus',\n","  'serves',\n","  'as',\n","  'a',\n","  'reality-check',\n","  'for',\n","  'the',\n","  'current',\n","  'benchmark',\n","  'of',\n","  'hateful',\n","  'meme',\n","  'detection',\n","  'and',\n","  'its',\n","  'applicability',\n","  'for',\n","  'detecting',\n","  'real',\n","  'world',\n","  'hate.'],\n"," ['Content',\n","  'moderation',\n","  'is',\n","  'often',\n","  'performed',\n","  'by',\n","  'a',\n","  'collaboration',\n","  'between',\n","  'humans',\n","  'and',\n","  'machine',\n","  'learning',\n","  'models.',\n","  'However,',\n","  'it',\n","  'is',\n","  'not',\n","  'well',\n","  'understood',\n","  'how',\n","  'to',\n","  'design',\n","  'the',\n","  'collaborative',\n","  'process',\n","  'so',\n","  'as',\n","  'to',\n","  'maximize',\n","  'the',\n","  'combined',\n","  'moderator-model',\n","  'system',\n","  'performance.',\n","  'This',\n","  'work',\n","  'presents',\n","  'a',\n","  'rigorous',\n","  'study',\n","  'of',\n","  'this',\n","  'problem,',\n","  'focusing',\n","  'on',\n","  'an',\n","  'approach',\n","  'that',\n","  'incorporates',\n","  'model',\n","  'uncertainty',\n","  'into',\n","  'the',\n","  'collaborative',\n","  'process.',\n","  'First,',\n","  'we',\n","  'introduce',\n","  'principled',\n","  'metrics',\n","  'to',\n","  'describe',\n","  'the',\n","  'performance',\n","  'of',\n","  'the',\n","  'collaborative',\n","  'system',\n","  'under',\n","  'capacity',\n","  'constraints',\n","  'on',\n","  'the',\n","  'human',\n","  'moderator,',\n","  'quantifying',\n","  'how',\n","  'efficiently',\n","  'the',\n","  'combined',\n","  'system',\n","  'utilizes',\n","  'human',\n","  'decisions.',\n","  'Using',\n","  'these',\n","  'metrics,',\n","  'we',\n","  'conduct',\n","  'a',\n","  'large',\n","  'benchmark',\n","  'study',\n","  'evaluating',\n","  'the',\n","  'performance',\n","  'of',\n","  'state-of-the-art',\n","  'uncertainty',\n","  'models',\n","  'under',\n","  'different',\n","  'collaborative',\n","  'review',\n","  'strategies.',\n","  'We',\n","  'find',\n","  'that',\n","  'an',\n","  'uncertainty-based',\n","  'strategy',\n","  'consistently',\n","  'outperforms',\n","  'the',\n","  'widely',\n","  'used',\n","  'strategy',\n","  'based',\n","  'on',\n","  'toxicity',\n","  'scores,',\n","  'and',\n","  'moreover',\n","  'that',\n","  'the',\n","  'choice',\n","  'of',\n","  'review',\n","  'strategy',\n","  'drastically',\n","  'changes',\n","  'the',\n","  'overall',\n","  'system',\n","  'performance.',\n","  'Our',\n","  'results',\n","  'demonstrate',\n","  'the',\n","  'importance',\n","  'of',\n","  'rigorous',\n","  'metrics',\n","  'for',\n","  'understanding',\n","  'and',\n","  'developing',\n","  'effective',\n","  'moderator-model',\n","  'systems',\n","  'for',\n","  'content',\n","  'moderation,',\n","  'as',\n","  'well',\n","  'as',\n","  'the',\n","  'utility',\n","  'of',\n","  'uncertainty',\n","  'estimation',\n","  'in',\n","  'this',\n","  'domain.']]\n","\n","\n","Word Tokenize:\n","[['Offensive',\n","  'language',\n","  'detection',\n","  '(',\n","  'OLD',\n","  ')',\n","  'has',\n","  'received',\n","  'increasing',\n","  'attention',\n","  'due',\n","  'to',\n","  'its',\n","  'societal',\n","  'impact',\n","  '.',\n","  'Recent',\n","  'work',\n","  'shows',\n","  'that',\n","  'bidirectional',\n","  'transformer',\n","  'based',\n","  'methods',\n","  'obtain',\n","  'impressive',\n","  'performance',\n","  'on',\n","  'OLD',\n","  '.',\n","  'However',\n","  ',',\n","  'such',\n","  'methods',\n","  'usually',\n","  'rely',\n","  'on',\n","  'large-scale',\n","  'well-labeled',\n","  'OLD',\n","  'datasets',\n","  'for',\n","  'model',\n","  'training',\n","  '.',\n","  'To',\n","  'address',\n","  'the',\n","  'issue',\n","  'of',\n","  'data/label',\n","  'scarcity',\n","  'in',\n","  'OLD',\n","  ',',\n","  'in',\n","  'this',\n","  'paper',\n","  ',',\n","  'we',\n","  'propose',\n","  'a',\n","  'simple',\n","  'yet',\n","  'effective',\n","  'domain',\n","  'adaptation',\n","  'approach',\n","  'to',\n","  'train',\n","  'bidirectional',\n","  'transformers',\n","  '.',\n","  'Our',\n","  'approach',\n","  'introduces',\n","  'domain',\n","  'adaptation',\n","  '(',\n","  'DA',\n","  ')',\n","  'training',\n","  'procedures',\n","  'to',\n","  'ALBERT',\n","  ',',\n","  'such',\n","  'that',\n","  'it',\n","  'can',\n","  'effectively',\n","  'exploit',\n","  'auxiliary',\n","  'data',\n","  'from',\n","  'source',\n","  'domains',\n","  'to',\n","  'improve',\n","  'the',\n","  'OLD',\n","  'performance',\n","  'in',\n","  'a',\n","  'target',\n","  'domain',\n","  '.',\n","  'Experimental',\n","  'results',\n","  'on',\n","  'benchmark',\n","  'datasets',\n","  'show',\n","  'that',\n","  'our',\n","  'approach',\n","  ',',\n","  'ALBERT',\n","  '(',\n","  'DA',\n","  ')',\n","  ',',\n","  'obtains',\n","  'the',\n","  'state-of-the-art',\n","  'performance',\n","  'in',\n","  'most',\n","  'cases',\n","  '.',\n","  'Particularly',\n","  ',',\n","  'our',\n","  'approach',\n","  'significantly',\n","  'benefits',\n","  'underrepresented',\n","  'and',\n","  'under-performing',\n","  'classes',\n","  ',',\n","  'with',\n","  'a',\n","  'significant',\n","  'improvement',\n","  'over',\n","  'ALBERT',\n","  '.'],\n"," ['Hate',\n","  'speech',\n","  'and',\n","  'profanity',\n","  'detection',\n","  'suffer',\n","  'from',\n","  'data',\n","  'sparsity',\n","  ',',\n","  'especially',\n","  'for',\n","  'languages',\n","  'other',\n","  'than',\n","  'English',\n","  ',',\n","  'due',\n","  'to',\n","  'the',\n","  'subjective',\n","  'nature',\n","  'of',\n","  'the',\n","  'tasks',\n","  'and',\n","  'the',\n","  'resulting',\n","  'annotation',\n","  'incompatibility',\n","  'of',\n","  'existing',\n","  'corpora',\n","  '.',\n","  'In',\n","  'this',\n","  'study',\n","  ',',\n","  'we',\n","  'identify',\n","  'profane',\n","  'subspaces',\n","  'in',\n","  'word',\n","  'and',\n","  'sentence',\n","  'representations',\n","  'and',\n","  'explore',\n","  'their',\n","  'generalization',\n","  'capability',\n","  'on',\n","  'a',\n","  'variety',\n","  'of',\n","  'similar',\n","  'and',\n","  'distant',\n","  'target',\n","  'tasks',\n","  'in',\n","  'a',\n","  'zero-shot',\n","  'setting',\n","  '.',\n","  'This',\n","  'is',\n","  'done',\n","  'monolingually',\n","  '(',\n","  'German',\n","  ')',\n","  'and',\n","  'cross-lingually',\n","  'to',\n","  'closely-related',\n","  '(',\n","  'English',\n","  ')',\n","  ',',\n","  'distantly-related',\n","  '(',\n","  'French',\n","  ')',\n","  'and',\n","  'non-related',\n","  '(',\n","  'Arabic',\n","  ')',\n","  'tasks',\n","  '.',\n","  'We',\n","  'observe',\n","  'that',\n","  ',',\n","  'on',\n","  'both',\n","  'similar',\n","  'and',\n","  'distant',\n","  'target',\n","  'tasks',\n","  'and',\n","  'across',\n","  'all',\n","  'languages',\n","  ',',\n","  'the',\n","  'subspace-based',\n","  'representations',\n","  'transfer',\n","  'more',\n","  'effectively',\n","  'than',\n","  'standard',\n","  'BERT',\n","  'representations',\n","  'in',\n","  'the',\n","  'zero-shot',\n","  'setting',\n","  ',',\n","  'with',\n","  'improvements',\n","  'between',\n","  'F1',\n","  '+10.9',\n","  'and',\n","  'F1',\n","  '+42.9',\n","  'over',\n","  'the',\n","  'baselines',\n","  'across',\n","  'all',\n","  'tested',\n","  'monolingual',\n","  'and',\n","  'cross-lingual',\n","  'scenarios',\n","  '.'],\n"," ['We',\n","  'introduce',\n","  'HateBERT',\n","  ',',\n","  'a',\n","  're-trained',\n","  'BERT',\n","  'model',\n","  'for',\n","  'abusive',\n","  'language',\n","  'detection',\n","  'in',\n","  'English',\n","  '.',\n","  'The',\n","  'model',\n","  'was',\n","  'trained',\n","  'on',\n","  'RAL-E',\n","  ',',\n","  'a',\n","  'large-scale',\n","  'dataset',\n","  'of',\n","  'Reddit',\n","  'comments',\n","  'in',\n","  'English',\n","  'from',\n","  'communities',\n","  'banned',\n","  'for',\n","  'being',\n","  'offensive',\n","  ',',\n","  'abusive',\n","  ',',\n","  'or',\n","  'hateful',\n","  'that',\n","  'we',\n","  'have',\n","  'curated',\n","  'and',\n","  'made',\n","  'available',\n","  'to',\n","  'the',\n","  'public',\n","  '.',\n","  'We',\n","  'present',\n","  'the',\n","  'results',\n","  'of',\n","  'a',\n","  'detailed',\n","  'comparison',\n","  'between',\n","  'a',\n","  'general',\n","  'pre-trained',\n","  'language',\n","  'model',\n","  'and',\n","  'the',\n","  'retrained',\n","  'version',\n","  'on',\n","  'three',\n","  'English',\n","  'datasets',\n","  'for',\n","  'offensive',\n","  ',',\n","  'abusive',\n","  'language',\n","  'and',\n","  'hate',\n","  'speech',\n","  'detection',\n","  'tasks',\n","  '.',\n","  'In',\n","  'all',\n","  'datasets',\n","  ',',\n","  'HateBERT',\n","  'outperforms',\n","  'the',\n","  'corresponding',\n","  'general',\n","  'BERT',\n","  'model',\n","  '.',\n","  'We',\n","  'also',\n","  'discuss',\n","  'a',\n","  'battery',\n","  'of',\n","  'experiments',\n","  'comparing',\n","  'the',\n","  'portability',\n","  'of',\n","  'the',\n","  'fine-tuned',\n","  'models',\n","  'across',\n","  'the',\n","  'datasets',\n","  ',',\n","  'suggesting',\n","  'that',\n","  'portability',\n","  'is',\n","  'affected',\n","  'by',\n","  'compatibility',\n","  'of',\n","  'the',\n","  'annotated',\n","  'phenomena',\n","  '.'],\n"," ['Hateful',\n","  'memes',\n","  'pose',\n","  'a',\n","  'unique',\n","  'challenge',\n","  'for',\n","  'current',\n","  'machine',\n","  'learning',\n","  'systems',\n","  'because',\n","  'their',\n","  'message',\n","  'is',\n","  'derived',\n","  'from',\n","  'both',\n","  'text-',\n","  'and',\n","  'visual-modalities',\n","  '.',\n","  'To',\n","  'this',\n","  'effect',\n","  ',',\n","  'Facebook',\n","  'released',\n","  'the',\n","  'Hateful',\n","  'Memes',\n","  'Challenge',\n","  ',',\n","  'a',\n","  'dataset',\n","  'of',\n","  'memes',\n","  'with',\n","  'pre-extracted',\n","  'text',\n","  'captions',\n","  ',',\n","  'but',\n","  'it',\n","  'is',\n","  'unclear',\n","  'whether',\n","  'these',\n","  'synthetic',\n","  'examples',\n","  'generalize',\n","  'to',\n","  '{',\n","  '`',\n","  '}',\n","  'memes',\n","  'in',\n","  'the',\n","  'wild',\n","  '{',\n","  \"'\",\n","  '}',\n","  '.',\n","  'In',\n","  'this',\n","  'paper',\n","  ',',\n","  'we',\n","  'collect',\n","  'hateful',\n","  'and',\n","  'non-hateful',\n","  'memes',\n","  'from',\n","  'Pinterest',\n","  'to',\n","  'evaluate',\n","  'out-of-sample',\n","  'performance',\n","  'on',\n","  'models',\n","  'pre-trained',\n","  'on',\n","  'the',\n","  'Facebook',\n","  'dataset',\n","  '.',\n","  'We',\n","  'find',\n","  'that',\n","  '{',\n","  '`',\n","  '}',\n","  'memes',\n","  'in',\n","  'the',\n","  'wild',\n","  '{',\n","  \"'\",\n","  '}',\n","  'differ',\n","  'in',\n","  'two',\n","  'key',\n","  'aspects',\n","  ':',\n","  '1',\n","  ')',\n","  'Captions',\n","  'must',\n","  'be',\n","  'extracted',\n","  'via',\n","  'OCR',\n","  ',',\n","  'injecting',\n","  'noise',\n","  'and',\n","  'diminishing',\n","  'performance',\n","  'of',\n","  'multimodal',\n","  'models',\n","  ',',\n","  'and',\n","  '2',\n","  ')',\n","  'Memes',\n","  'are',\n","  'more',\n","  'diverse',\n","  'than',\n","  '{',\n","  '`',\n","  '}',\n","  'traditional',\n","  'memes',\n","  '{',\n","  \"'\",\n","  '}',\n","  ',',\n","  'including',\n","  'screenshots',\n","  'of',\n","  'conversations',\n","  'or',\n","  'text',\n","  'on',\n","  'a',\n","  'plain',\n","  'background',\n","  '.',\n","  'This',\n","  'paper',\n","  'thus',\n","  'serves',\n","  'as',\n","  'a',\n","  'reality-check',\n","  'for',\n","  'the',\n","  'current',\n","  'benchmark',\n","  'of',\n","  'hateful',\n","  'meme',\n","  'detection',\n","  'and',\n","  'its',\n","  'applicability',\n","  'for',\n","  'detecting',\n","  'real',\n","  'world',\n","  'hate',\n","  '.'],\n"," ['Content',\n","  'moderation',\n","  'is',\n","  'often',\n","  'performed',\n","  'by',\n","  'a',\n","  'collaboration',\n","  'between',\n","  'humans',\n","  'and',\n","  'machine',\n","  'learning',\n","  'models',\n","  '.',\n","  'However',\n","  ',',\n","  'it',\n","  'is',\n","  'not',\n","  'well',\n","  'understood',\n","  'how',\n","  'to',\n","  'design',\n","  'the',\n","  'collaborative',\n","  'process',\n","  'so',\n","  'as',\n","  'to',\n","  'maximize',\n","  'the',\n","  'combined',\n","  'moderator-model',\n","  'system',\n","  'performance',\n","  '.',\n","  'This',\n","  'work',\n","  'presents',\n","  'a',\n","  'rigorous',\n","  'study',\n","  'of',\n","  'this',\n","  'problem',\n","  ',',\n","  'focusing',\n","  'on',\n","  'an',\n","  'approach',\n","  'that',\n","  'incorporates',\n","  'model',\n","  'uncertainty',\n","  'into',\n","  'the',\n","  'collaborative',\n","  'process',\n","  '.',\n","  'First',\n","  ',',\n","  'we',\n","  'introduce',\n","  'principled',\n","  'metrics',\n","  'to',\n","  'describe',\n","  'the',\n","  'performance',\n","  'of',\n","  'the',\n","  'collaborative',\n","  'system',\n","  'under',\n","  'capacity',\n","  'constraints',\n","  'on',\n","  'the',\n","  'human',\n","  'moderator',\n","  ',',\n","  'quantifying',\n","  'how',\n","  'efficiently',\n","  'the',\n","  'combined',\n","  'system',\n","  'utilizes',\n","  'human',\n","  'decisions',\n","  '.',\n","  'Using',\n","  'these',\n","  'metrics',\n","  ',',\n","  'we',\n","  'conduct',\n","  'a',\n","  'large',\n","  'benchmark',\n","  'study',\n","  'evaluating',\n","  'the',\n","  'performance',\n","  'of',\n","  'state-of-the-art',\n","  'uncertainty',\n","  'models',\n","  'under',\n","  'different',\n","  'collaborative',\n","  'review',\n","  'strategies',\n","  '.',\n","  'We',\n","  'find',\n","  'that',\n","  'an',\n","  'uncertainty-based',\n","  'strategy',\n","  'consistently',\n","  'outperforms',\n","  'the',\n","  'widely',\n","  'used',\n","  'strategy',\n","  'based',\n","  'on',\n","  'toxicity',\n","  'scores',\n","  ',',\n","  'and',\n","  'moreover',\n","  'that',\n","  'the',\n","  'choice',\n","  'of',\n","  'review',\n","  'strategy',\n","  'drastically',\n","  'changes',\n","  'the',\n","  'overall',\n","  'system',\n","  'performance',\n","  '.',\n","  'Our',\n","  'results',\n","  'demonstrate',\n","  'the',\n","  'importance',\n","  'of',\n","  'rigorous',\n","  'metrics',\n","  'for',\n","  'understanding',\n","  'and',\n","  'developing',\n","  'effective',\n","  'moderator-model',\n","  'systems',\n","  'for',\n","  'content',\n","  'moderation',\n","  ',',\n","  'as',\n","  'well',\n","  'as',\n","  'the',\n","  'utility',\n","  'of',\n","  'uncertainty',\n","  'estimation',\n","  'in',\n","  'this',\n","  'domain',\n","  '.']]\n","\n","\n","Spacy Tokenize:\n","[Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.,\n"," Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.,\n"," We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.,\n"," Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.,\n"," Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"0CbwIX8AQm9Y"},"source":["### Question 6B: Free response (5/8)"]},{"cell_type":"markdown","metadata":{"id":"hCVSKvDZGUDS"},"source":["White space tokenization will be useful if the sentences are simple. The major disadvantage with white space tokenization is that it considers punctuations along with the word like the full-stop punctuation is considered as a token along with the last word of a sentence. \n","\n","Word Tokenization looks better than white space tokenization. It is able to separate the punctuations where it is required.\n","\n","Spacy Tokenization looks similar to word tokenization except that it consumes more time to generate the results and the output is of Doc."]},{"cell_type":"markdown","metadata":{"id":"GlivQPJXf6m2"},"source":["# Question 7: Tabulating word counts under different algorithms (8 points)\n","\n","Now that you have compared and contrasted different tokenization algorithms, consider the effect that tokenization can have on our ability to characterize a corpus as a whole. \n","\n","Load in the `Counter` module and extract counts of all of the words under each of the three tokenizations schemes. Look at the top 5 most frequent (using the `.most_frequent()` method) and the top 10 least frequent (hint: use negative indices) words. In our data, what appear to be the biggest sources of disagreement? Do these confirm or disconfirm your hypotheses in the previous question? How or how not? "]},{"cell_type":"markdown","metadata":{"id":"WjwPTSJA_txp"},"source":["### Question 7A: Code (3/8)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiXEsEaVdJTB","executionInfo":{"status":"ok","timestamp":1652982892454,"user_tz":240,"elapsed":1305,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"28c92520-b142-4926-f716-6cca65ec8a47"},"source":["## Your code for question 6 goes here\n","from collections import Counter\n","\n","split_cnt = []\n","split_cnt_most_frequent = []\n","split_cnt_less_frequent = []\n","\n","for word in split_abstracts[0:5]:\n","    # split_cnt.append(Counter(word))\n","    # split_cnt_most_frequent.append(Counter(word).most_common(5))\n","    # split_cnt_less_frequent.append(Counter(word).most_common()[:-10-1:-1])\n","    split_cnt = split_cnt + word\n","split_cnt = Counter(split_cnt)\n","split_cnt_most_frequent = Counter(split_cnt).most_common(5)\n","split_cnt_less_frequent = Counter(split_cnt).most_common()[:-10-1:-1]\n","print('Split Tokenization Count')\n","pprint(split_cnt)\n","print('\\n')\n","\n","word_cnt = []\n","word_cnt_most_frequent = []\n","word_cnt_less_frequent = []\n","\n","for word in nltk_tokenized_abstracts[0:5]:\n","#     word_cnt.append(Counter(word))\n","#     word_cnt_most_frequent.append(Counter(word).most_common(5))\n","#     word_cnt_less_frequent.append(Counter(word).most_common()[:-10-1:-1])\n","    word_cnt = word_cnt + word\n","word_cnt = Counter(word_cnt)\n","word_cnt_most_frequent = Counter(word_cnt).most_common(5)\n","word_cnt_less_frequent = Counter(word_cnt).most_common()[:-10-1:-1]\n","print('Word Tokenization Count')\n","pprint(word_cnt)\n","print('\\n')\n","\n","spacy_cnt = {}\n","\n","for abstract in abstracts[0:5]:\n","  abstract_tokenized_into_words = spacy_model(abstract)\n","  for word in abstract_tokenized_into_words:\n","    # we have to change the previous examples\n","    # from *word* to *lemma*\n","    lemma = word.lemma_\n","    # print('lemma', lemma)\n","    if lemma not in spacy_cnt:\n","      spacy_cnt[lemma] = 1\n","    else:\n","      spacy_cnt[lemma] += 1\n","spacy_cnt = Counter(spacy_cnt)\n","spacy_cnt_most_frequent = Counter(spacy_cnt).most_common(5)\n","spacy_cnt_less_frequent = Counter(spacy_cnt).most_common()[:-10-1:-1]\n","print('Spacy Tokenization Count')\n","pprint(spacy_cnt)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Split Tokenization Count\n","Counter({'the': 35,\n","         'and': 23,\n","         'of': 19,\n","         'a': 17,\n","         'on': 13,\n","         'in': 13,\n","         'to': 12,\n","         'for': 10,\n","         'that': 9,\n","         'performance': 7,\n","         'this': 6,\n","         'we': 6,\n","         'is': 6,\n","         'We': 6,\n","         'detection': 5,\n","         'model': 5,\n","         'from': 5,\n","         'language': 4,\n","         'approach': 4,\n","         'as': 4,\n","         'collaborative': 4,\n","         'system': 4,\n","         'datasets': 3,\n","         'it': 3,\n","         'target': 3,\n","         'results': 3,\n","         'benchmark': 3,\n","         'with': 3,\n","         'than': 3,\n","         'tasks': 3,\n","         'In': 3,\n","         'representations': 3,\n","         'This': 3,\n","         'across': 3,\n","         'all': 3,\n","         'BERT': 3,\n","         'between': 3,\n","         'hateful': 3,\n","         'models': 3,\n","         'memes': 3,\n","         'uncertainty': 3,\n","         'strategy': 3,\n","         'due': 2,\n","         'its': 2,\n","         'work': 2,\n","         'bidirectional': 2,\n","         'based': 2,\n","         'methods': 2,\n","         'However,': 2,\n","         'such': 2,\n","         'large-scale': 2,\n","         'OLD': 2,\n","         'To': 2,\n","         'paper,': 2,\n","         'effective': 2,\n","         'domain': 2,\n","         'adaptation': 2,\n","         'Our': 2,\n","         'effectively': 2,\n","         'data': 2,\n","         'domain.': 2,\n","         'our': 2,\n","         'state-of-the-art': 2,\n","         'over': 2,\n","         'speech': 2,\n","         'their': 2,\n","         'similar': 2,\n","         'distant': 2,\n","         'zero-shot': 2,\n","         'tasks.': 2,\n","         'both': 2,\n","         'more': 2,\n","         'F1': 2,\n","         'introduce': 2,\n","         'abusive': 2,\n","         'dataset': 2,\n","         'English': 2,\n","         'offensive,': 2,\n","         'or': 2,\n","         'general': 2,\n","         'pre-trained': 2,\n","         'datasets,': 2,\n","         'outperforms': 2,\n","         'portability': 2,\n","         'by': 2,\n","         'Hateful': 2,\n","         'current': 2,\n","         'machine': 2,\n","         'learning': 2,\n","         'systems': 2,\n","         'Facebook': 2,\n","         'Memes': 2,\n","         'text': 2,\n","         'these': 2,\n","         '{`}memes': 2,\n","         'find': 2,\n","         'well': 2,\n","         'how': 2,\n","         'combined': 2,\n","         'moderator-model': 2,\n","         'performance.': 2,\n","         'rigorous': 2,\n","         'study': 2,\n","         'an': 2,\n","         'metrics': 2,\n","         'under': 2,\n","         'human': 2,\n","         'review': 2,\n","         'Offensive': 1,\n","         '(OLD)': 1,\n","         'has': 1,\n","         'received': 1,\n","         'increasing': 1,\n","         'attention': 1,\n","         'societal': 1,\n","         'impact.': 1,\n","         'Recent': 1,\n","         'shows': 1,\n","         'transformer': 1,\n","         'obtain': 1,\n","         'impressive': 1,\n","         'OLD.': 1,\n","         'usually': 1,\n","         'rely': 1,\n","         'well-labeled': 1,\n","         'training.': 1,\n","         'address': 1,\n","         'issue': 1,\n","         'data/label': 1,\n","         'scarcity': 1,\n","         'OLD,': 1,\n","         'propose': 1,\n","         'simple': 1,\n","         'yet': 1,\n","         'train': 1,\n","         'transformers.': 1,\n","         'introduces': 1,\n","         '(DA)': 1,\n","         'training': 1,\n","         'procedures': 1,\n","         'ALBERT,': 1,\n","         'can': 1,\n","         'exploit': 1,\n","         'auxiliary': 1,\n","         'source': 1,\n","         'domains': 1,\n","         'improve': 1,\n","         'Experimental': 1,\n","         'show': 1,\n","         'approach,': 1,\n","         'ALBERT': 1,\n","         '(DA),': 1,\n","         'obtains': 1,\n","         'most': 1,\n","         'cases.': 1,\n","         'Particularly,': 1,\n","         'significantly': 1,\n","         'benefits': 1,\n","         'underrepresented': 1,\n","         'under-performing': 1,\n","         'classes,': 1,\n","         'significant': 1,\n","         'improvement': 1,\n","         'ALBERT.': 1,\n","         'Hate': 1,\n","         'profanity': 1,\n","         'suffer': 1,\n","         'sparsity,': 1,\n","         'especially': 1,\n","         'languages': 1,\n","         'other': 1,\n","         'English,': 1,\n","         'subjective': 1,\n","         'nature': 1,\n","         'resulting': 1,\n","         'annotation': 1,\n","         'incompatibility': 1,\n","         'existing': 1,\n","         'corpora.': 1,\n","         'study,': 1,\n","         'identify': 1,\n","         'profane': 1,\n","         'subspaces': 1,\n","         'word': 1,\n","         'sentence': 1,\n","         'explore': 1,\n","         'generalization': 1,\n","         'capability': 1,\n","         'variety': 1,\n","         'setting.': 1,\n","         'done': 1,\n","         'monolingually': 1,\n","         '(German)': 1,\n","         'cross-lingually': 1,\n","         'closely-related': 1,\n","         '(English),': 1,\n","         'distantly-related': 1,\n","         '(French)': 1,\n","         'non-related': 1,\n","         '(Arabic)': 1,\n","         'observe': 1,\n","         'that,': 1,\n","         'languages,': 1,\n","         'subspace-based': 1,\n","         'transfer': 1,\n","         'standard': 1,\n","         'setting,': 1,\n","         'improvements': 1,\n","         '+10.9': 1,\n","         '+42.9': 1,\n","         'baselines': 1,\n","         'tested': 1,\n","         'monolingual': 1,\n","         'cross-lingual': 1,\n","         'scenarios.': 1,\n","         'HateBERT,': 1,\n","         're-trained': 1,\n","         'English.': 1,\n","         'The': 1,\n","         'was': 1,\n","         'trained': 1,\n","         'RAL-E,': 1,\n","         'Reddit': 1,\n","         'comments': 1,\n","         'communities': 1,\n","         'banned': 1,\n","         'being': 1,\n","         'abusive,': 1,\n","         'have': 1,\n","         'curated': 1,\n","         'made': 1,\n","         'available': 1,\n","         'public.': 1,\n","         'present': 1,\n","         'detailed': 1,\n","         'comparison': 1,\n","         'retrained': 1,\n","         'version': 1,\n","         'three': 1,\n","         'hate': 1,\n","         'HateBERT': 1,\n","         'corresponding': 1,\n","         'model.': 1,\n","         'also': 1,\n","         'discuss': 1,\n","         'battery': 1,\n","         'experiments': 1,\n","         'comparing': 1,\n","         'fine-tuned': 1,\n","         'suggesting': 1,\n","         'affected': 1,\n","         'compatibility': 1,\n","         'annotated': 1,\n","         'phenomena.': 1,\n","         'pose': 1,\n","         'unique': 1,\n","         'challenge': 1,\n","         'because': 1,\n","         'message': 1,\n","         'derived': 1,\n","         'text-': 1,\n","         'visual-modalities.': 1,\n","         'effect,': 1,\n","         'released': 1,\n","         'Challenge,': 1,\n","         'pre-extracted': 1,\n","         'captions,': 1,\n","         'but': 1,\n","         'unclear': 1,\n","         'whether': 1,\n","         'synthetic': 1,\n","         'examples': 1,\n","         'generalize': 1,\n","         \"wild{'}.\": 1,\n","         'collect': 1,\n","         'non-hateful': 1,\n","         'Pinterest': 1,\n","         'evaluate': 1,\n","         'out-of-sample': 1,\n","         'dataset.': 1,\n","         \"wild{'}\": 1,\n","         'differ': 1,\n","         'two': 1,\n","         'key': 1,\n","         'aspects:': 1,\n","         '1)': 1,\n","         'Captions': 1,\n","         'must': 1,\n","         'be': 1,\n","         'extracted': 1,\n","         'via': 1,\n","         'OCR,': 1,\n","         'injecting': 1,\n","         'noise': 1,\n","         'diminishing': 1,\n","         'multimodal': 1,\n","         'models,': 1,\n","         '2)': 1,\n","         'are': 1,\n","         'diverse': 1,\n","         '{`}traditional': 1,\n","         \"memes{'},\": 1,\n","         'including': 1,\n","         'screenshots': 1,\n","         'conversations': 1,\n","         'plain': 1,\n","         'background.': 1,\n","         'paper': 1,\n","         'thus': 1,\n","         'serves': 1,\n","         'reality-check': 1,\n","         'meme': 1,\n","         'applicability': 1,\n","         'detecting': 1,\n","         'real': 1,\n","         'world': 1,\n","         'hate.': 1,\n","         'Content': 1,\n","         'moderation': 1,\n","         'often': 1,\n","         'performed': 1,\n","         'collaboration': 1,\n","         'humans': 1,\n","         'models.': 1,\n","         'not': 1,\n","         'understood': 1,\n","         'design': 1,\n","         'process': 1,\n","         'so': 1,\n","         'maximize': 1,\n","         'presents': 1,\n","         'problem,': 1,\n","         'focusing': 1,\n","         'incorporates': 1,\n","         'into': 1,\n","         'process.': 1,\n","         'First,': 1,\n","         'principled': 1,\n","         'describe': 1,\n","         'capacity': 1,\n","         'constraints': 1,\n","         'moderator,': 1,\n","         'quantifying': 1,\n","         'efficiently': 1,\n","         'utilizes': 1,\n","         'decisions.': 1,\n","         'Using': 1,\n","         'metrics,': 1,\n","         'conduct': 1,\n","         'large': 1,\n","         'evaluating': 1,\n","         'different': 1,\n","         'strategies.': 1,\n","         'uncertainty-based': 1,\n","         'consistently': 1,\n","         'widely': 1,\n","         'used': 1,\n","         'toxicity': 1,\n","         'scores,': 1,\n","         'moreover': 1,\n","         'choice': 1,\n","         'drastically': 1,\n","         'changes': 1,\n","         'overall': 1,\n","         'demonstrate': 1,\n","         'importance': 1,\n","         'understanding': 1,\n","         'developing': 1,\n","         'content': 1,\n","         'moderation,': 1,\n","         'utility': 1,\n","         'estimation': 1})\n","\n","\n","Word Tokenization Count\n","Counter({',': 36,\n","         'the': 35,\n","         '.': 28,\n","         'and': 23,\n","         'of': 19,\n","         'a': 17,\n","         'on': 13,\n","         'in': 13,\n","         'to': 12,\n","         'that': 10,\n","         'for': 10,\n","         ')': 9,\n","         'performance': 9,\n","         '(': 7,\n","         'model': 6,\n","         'this': 6,\n","         'we': 6,\n","         'is': 6,\n","         'We': 6,\n","         'memes': 6,\n","         '{': 6,\n","         '}': 6,\n","         'detection': 5,\n","         'OLD': 5,\n","         'datasets': 5,\n","         'approach': 5,\n","         'from': 5,\n","         'English': 5,\n","         'tasks': 5,\n","         'models': 5,\n","         'language': 4,\n","         'domain': 4,\n","         'as': 4,\n","         'collaborative': 4,\n","         'system': 4,\n","         'paper': 3,\n","         'ALBERT': 3,\n","         'it': 3,\n","         'target': 3,\n","         'results': 3,\n","         'benchmark': 3,\n","         'with': 3,\n","         'than': 3,\n","         'In': 3,\n","         'study': 3,\n","         'representations': 3,\n","         'This': 3,\n","         'across': 3,\n","         'all': 3,\n","         'BERT': 3,\n","         'between': 3,\n","         'abusive': 3,\n","         'dataset': 3,\n","         'hateful': 3,\n","         '`': 3,\n","         \"'\": 3,\n","         'uncertainty': 3,\n","         'metrics': 3,\n","         'strategy': 3,\n","         'due': 2,\n","         'its': 2,\n","         'work': 2,\n","         'bidirectional': 2,\n","         'based': 2,\n","         'methods': 2,\n","         'However': 2,\n","         'such': 2,\n","         'large-scale': 2,\n","         'training': 2,\n","         'To': 2,\n","         'effective': 2,\n","         'adaptation': 2,\n","         'Our': 2,\n","         'DA': 2,\n","         'effectively': 2,\n","         'data': 2,\n","         'our': 2,\n","         'state-of-the-art': 2,\n","         'over': 2,\n","         'speech': 2,\n","         'languages': 2,\n","         'their': 2,\n","         'similar': 2,\n","         'distant': 2,\n","         'zero-shot': 2,\n","         'setting': 2,\n","         'both': 2,\n","         'more': 2,\n","         'F1': 2,\n","         'introduce': 2,\n","         'HateBERT': 2,\n","         'offensive': 2,\n","         'or': 2,\n","         'general': 2,\n","         'pre-trained': 2,\n","         'hate': 2,\n","         'outperforms': 2,\n","         'portability': 2,\n","         'by': 2,\n","         'Hateful': 2,\n","         'current': 2,\n","         'machine': 2,\n","         'learning': 2,\n","         'systems': 2,\n","         'Facebook': 2,\n","         'Memes': 2,\n","         'text': 2,\n","         'these': 2,\n","         'wild': 2,\n","         'find': 2,\n","         'moderation': 2,\n","         'well': 2,\n","         'how': 2,\n","         'process': 2,\n","         'combined': 2,\n","         'moderator-model': 2,\n","         'rigorous': 2,\n","         'an': 2,\n","         'under': 2,\n","         'human': 2,\n","         'review': 2,\n","         'Offensive': 1,\n","         'has': 1,\n","         'received': 1,\n","         'increasing': 1,\n","         'attention': 1,\n","         'societal': 1,\n","         'impact': 1,\n","         'Recent': 1,\n","         'shows': 1,\n","         'transformer': 1,\n","         'obtain': 1,\n","         'impressive': 1,\n","         'usually': 1,\n","         'rely': 1,\n","         'well-labeled': 1,\n","         'address': 1,\n","         'issue': 1,\n","         'data/label': 1,\n","         'scarcity': 1,\n","         'propose': 1,\n","         'simple': 1,\n","         'yet': 1,\n","         'train': 1,\n","         'transformers': 1,\n","         'introduces': 1,\n","         'procedures': 1,\n","         'can': 1,\n","         'exploit': 1,\n","         'auxiliary': 1,\n","         'source': 1,\n","         'domains': 1,\n","         'improve': 1,\n","         'Experimental': 1,\n","         'show': 1,\n","         'obtains': 1,\n","         'most': 1,\n","         'cases': 1,\n","         'Particularly': 1,\n","         'significantly': 1,\n","         'benefits': 1,\n","         'underrepresented': 1,\n","         'under-performing': 1,\n","         'classes': 1,\n","         'significant': 1,\n","         'improvement': 1,\n","         'Hate': 1,\n","         'profanity': 1,\n","         'suffer': 1,\n","         'sparsity': 1,\n","         'especially': 1,\n","         'other': 1,\n","         'subjective': 1,\n","         'nature': 1,\n","         'resulting': 1,\n","         'annotation': 1,\n","         'incompatibility': 1,\n","         'existing': 1,\n","         'corpora': 1,\n","         'identify': 1,\n","         'profane': 1,\n","         'subspaces': 1,\n","         'word': 1,\n","         'sentence': 1,\n","         'explore': 1,\n","         'generalization': 1,\n","         'capability': 1,\n","         'variety': 1,\n","         'done': 1,\n","         'monolingually': 1,\n","         'German': 1,\n","         'cross-lingually': 1,\n","         'closely-related': 1,\n","         'distantly-related': 1,\n","         'French': 1,\n","         'non-related': 1,\n","         'Arabic': 1,\n","         'observe': 1,\n","         'subspace-based': 1,\n","         'transfer': 1,\n","         'standard': 1,\n","         'improvements': 1,\n","         '+10.9': 1,\n","         '+42.9': 1,\n","         'baselines': 1,\n","         'tested': 1,\n","         'monolingual': 1,\n","         'cross-lingual': 1,\n","         'scenarios': 1,\n","         're-trained': 1,\n","         'The': 1,\n","         'was': 1,\n","         'trained': 1,\n","         'RAL-E': 1,\n","         'Reddit': 1,\n","         'comments': 1,\n","         'communities': 1,\n","         'banned': 1,\n","         'being': 1,\n","         'have': 1,\n","         'curated': 1,\n","         'made': 1,\n","         'available': 1,\n","         'public': 1,\n","         'present': 1,\n","         'detailed': 1,\n","         'comparison': 1,\n","         'retrained': 1,\n","         'version': 1,\n","         'three': 1,\n","         'corresponding': 1,\n","         'also': 1,\n","         'discuss': 1,\n","         'battery': 1,\n","         'experiments': 1,\n","         'comparing': 1,\n","         'fine-tuned': 1,\n","         'suggesting': 1,\n","         'affected': 1,\n","         'compatibility': 1,\n","         'annotated': 1,\n","         'phenomena': 1,\n","         'pose': 1,\n","         'unique': 1,\n","         'challenge': 1,\n","         'because': 1,\n","         'message': 1,\n","         'derived': 1,\n","         'text-': 1,\n","         'visual-modalities': 1,\n","         'effect': 1,\n","         'released': 1,\n","         'Challenge': 1,\n","         'pre-extracted': 1,\n","         'captions': 1,\n","         'but': 1,\n","         'unclear': 1,\n","         'whether': 1,\n","         'synthetic': 1,\n","         'examples': 1,\n","         'generalize': 1,\n","         'collect': 1,\n","         'non-hateful': 1,\n","         'Pinterest': 1,\n","         'evaluate': 1,\n","         'out-of-sample': 1,\n","         'differ': 1,\n","         'two': 1,\n","         'key': 1,\n","         'aspects': 1,\n","         ':': 1,\n","         '1': 1,\n","         'Captions': 1,\n","         'must': 1,\n","         'be': 1,\n","         'extracted': 1,\n","         'via': 1,\n","         'OCR': 1,\n","         'injecting': 1,\n","         'noise': 1,\n","         'diminishing': 1,\n","         'multimodal': 1,\n","         '2': 1,\n","         'are': 1,\n","         'diverse': 1,\n","         'traditional': 1,\n","         'including': 1,\n","         'screenshots': 1,\n","         'conversations': 1,\n","         'plain': 1,\n","         'background': 1,\n","         'thus': 1,\n","         'serves': 1,\n","         'reality-check': 1,\n","         'meme': 1,\n","         'applicability': 1,\n","         'detecting': 1,\n","         'real': 1,\n","         'world': 1,\n","         'Content': 1,\n","         'often': 1,\n","         'performed': 1,\n","         'collaboration': 1,\n","         'humans': 1,\n","         'not': 1,\n","         'understood': 1,\n","         'design': 1,\n","         'so': 1,\n","         'maximize': 1,\n","         'presents': 1,\n","         'problem': 1,\n","         'focusing': 1,\n","         'incorporates': 1,\n","         'into': 1,\n","         'First': 1,\n","         'principled': 1,\n","         'describe': 1,\n","         'capacity': 1,\n","         'constraints': 1,\n","         'moderator': 1,\n","         'quantifying': 1,\n","         'efficiently': 1,\n","         'utilizes': 1,\n","         'decisions': 1,\n","         'Using': 1,\n","         'conduct': 1,\n","         'large': 1,\n","         'evaluating': 1,\n","         'different': 1,\n","         'strategies': 1,\n","         'uncertainty-based': 1,\n","         'consistently': 1,\n","         'widely': 1,\n","         'used': 1,\n","         'toxicity': 1,\n","         'scores': 1,\n","         'moreover': 1,\n","         'choice': 1,\n","         'drastically': 1,\n","         'changes': 1,\n","         'overall': 1,\n","         'demonstrate': 1,\n","         'importance': 1,\n","         'understanding': 1,\n","         'developing': 1,\n","         'content': 1,\n","         'utility': 1,\n","         'estimation': 1})\n","\n","\n","Spacy Tokenization Count\n","Counter({'the': 38,\n","         ',': 36,\n","         '-': 32,\n","         '.': 28,\n","         '-PRON-': 23,\n","         'and': 23,\n","         'of': 22,\n","         'a': 17,\n","         'in': 16,\n","         'to': 14,\n","         'on': 13,\n","         'model': 13,\n","         'that': 10,\n","         'for': 10,\n","         'be': 10,\n","         ')': 9,\n","         'performance': 9,\n","         'this': 9,\n","         'dataset': 8,\n","         'meme': 8,\n","         '(': 7,\n","         'language': 6,\n","         'system': 6,\n","         '{': 6,\n","         '}': 6,\n","         'detection': 5,\n","         'domain': 5,\n","         'approach': 5,\n","         'from': 5,\n","         'task': 5,\n","         'hateful': 5,\n","         'base': 4,\n","         'train': 4,\n","         'result': 4,\n","         'English': 4,\n","         'as': 4,\n","         'collaborative': 4,\n","         'uncertainty': 4,\n","         'strategy': 4,\n","         'offensive': 3,\n","         'OLD': 3,\n","         'large': 3,\n","         'well': 3,\n","         'datum': 3,\n","         'paper': 3,\n","         'introduce': 3,\n","         'ALBERT': 3,\n","         'target': 3,\n","         'benchmark': 3,\n","         'under': 3,\n","         'with': 3,\n","         'hate': 3,\n","         'than': 3,\n","         'study': 3,\n","         'representation': 3,\n","         'across': 3,\n","         'all': 3,\n","         'BERT': 3,\n","         'between': 3,\n","         'abusive': 3,\n","         'pre': 3,\n","         '`': 3,\n","         \"'\": 3,\n","         'human': 3,\n","         'moderator': 3,\n","         'metric': 3,\n","         'have': 2,\n","         'due': 2,\n","         'work': 2,\n","         'show': 2,\n","         'bidirectional': 2,\n","         'transformer': 2,\n","         'method': 2,\n","         'obtain': 2,\n","         'however': 2,\n","         'such': 2,\n","         'scale': 2,\n","         'label': 2,\n","         'old': 2,\n","         'training': 2,\n","         'effective': 2,\n","         'adaptation': 2,\n","         'DA': 2,\n","         'effectively': 2,\n","         'state': 2,\n","         'art': 2,\n","         'perform': 2,\n","         'improvement': 2,\n","         'over': 2,\n","         'speech': 2,\n","         'subspace': 2,\n","         'similar': 2,\n","         'distant': 2,\n","         'zero': 2,\n","         'shot': 2,\n","         'setting': 2,\n","         'cross': 2,\n","         'relate': 2,\n","         'non': 2,\n","         'both': 2,\n","         'more': 2,\n","         'F1': 2,\n","         'HateBERT': 2,\n","         'or': 2,\n","         'present': 2,\n","         'general': 2,\n","         'outperform': 2,\n","         'portability': 2,\n","         'by': 2,\n","         'current': 2,\n","         'machine': 2,\n","         'learning': 2,\n","         'Facebook': 2,\n","         'extract': 2,\n","         'text': 2,\n","         'caption': 2,\n","         'these': 2,\n","         'wild': 2,\n","         'evaluate': 2,\n","         'find': 2,\n","         'content': 2,\n","         'moderation': 2,\n","         'understand': 2,\n","         'how': 2,\n","         'process': 2,\n","         'combine': 2,\n","         'rigorous': 2,\n","         'an': 2,\n","         'use': 2,\n","         'review': 2,\n","         'receive': 1,\n","         'increase': 1,\n","         'attention': 1,\n","         'societal': 1,\n","         'impact': 1,\n","         'recent': 1,\n","         'impressive': 1,\n","         'usually': 1,\n","         'rely': 1,\n","         'address': 1,\n","         'issue': 1,\n","         '/': 1,\n","         'scarcity': 1,\n","         'propose': 1,\n","         'simple': 1,\n","         'yet': 1,\n","         'procedure': 1,\n","         'can': 1,\n","         'exploit': 1,\n","         'auxiliary': 1,\n","         'source': 1,\n","         'improve': 1,\n","         'experimental': 1,\n","         'most': 1,\n","         'case': 1,\n","         'particularly': 1,\n","         'significantly': 1,\n","         'benefit': 1,\n","         'underrepresented': 1,\n","         'class': 1,\n","         'significant': 1,\n","         'profanity': 1,\n","         'suffer': 1,\n","         'sparsity': 1,\n","         'especially': 1,\n","         'other': 1,\n","         'subjective': 1,\n","         'nature': 1,\n","         'annotation': 1,\n","         'incompatibility': 1,\n","         'exist': 1,\n","         'corpora': 1,\n","         'identify': 1,\n","         'profane': 1,\n","         'word': 1,\n","         'sentence': 1,\n","         'explore': 1,\n","         'generalization': 1,\n","         'capability': 1,\n","         'variety': 1,\n","         'do': 1,\n","         'monolingually': 1,\n","         'german': 1,\n","         'lingually': 1,\n","         'closely': 1,\n","         'distantly': 1,\n","         'French': 1,\n","         'related': 1,\n","         'arabic': 1,\n","         'observe': 1,\n","         'transfer': 1,\n","         'standard': 1,\n","         '+10.9': 1,\n","         '+42.9': 1,\n","         'baseline': 1,\n","         'test': 1,\n","         'monolingual': 1,\n","         'lingual': 1,\n","         'scenario': 1,\n","         're': 1,\n","         'ral': 1,\n","         'E': 1,\n","         'Reddit': 1,\n","         'comment': 1,\n","         'community': 1,\n","         'ban': 1,\n","         'curate': 1,\n","         'make': 1,\n","         'available': 1,\n","         'public': 1,\n","         'detailed': 1,\n","         'comparison': 1,\n","         'trained': 1,\n","         'retrain': 1,\n","         'version': 1,\n","         'three': 1,\n","         'english': 1,\n","         'corresponding': 1,\n","         'also': 1,\n","         'discuss': 1,\n","         'battery': 1,\n","         'experiment': 1,\n","         'compare': 1,\n","         'fine': 1,\n","         'tune': 1,\n","         'suggest': 1,\n","         'affect': 1,\n","         'compatibility': 1,\n","         'annotate': 1,\n","         'phenomenon': 1,\n","         'pose': 1,\n","         'unique': 1,\n","         'challenge': 1,\n","         'because': 1,\n","         'message': 1,\n","         'derive': 1,\n","         'text-': 1,\n","         'visual': 1,\n","         'modality': 1,\n","         'effect': 1,\n","         'release': 1,\n","         'Hateful': 1,\n","         'Memes': 1,\n","         'Challenge': 1,\n","         'but': 1,\n","         'unclear': 1,\n","         'whether': 1,\n","         'synthetic': 1,\n","         'example': 1,\n","         'generalize': 1,\n","         'collect': 1,\n","         'Pinterest': 1,\n","         'out': 1,\n","         'sample': 1,\n","         'differ': 1,\n","         'two': 1,\n","         'key': 1,\n","         'aspect': 1,\n","         ':': 1,\n","         '1': 1,\n","         'must': 1,\n","         'via': 1,\n","         'OCR': 1,\n","         'inject': 1,\n","         'noise': 1,\n","         'diminish': 1,\n","         'multimodal': 1,\n","         '2': 1,\n","         'diverse': 1,\n","         'traditional': 1,\n","         'include': 1,\n","         'screenshot': 1,\n","         'conversation': 1,\n","         'plain': 1,\n","         'background': 1,\n","         'thus': 1,\n","         'serve': 1,\n","         'reality': 1,\n","         'check': 1,\n","         'applicability': 1,\n","         'detect': 1,\n","         'real': 1,\n","         'world': 1,\n","         'often': 1,\n","         'collaboration': 1,\n","         'not': 1,\n","         'design': 1,\n","         'so': 1,\n","         'maximize': 1,\n","         'problem': 1,\n","         'focus': 1,\n","         'incorporate': 1,\n","         'into': 1,\n","         'first': 1,\n","         'principle': 1,\n","         'describe': 1,\n","         'capacity': 1,\n","         'constraint': 1,\n","         'quantify': 1,\n","         'efficiently': 1,\n","         'utilize': 1,\n","         'decision': 1,\n","         'conduct': 1,\n","         'different': 1,\n","         'consistently': 1,\n","         'widely': 1,\n","         'toxicity': 1,\n","         'score': 1,\n","         'moreover': 1,\n","         'choice': 1,\n","         'drastically': 1,\n","         'change': 1,\n","         'overall': 1,\n","         'demonstrate': 1,\n","         'importance': 1,\n","         'develop': 1,\n","         'utility': 1,\n","         'estimation': 1})\n"]}]},{"cell_type":"code","metadata":{"id":"-UJrv9AICONC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652982892455,"user_tz":240,"elapsed":5,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"ce651291-33d5-47e5-a80b-5f1a54beec2b"},"source":["print('Split Tokenization Most Count')\n","pprint(split_cnt_most_frequent)\n","print('\\n')\n","\n","print('Word Tokenization Most Count')\n","pprint(word_cnt_most_frequent)\n","print('\\n')\n","\n","print('Spacy Tokenization Most Count')\n","pprint(spacy_cnt_most_frequent)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Split Tokenization Most Count\n","[('the', 35), ('and', 23), ('of', 19), ('a', 17), ('on', 13)]\n","\n","\n","Word Tokenization Most Count\n","[(',', 36), ('the', 35), ('.', 28), ('and', 23), ('of', 19)]\n","\n","\n","Spacy Tokenization Most Count\n","[('the', 38), (',', 36), ('-', 32), ('.', 28), ('-PRON-', 23)]\n"]}]},{"cell_type":"code","metadata":{"id":"AsQoTUmUE1Y5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652982892455,"user_tz":240,"elapsed":4,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"b4a52721-5e35-4db0-fcd2-d4b9ca7a51bf"},"source":["print('Split Tokenization Less Count')\n","pprint(split_cnt_less_frequent)\n","print('\\n')\n","\n","print('Word Tokenization Less Count')\n","pprint(word_cnt_less_frequent)\n","print('\\n')\n","\n","print('Spacy Tokenization Less Count')\n","pprint(spacy_cnt_less_frequent)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Split Tokenization Less Count\n","[('estimation', 1),\n"," ('utility', 1),\n"," ('moderation,', 1),\n"," ('content', 1),\n"," ('developing', 1),\n"," ('understanding', 1),\n"," ('importance', 1),\n"," ('demonstrate', 1),\n"," ('overall', 1),\n"," ('changes', 1)]\n","\n","\n","Word Tokenization Less Count\n","[('estimation', 1),\n"," ('utility', 1),\n"," ('content', 1),\n"," ('developing', 1),\n"," ('understanding', 1),\n"," ('importance', 1),\n"," ('demonstrate', 1),\n"," ('overall', 1),\n"," ('changes', 1),\n"," ('drastically', 1)]\n","\n","\n","Spacy Tokenization Less Count\n","[('estimation', 1),\n"," ('utility', 1),\n"," ('develop', 1),\n"," ('importance', 1),\n"," ('demonstrate', 1),\n"," ('overall', 1),\n"," ('change', 1),\n"," ('drastically', 1),\n"," ('choice', 1),\n"," ('moreover', 1)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bud1U9iM_vgh"},"source":["### Question 7B: Free response (5/8)"]},{"cell_type":"markdown","metadata":{"id":"PTWirBBEGPjO"},"source":["The above word count results for all the 3 tokenizations show that Spacy Tokenization produce better results when compared with the word tokenization and white space tokenization. And applying lemmatization on spacy tokenization helped to produce better results as lemmatization converts every word into its base form. And it looks like spacy tokenization understands about text formation than the word tokenization."]},{"cell_type":"markdown","metadata":{"id":"5FvuYyre5F_P"},"source":["# Question 8: Tabulating pointwise mutual information under different tokenization schemes: 8 points\n","\n","Mutual information is a computation that is very similar to computing a conditional probability. Recall that computing a conditional probability, defined below, requires knowing two probabilities. The first, $p(A \\cap B)$, is the probability of observing $A$ and $B$ at the same time. The second, $p(A)$, is the probability of observing $A$ across all contexts.\n","\n","Recall that we can approximate all of these by their frequencies in a corpus. For example, $p(A)$ can be approximated by:\n","\n","<center> $\\large p(A) \\approx \\frac{count(A)}{\\sum_{w \\in V}count(w)}$ </center>\n","\n","A conditional probability like $p(B | A)$ is a measure that allows us to estimate how many of our observations of $B$ occur having already seen $A$.\n","\n","<center>$\\large p(B | A) = \\frac{p(A \\cap B)}{p(A)}$</center>\n","\n","Mutual information is very similar, but requires dividing the co-occurence statistic by two probabilities $p(A)$ and $p(B)$.\n","\n","<center>$\\large MI = \\frac{p(A \\cap B)}{p(A) \\cdot p(B)}$</center>\n","\n","<hr />\n","\n","This question contains multiple parts to respond to.\n","\n","1. Compute the bigram frequencies of all words in our `abstracts.tsv` corpus. You may use whatever tokenization scheme you think performs the best.\n","2. Pick one of your tokenized abstracts from Question 5 that you think sounds interesting.\n","3. For each of the bigrams in that abstracts, compute the mutual information of that bigram and print the bigram and its mutual information value to the notebook.\n","4. Answer the questions in the free response section.\n"]},{"cell_type":"markdown","metadata":{"id":"7pXgmepfDDAm"},"source":["### Question 8A: Computing mutual information for bigrams in one sentence (5 points)"]},{"cell_type":"code","metadata":{"id":"PXPXWtAfCRGy","executionInfo":{"status":"ok","timestamp":1652982931843,"user_tz":240,"elapsed":39390,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}}},"source":["## your code for question 7 goes here\n","abstracts_bigram_dict = {}\n","bigram_tuple = []\n","for abstract in abstracts:\n","  abstract_token = word_tokenize(abstract)\n","  for i in range(len(abstract_token)-1):\n","    current_word = abstract_token[i]\n","    next_word = abstract_token[i+1]\n","    if current_word not in abstracts_bigram_dict:\n","      abstracts_bigram_dict[current_word] = {next_word : 1}\n","    else:\n","      if next_word not in abstracts_bigram_dict[current_word]:\n","        abstracts_bigram_dict[current_word][next_word] = 1\n","      else:\n","           abstracts_bigram_dict[current_word][next_word] += 1\n","    bigram_tuple.append((current_word, next_word))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cR8zPoUTHURq","executionInfo":{"status":"ok","timestamp":1652982931844,"user_tz":240,"elapsed":39,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"4067029b-b113-4bfd-c8b3-532679d3b3e7"},"source":["print(len(bigram_tuple))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["4214384\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjXbgp9y7Pbt","executionInfo":{"status":"ok","timestamp":1652985212432,"user_tz":240,"elapsed":2280616,"user":{"displayName":"Aditya Athota","userId":"12252802024110680972"}},"outputId":"388ac7b2-359f-4d4c-f5ca-3382fda65347"},"source":["total_pairs = 0\n","max_value = 0\n","min_value = 1\n","max_w1 = ''\n","max_w2 = ''\n","min_w1 = ''\n","min_w2 = ''\n","for current_word, next_word in bigram_tuple[0:1000000]:\n","    total_pairs += sum(abstracts_bigram_dict[current_word].values())\n","    freq_w1 = abstract_counts_dict[current_word]\n","    freq_w2 = abstract_counts_dict[next_word]\n","    total_count_of_completions = sum(abstract_counts_dict.values())\n","    p_w1 = freq_w1 / total_count_of_completions\n","    p_w2 = freq_w2 / total_count_of_completions\n","    total_w1_w2 = abstracts_bigram_dict[current_word][next_word]\n","    p_w1_and_w2 = total_w1_w2 / total_pairs\n","    MI = p_w1_and_w2 / (p_w1 * p_w2)\n","    if MI > max_value:\n","        max_value = MI\n","        max_w1 = current_word\n","        max_w2 = next_word\n","    if MI < min_value:\n","        min_value = MI\n","        min_w1 = current_word\n","        min_w2 = next_word\n","    print(current_word, next_word, MI)\n","\n","print('The max and min values are {} and {} and their tokens are {}_{} and {}_{}'.format(max_value, min_value, max_w1, max_w2, min_w1, min_w2))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","della Ghigliottina 133.69162190703855\n","Ghigliottina can 0.010532704789344137\n","can also 0.0017079231090073668\n","also generate 0.00045936450011037344\n","generate new 0.001305576403729583\n","new game 0.0006475490121722775\n","game instances 0.0032097260877491604\n","instances and 0.0002456549237573767\n","and challenge 5.546037543443161e-05\n","challenge the 1.8515847285554988e-05\n","the users 0.0002017575789365827\n","users to 0.0009264226822587356\n","to match 0.0015550482581180261\n","match the 0.0008308600014280398\n","the solution 0.0001293070632508548\n","solution . 0.000248120144758863\n","Gamification has 0.024368097943235984\n","has been 0.03409953642992584\n","been applied 0.007460501632463782\n","applied to 0.003600605872534376\n","to many 0.0002481277561948249\n","many linguistic 0.00043268200323977584\n","linguistic annotation 0.0024736928777885163\n","annotation tasks 0.00069421113982691\n","tasks , 0.0006075474333182867\n",", as 0.00030565071441362865\n","as an 0.0010958655157710076\n","an alternative 0.010959618064460351\n","alternative to 0.0013340727044509658\n","to crowdsourcing 0.00011182907400161346\n","crowdsourcing platforms 0.050471129293792785\n","platforms to 0.00024862906708871983\n","to collect 0.002915602854630982\n","collect annotated 0.0008590927354785637\n","annotated data 0.003921585885029133\n","data in 0.0002592354239646954\n","in an 0.0005010107027946518\n","an inexpensive 0.0058027335760213955\n","inexpensive way 0.03469392157948772\n","way . 0.00047209766807085985\n",". However 0.0033745490517358874\n","However , 0.0032924048202252025\n",", we 0.0025202318350003353\n","we think 0.00436853615265242\n","think that 0.0035668608098389735\n","that still 0.00011393686435303842\n","still much 0.002747975246411657\n","much has 0.0002822481665394243\n","has to 0.00010307369777133092\n","to be 0.001540189805075783\n","be explored 0.002316899539741797\n","explored . 0.0005628479422009971\n",". Games 0.00037523049571196355\n","Games with 0.004522415709934796\n","with a 0.0007614785727644001\n","a Purpose 0.0015950702440591285\n","Purpose ( 0.0077841173979886005\n","( GWAPs 0.009340931342579054\n","GWAPs ) 0.00901774388564126\n",") tend 0.00025545989678611025\n","tend to 0.006503072992730784\n","to lack 0.0001836616058332117\n","lack important 0.0007209088415237584\n","important elements 0.0024169782877660052\n","elements that 0.0006023099507329158\n","that we 0.00025688607893653624\n","we commonly 4.774271063205856e-05\n","commonly see 0.007771395823780694\n","see in 0.0002510707979320182\n","in commercial 0.00042694580400536665\n","commercial games 0.023571070155026415\n","games , 0.0006446894119317674\n",", such 0.0007826954575276049\n","such as 0.014373075456708873\n","as 2D 0.0007081019727767598\n","2D and 0.0006040529069384252\n","and 3D 0.0006606808805633029\n","3D worlds 0.202548740944808\n","worlds or 0.0022018887198581773\n","or a 0.00017654828886218588\n","a story 0.0011973961358914234\n","story . 0.00045405001382440083\n",". Making 0.0022513225244860427\n","Making GWAPs 5.092620194210639\n","GWAPs more 0.01722419456636052\n","more similar 0.002342588664103563\n","similar to 0.0011420403893981856\n","to full-fledged 0.0007479731166646753\n","full-fledged video 0.11718688078921952\n","video games 0.055509574275971335\n","games in 0.0004968461630988682\n","in order 0.00486007330058851\n","order to 0.004953867491036783\n","to involve 0.0004305176305299078\n","involve users 0.001608301122658332\n","users more 0.00013365813039848103\n","more easily 0.004618442705149741\n","easily and 0.0001750676248294347\n","and increase 0.0001540211998477748\n","increase dissemination 0.01782874655226253\n","dissemination is 0.00032482933929985303\n","is a 0.0007170569711673788\n","a demanding 0.0008232314803544295\n","demanding yet 0.02510766140335627\n","yet interesting 0.0018400413418759585\n","interesting ground 0.004734488336997266\n","ground to 0.00015127379431666913\n","to explore 0.001208120615552247\n","explore . 1.496423804178578e-05\n",". In 0.002737860754873893\n","In this 0.01435118372285273\n","this paper 0.013152369768956599\n","paper we 0.001756190679155444\n","we present 0.006393694797848444\n","present a 0.0023145421614496073\n","a 3D 0.0013774936403916252\n","3D role-playing 0.6751410479119864\n","role-playing game 0.6675551932625242\n","game for 0.0001743457082201166\n","for abusive 0.0006681852708267671\n","abusive language 0.015815499190491387\n","language annotation 6.502130862088406e-05\n","annotation that 6.1023103061190545e-05\n","that is 0.0004305557122271526\n","is currently 0.003998115411288864\n","currently under 0.023434294366554226\n","under development 0.012587942445196735\n","development . 0.0002179246534703145\n","In this 0.014351048166386788\n","this paper 0.013152245536169204\n","paper we 0.001756174090785708\n","we present 0.006393634405273711\n","present a 0.0023145202991091234\n","a new 0.0031332310761888024\n","new method 0.0017444382002930723\n","method for 0.0016610357765506681\n","for collecting 0.0021849898295974828\n","collecting naturally 0.009326804809511806\n","naturally generated 0.0013378613363235364\n","generated dialogue 0.0007271998532169943\n","dialogue data 0.0005924230565338802\n","data for 0.00047260246035255954\n","for a 0.0002529904798049389\n","a low 0.0004819957346352022\n","low resourced 0.16788173980916407\n","resourced language 0.005501346167354799\n","language , 0.00016399640859892804\n",", ( 5.1641537634928326e-05\n","( specifically 0.0003107212771759754\n","specifically here 0.0024936108106415624\n","here { 5.477059434926587e-05\n","{ -- 0.007518299771018795\n","-- - 0.11639653896147273\n","- } 0.003632417494282886\n","} Uyghur 0.00015321383287201\n","Uyghur ) 0.00029467577373477205\n",") . 0.0005723080441533152\n",". We 0.002897941738318546\n","We plan 0.0027260949707356985\n","plan to 0.002731579203900502\n","to build 0.0035443459487306244\n","build a 0.002381482563032689\n","a games 5.596259266880843e-05\n","games with 0.00178500564708347\n","with a 0.0007614109849267596\n","a purpose 0.00014417434287661637\n","purpose ( 0.00017589664115122688\n","( GWAPs 0.009340102111815726\n","GWAPs ) 0.009016943345484134\n",") to 0.00017973401330219894\n","to encourage 0.003683167816950822\n","encourage native 0.006196836977055869\n","native speakers 0.21668004773036784\n","speakers to 0.000183581084244746\n","to actively 0.0008975054044818144\n","actively contribute 0.034045628796022936\n","contribute dialogue 0.0005931294153822595\n","dialogue data 0.000592401901291201\n","data to 0.00018942637859801624\n","to our 9.019203845044248e-05\n","our research 0.0006620503513815745\n","research project 0.003024381037554815\n","project . 0.00044880572172538295\n",". Since 0.003100438845643314\n","Since we 0.000476513101281437\n","we aim 0.0038082968513310786\n","aim to 0.002740861923438259\n","to characterize 0.002742348294590075\n","characterize the 0.001577715186058052\n","the response 0.0003403911224170581\n","response space 0.0014707347784540918\n","space of 0.0004538412950205647\n","of queries 0.0002777108553833607\n","queries in 0.00043360474238284436\n","in Uyghur 0.00018508285904983034\n","Uyghur , 0.0004584728625641236\n",", we 0.0025198958158252362\n","we design 0.002552959241374088\n","design various 0.0004898528770444742\n","various scenarios 0.0018592334857683306\n","scenarios for 0.00026217914977705954\n","for conversations 6.290110722664465e-05\n","conversations that 0.0003084436617066773\n","that yield 0.0008800148242125821\n","yield to 2.185420437722135e-05\n","to questions 9.16007943473363e-05\n","questions being 0.0004248956428888384\n","being posed 0.006117922859600027\n","posed and 0.00014283417774636707\n","and responded 0.0007549783841440876\n","responded to 0.003846318940892062\n","to . 1.8278862295931302e-06\n",". We 0.0028977775613129556\n","We will 0.0015105466619389075\n","will implement 0.0008351089383369502\n","implement the 0.00025108746719451966\n","the GWAP 0.0007281501243632927\n","GWAP with 0.0022608789781708187\n","with the 0.000457781495832654\n","the RPG 0.0032766571133334994\n","RPG Maker 267.3293472404834\n","Maker MV 267.32934722459635\n","MV Game 35.643912962220384\n","Game Engine 1.2291004464252893\n","Engine , 0.0004607168623458115\n",", and 0.0005702772830310962\n","and will 0.00015296156288341155\n","will integrate 0.001267206042331764\n","integrate the 0.0006477062073287996\n","the chatroom 0.003276615515109581\n","chatroom system 0.05711482820377888\n","system in 0.00018653767881293603\n","in the 0.000700626532067899\n","the game 0.0004663316168003147\n","game with 0.0006096599015540185\n","with the 0.00045776998625430854\n","the Dialogue 0.0001341287901890985\n","Dialogue Experimental 0.002786613552525107\n","Experimental Toolkit 0.015371319406309519\n","Toolkit ( 0.008033828046113\n","( DiET 0.005188508651054164\n","DiET ) 0.005008991122024854\n",") . 0.000572251260945711\n",". DiET 0.0011254863513340371\n","DiET will 0.07265145161017153\n","will help 0.009875514562668495\n","help us 0.017879990468256758\n","us improve 0.0006206000273014252\n","improve the 0.0014514148248438523\n","the data 0.00022035591384904235\n","data collection 0.00835412950844398\n","collection process 0.004708913684847381\n","process , 0.0003769158556185783\n",", and 0.0005702563111494954\n","and most 9.977515721711095e-05\n","most importantly 0.02229952525639742\n","importantly , 0.0029612798351882274\n",", make 3.7285907002873136e-05\n","make us 0.0010165696236267704\n","us have 7.378887119398173e-05\n","have some 0.0006148518445968413\n","some control 0.0008156282478137968\n","control over 0.011451241419185215\n","over the 0.0008045162476913711\n","the interactions 0.0002822075648528677\n","interactions among 0.028436125032745646\n","among the 0.0006654551135462611\n","the participants 0.000488387182308293\n","participants . 0.0004641887963460112\n","In this 0.01434883015304362\n","this paper 0.013150212804065928\n","paper , 0.0013886570780471375\n",", we 0.002519662232950837\n","we present 0.006392615842064326\n","present the 0.0004341853785900125\n","the ongoing 0.0005346461538945649\n","ongoing development 0.00878694309514607\n","development of 0.0023579841266550827\n","of CALLIG 0.00231128342934448\n","CALLIG { 0.003906372644953483\n","{ -- 0.007517152066790617\n","-- } 0.004692746088805309\n","} a 1.9575828202663186e-05\n","a web 0.0009015752474193256\n","web system 0.0001213833445093944\n","system that 0.0007253138609569883\n","that uses 0.0028704304688197293\n","uses improvisation 0.061196660590017445\n","improvisation games 0.7815994544138689\n","games in 0.0004967442317561122\n","in Computer 0.0010486805021166267\n","Computer Assisted 4.37694955180378\n","Assisted Language 0.07037902155362684\n","Language Learning 0.02004236185626179\n","Learning ( 0.004483481337715737\n","( CALL 0.006485245858534105\n","CALL ) 0.006260862435427833\n",") . 0.000572216853450599\n",". Improvisation 0.003376256039954573\n","Improvisation games 4.689560406795463\n","games are 0.0011578589676834144\n","are structured 0.00038217059190225484\n","structured activities 0.0030374161892148953\n","activities with 0.00020761148744336268\n","with built-in 0.0018496281713986278\n","built-in constraints 0.03957722296408977\n","constraints where 0.00029818455113901475\n","where improvisers 0.18308529851530483\n","improvisers are 0.026399144321209128\n","are asked 0.005821346270760521\n","asked to 0.003796585360099408\n","to generate 0.004098437174265505\n","generate a 0.000808522520399222\n","a lot 0.005826706911192735\n","lot of 0.004111323613831232\n","of different 0.0004618202816847929\n","different ideas 0.0007496645578697672\n","ideas and 0.00045212545625028766\n","and weave 0.0010568400772431928\n","weave a 0.0012757336220640507\n","a diverse 0.0008658803927644727\n","diverse range 0.010370889559407414\n","range of 0.004186434790593134\n","of elements 8.537419139843788e-05\n","elements into 0.0008136237498951275\n","into a 0.0010130258559553984\n","a sensible 0.0019135839069419668\n","sensible narrative 0.095463917100841\n","narrative spontaneously 0.1735707569212212\n","spontaneously . 0.0003069255034738199\n",". This 0.002101516584594754\n","This paper 0.018810484288524783\n","paper discusses 0.021657886514018735\n","discusses how 0.012823001611654108\n","how computer-based 0.010560117839786349\n","computer-based language 0.0022917648597277755\n","language games 0.0011257786720671294\n","games can 0.0007389003454837165\n","can be 0.018626122839786013\n","be created 0.001565854788730806\n","created combining 0.0010159534998937772\n","combining improvisation 0.1484985339206077\n","improvisation elements 0.23509002727303802\n","elements and 0.00019519187573955175\n","and language 0.00011258300316430872\n","language technology 0.007013017342860731\n","technology . 0.00038127722816480364\n",". In 0.0027372272670515207\n","In contrast 0.019650955840796118\n","contrast with 0.0008934020207790475\n","with traditional 0.0009810047120900713\n","traditional language 0.0002354200309867427\n","language exercises 0.0013368487999264708\n","exercises , 0.0002783139000001131\n",", improvisational 0.003339750917471312\n","improvisational language 0.032084218571715\n","language games 0.0011257614978920666\n","games are 0.001157809368324198\n","are open 0.00016623449442736097\n","open and 0.00015306422328042454\n","and unpredictable 0.0005562134003811246\n","unpredictable . 0.00017768942834838432\n",". CALLIG 0.001688042977271323\n","CALLIG encourages 3.220379243098407\n","encourages spontaneity 6.440758470314316\n","spontaneity and 0.005284006652369338\n","and witty 0.0010567981539840413\n","witty language 0.006416772849758916\n","language use 0.0004715198162614365\n","use . 8.143563870831732e-05\n",". It 0.0031765588120882624\n","It also 0.00432542134781182\n","also provides 0.0045527448720533515\n","provides opportunities 0.005966946391135019\n","opportunities for 0.005160544352062253\n","for collecting 0.0021844678888412942\n","collecting useful 0.0033776331053307867\n","useful data 0.0003101226788449863\n","data for 0.00047248961260500474\n","for many 0.001456097664357184\n","many NLP 0.008682329055861337\n","NLP applications 0.016698011134841685\n","applications . 0.0010836487271638413\n","Although the 0.0005715411730166284\n","the roguelike 0.0016380698852059422\n","roguelike video 0.5271935686251321\n","video game 0.043439169795489486\n","game genre 0.008483689873502072\n","genre has 0.0003096575776272143\n","has a 0.0003261100630207137\n","a large 0.002733799819239139\n","large community 0.0007442752959816725\n","community of 8.907827105940377e-05\n","of fans 0.0009244313255160637\n","fans ( 0.0031126755707858687\n","( both 9.07640320846656e-05\n","both players 0.0010661978228675587\n","players and 0.0004915238344518038\n","and developers 0.0007732485861456273\n","developers ) 9.16148560002548e-05\n",") and 0.00037929615091667087\n","and the 0.0001826023795016372\n","the graphic 0.0008736202957372439\n","graphic aspect 0.05449173196961474\n","aspect of 0.001038911465737804\n","of these 0.000959180127506126\n","these games 0.0021418117314833357\n","games is 0.00013103280058066288\n","is usually 0.002531817522875307\n","usually given 0.0008825887567853159\n","given little 0.0004595692659950439\n","little relevance 0.0026815753085247486\n","relevance ( 0.00011499826428942942\n","( ASCII-based 0.007781541276320877\n","ASCII-based graphics 17.818691982879223\n","graphics are 0.0017597839094305105\n","are not 0.0030389920733438786\n","not rare 0.0005719042382082667\n","rare even 0.0014517416684703948\n","even today 0.00282590923342595\n","today ) 0.00011469161351319377\n",") , 0.0004650011030413554\n",", their 0.0001564499316055225\n","their accessibility 0.0028587469195195917\n","accessibility for 0.0005817105420681557\n","for blind 0.00019072450754794974\n","blind players 0.10189789456307825\n","players and 0.0004915110449711857\n","and other 0.0005649496538733187\n","other visually-impaired 0.030532024065245966\n","visually-impaired users 0.13827073907677265\n","users remains 0.0006476380849670843\n","remains a 0.0016729433256300803\n","a pending 0.0015945201370474206\n","pending issue 0.16767669660000087\n","issue . 0.00041086846827394165\n",". In 0.0027370248164769796\n","In this 0.01434680194416604\n","this document 8.980078820191411e-05\n","document , 0.00018073082126880525\n",", we 0.0025193069704407245\n","we describe 0.006220216769882418\n","describe an 0.0016725697158674955\n","an initiative 0.00457012577377496\n","initiative for 0.0005288180062889361\n","for the 0.00043421015720373444\n","the development 0.0014853930994764186\n","development of 0.002357647541340182\n","of roguelikes 0.0023109535113525746\n","roguelikes adapted 0.8457943588167424\n","adapted to 0.003066594149188528\n","to visually-impaired 0.0016823636535061948\n","visually-impaired players 1.5538975822450887\n","players by 0.0006764900210165828\n","by using 0.0020113741708973035\n","using Natural 0.0004913246446093453\n","Natural Language 0.2261583373166289\n","Language Processing 0.24263645535728007\n","Processing techniques 0.0045278023799561725\n","techniques , 0.000363376191152664\n",", together 0.0005419749073303917\n","together with 0.010271644005703918\n","with the 0.0004576753122519123\n","the first 0.0015422580766651342\n","first completed 0.002177685222407774\n","completed games 0.07814833534202262\n","games resulting 0.004998827403669225\n","resulting from 0.0016118057124400714\n","from it 0.00012767236494562315\n","it . 0.00011499656134657317\n",". These 0.003343281478542669\n","These games 0.0034706719979520585\n","games were 0.0020037939544246828\n","were developed 0.004275820948680123\n","developed as 0.0006042531563173795\n","as Bachelor 0.024774363529552763\n","Bachelor { 0.007811478576301729\n","{ ' 0.007645570205312644\n","' } 0.007645566782946891\n","} s 0.0064317536026585196\n","s and 1.8663787739389503e-05\n","and Master 0.0013208647472647645\n","Master { 0.0019528545859927374\n","{ ' 0.007645511252379089\n","' } 0.007645507830066113\n","} s 0.0064317040092609495\n","s theses 0.016419137792280755\n","theses . 0.0006751440291369482\n",". Our 0.0033287551063580383\n","Our approach 0.005210267306275991\n","approach consists 0.001824124257364527\n","consists in 0.000769056227841864\n","in integrating 7.015993970085775e-05\n","integrating a 0.00028450725508520326\n","a multilingual 0.000850977348973563\n","multilingual module 0.00040523442175089937\n","module that 0.000846775582691923\n","that , 4.606279055857407e-05\n",", apart 0.0005300497930727125\n","apart from 0.015690488681198902\n","from the 0.0006954257694535446\n","the classic 0.0006839525390577974\n","classic ASCII-based 2.93689219477497\n","ASCII-based graphical 1.6197405436887509\n","graphical interface 0.06738120628718085\n","interface , 0.0003472867416024863\n",", automatically 6.120271268913784e-05\n","automatically generates 0.011307923756171068\n","generates text 0.0007380506531422273\n","text descriptions 0.001969896850435385\n","descriptions of 0.0010726451968337098\n","of what 0.00045754539357131514\n","what is 0.001709329235252601\n","is happening 0.004978755257588559\n","happening within 0.02903678871977701\n","within the 0.0012321398029935527\n","the game 0.0004662080453220288\n","game . 0.00044249486292956557\n",". The 0.0029174195457316006\n","The visually-impaired 0.0072693968071109874\n","visually-impaired user 0.0564061005033501\n","user can 0.0008176728718377712\n","can then 0.0010334850600200829\n","then read 0.002964525407919671\n","read such 0.0003058152691814918\n","such descriptions 0.00029573819350159515\n","descriptions by 0.00016668801621586192\n","by means 0.010278481480264688\n","means of 0.002357086803363155\n","of a 0.000285197616167807\n","a screen 0.0006377448372232114\n","screen reader 0.11494630646211142\n","reader . 0.00034844532693232514\n",". In 0.0027367535820406216\n","In these 0.000144701533128627\n","these projects 0.0022079598814031182\n","projects we 6.769809811643304e-05\n","we seek 0.006573260970767994\n","seek expressivity 0.2052601343589109\n","expressivity and 0.0018868431827123952\n","and variety 6.41677534521783e-05\n","variety in 8.595594537235726e-05\n","in the 0.0007004221167262924\n","the descriptions 9.855069016573946e-05\n","descriptions , 0.0003157369576953941\n",", so 0.0008354747469893427\n","so we 0.0005913290948007728\n","we can 0.0007402989528947585\n","can offer 0.001765859576634606\n","offer the 0.00024302966487271216\n","the users 0.00020165367827093233\n","users a 2.969246720709905e-05\n","a fun 0.0027331141057543194\n","fun roguelike 38.17750437829088\n","roguelike experience 0.832531248075263\n","experience that 0.00017770143866316384\n","that does 0.0012867636129092585\n","does not 0.05893634002449436\n","not sacrifice 0.026511790659914557\n","sacrifice any 0.02829905900761897\n","any of 0.00015072733722656936\n","of the 0.0005979668579067642\n","the key 0.0005464822361290567\n","key characteristics 0.0051375415880502455\n","characteristics that 0.00042668917836227374\n","that define 0.000259870352820364\n","define the 0.0005894510968786219\n","the genre 0.00033310591371634764\n","genre . 0.0003861703158527162\n",". Moreover 0.0033635579201044766\n","Moreover , 0.0032921830338890528\n",", we 0.0025189487307147916\n","we intend 0.0075682742007047625\n","intend to 0.0058314384924044345\n","to make 0.0023680444377246657\n","make these 0.0006215472895493515\n","these projects 0.002207845248358219\n","projects easy 0.0052981832225378975\n","easy to 0.0030459233848702705\n","to extend 0.0020074890011154358\n","extend to 0.0002215160245114814\n","to other 0.0007148202002554356\n","other languages 0.006019289844936521\n","languages , 0.0005592271055707844\n",", thus 0.0013062766270017549\n","thus avoiding 0.026071456454469958\n","avoiding costly 0.07464592961544633\n","costly and 0.0007968528571319964\n","and complex 0.000267792893102828\n","complex solutions 0.0030201890388282192\n","solutions . 0.0005359965713879456\n",". KEYWORDS 0.0033753165822936063\n","KEYWORDS : 0.07075206895562926\n",": Natural 0.00036621145486735995\n","Natural Language 0.22612477656980334\n","Language Generation 0.10593674384248325\n","Generation , 0.00022049748607633746\n",", roguelikes 0.0016694730285686768\n","roguelikes , 0.0016694730284694999\n",", visually-impaired 0.0008347325455535491\n","visually-impaired users 0.13824518270645686\n","Increasing efforts 0.1781519587493042\n","efforts are 0.0008973136893266968\n","are put 0.0011996165101321454\n","put into 0.01012031788978641\n","into gamification 0.02569003375149586\n","gamification of 0.0011552896183932683\n","of experimentation 0.0001467029396414401\n","experimentation software 0.017207689401157692\n","software in 7.65542275544186e-05\n","in psychology 0.002291428220171922\n","psychology and 0.0007546741933120573\n","and educational 0.0006375676611950098\n","educational applications 0.023071292675031777\n","applications and 0.0001507832145089971\n","and the 0.0001825624097794712\n","the development 0.0014851235036383568\n","development of 0.0023572196331108092\n","of serious 0.0002453664509134969\n","serious games 0.12446320797677089\n","games . 0.0005329286535802636\n",". Computer-based 0.0016876008144188065\n","Computer-based experiments 0.036555603738246664\n","experiments with 0.0012910487360059263\n","with game-like 0.006779855527724335\n","game-like features 0.03415404214543912\n","features have 0.00044571542465632\n","have been 0.025625447358510776\n","been developed 0.011903846923850594\n","developed previously 0.0005299712955092776\n","previously for 4.067031049290382e-05\n","for research 0.0005765948829331145\n","research on 0.0013436130946448278\n","on cognitive 0.0002841451059981338\n","cognitive skills 0.006886235146122885\n","skills , 0.00046978588695693164\n",", cognitive 7.704968162768699e-05\n","cognitive processing 0.004158890000308734\n","processing speed 0.004581827607709777\n","speed , 0.00033010924313620955\n",", working 6.005041957711593e-05\n","working memory 0.02448955536688176\n","memory , 0.00017438335305213092\n",", attention 4.884838843752207e-05\n","attention , 0.0001257845900123385\n",", learning 2.5047375454844314e-05\n","learning , 0.00017313445091439702\n",", problem 2.79940294480915e-06\n","problem solving 0.0127662331332337\n","solving , 2.8536360692483638e-05\n",", group 5.123383623713632e-05\n","group behavior 0.0025788986174739635\n","behavior and 0.0003787414752915836\n","and other 0.0005648109931374308\n","other phenomena 0.0011931412500446532\n","phenomena . 0.0005167058647454147\n",". It 0.0031756228378309608\n","It has 0.005053504742733413\n","has been 0.03407807503067876\n","been argued 0.02668079358683215\n","argued that 0.009845386258066538\n","that computer 4.933847542365816e-05\n","computer game 0.027703461318439836\n","game experiments 0.001095253113206255\n","experiments are 0.0006642584206457416\n","are superior 0.0012566579022564067\n","superior to 0.001700467398344512\n","to traditional 0.000394930335330106\n","traditional computerized 0.031121528211708653\n","computerized experimentation 0.471268855525354\n","experimentation methods 0.0013847272908233143\n","methods in 0.0003265026186862135\n","in laboratory 0.002177233320452612\n","laboratory tasks 0.0019272887023107292\n","tasks in 0.0003874719142309439\n","in that 3.624842630706661e-05\n","that they 0.0017943323722951607\n","they represent 0.0039037619705423955\n","represent holistic 0.012085396169260973\n","holistic , 0.0002225788280199102\n",", meaningful 4.511711551460296e-05\n","meaningful , 0.0001353513453538816\n",", and 0.0005700159397034384\n","and natural 0.0002376521300949812\n","natural human 0.00015588341719925756\n","human activity 0.004128403194621031\n","activity . 0.0003355253147688001\n",". We 0.002896382113216603\n","We present 0.007347712582448289\n","present a 0.002313216401865685\n","a novel 0.00478592709898546\n","novel experimental 0.000571649483909599\n","experimental framework 0.0010605325053931378\n","framework for 0.0018659400714478868\n","for forced 0.0005409717045582546\n","forced choice 0.034522259552214336\n","choice categorization 0.007422285724366585\n","categorization tasks 0.0018790596658931583\n","tasks or 0.0001949857672919036\n","or speech 0.00033095903759683464\n","speech perception 0.007245742094724445\n","perception studies 0.0020259161705339333\n","studies in 0.0004169573062048689\n","in the 0.000700302185418641\n","the form 0.0009367576459848444\n","form of 0.0020816624408685606\n","of a 0.0002851427882444538\n","a computer 0.0005074502208555002\n","computer game 0.02770227005053002\n","game , 0.00032510379262102413\n",", based 0.0002145366878510216\n","based on 0.010647440262630654\n","on the 0.0008115015122003215\n","the Unity 0.0019650125281334635\n","Unity Engine 3.685460048195174\n","Engine { 0.0002692908010507933\n","{ -- 0.007513965324756057\n","-- } 0.004690756695729988\n","} the 7.466149563405876e-06\n","the Gamified 0.0032749914910876403\n","Gamified Discrimination 89.0644852613136\n","Discrimination Experiments 0.0780582692773381\n","Experiments engine 0.001593025849636663\n","engine ( 0.0003175106227840777\n","( GDX 0.015558004645587157\n","GDX ) 0.015019712289574098\n",") . 0.0005719747477920577\n",". The 0.0029167601591733247\n","The setting 0.0002041094347150364\n","setting is 0.00022466524326176897\n","is that 0.0002012014581708925\n","that of 3.353209331877096e-05\n","of a 0.00028513290991272254\n","a first 0.0003085916124184586\n","first person 0.0011983768179996226\n","person shooter 1.2256398826739243\n","shooter game 1.0007097168803187\n","game with 0.0006093528445395822\n","with the 0.0004575394292253315\n","the narrative 0.0004327578783066239\n","narrative background 0.006136609709902758\n","background of 0.0003119932277392395\n","of an 0.0002708791702590918\n","an alien 0.030926202843781725\n","alien invasion 534.3738589218343\n","invasion on 0.013850341063344675\n","on earth 0.003957235769510846\n","earth . 0.00048210879612592655\n",". We 0.0028961861394389073\n","We demonstrate 0.00391666457206972\n","demonstrate the 0.0009279053515374638\n","the utility 0.0018638362092527388\n","utility of 0.0028970371507402344\n","of our 0.0007315296844559598\n","our game 0.00041045340364072536\n","game as 0.00027827736099590776\n","as a 0.0010679593693210098\n","a research 0.0001291575745046048\n","research tool 0.0007189886921692267\n","tool with 7.175931518748199e-05\n","with an 0.0009827559080748634\n","an application 0.0017165754858664039\n","application focusing 0.001422371208963195\n","focusing on 0.012385563907156562\n","on attention 9.11854539513036e-05\n","attention to 0.0004945757896862169\n","to fine 0.00045224867904371225\n","fine phonetic 0.01733758165807163\n","phonetic detail 0.01618174275640139\n","detail in 0.00018497419595055374\n","in natural 0.0015518813695827786\n","natural speech 0.0002174745003071425\n","speech perception 0.00724514974957551\n","perception . 0.0005236574895434301\n",". The 0.002916622635631952\n","The game-based 0.004844940753480404\n","game-based framework 0.05463785332184518\n","framework is 0.0006870572741982559\n","is additionally 0.0002164062157987955\n","additionally compared 0.004412713850049845\n","compared against 0.007490215000844574\n","against a 0.001090071979969762\n","a traditional 0.0005413354706961322\n","traditional experimental 0.0005176726438137242\n","experimental setup 0.0506522755209505\n","setup in 0.00030235859935440895\n","in an 0.0005006415833727828\n","an auditory 0.001288546314827735\n","auditory discrimination 0.34253525781599203\n","discrimination task 0.004859887104298959\n","task . 0.0006842769218380387\n",". Applications 0.0003461161013584425\n","Applications of 0.00041463963069354986\n","of this 0.00032716756686277895\n","this novel 0.00018304088694851684\n","novel game-based 0.025769216544322588\n","game-based framework 0.0546370627983498\n","framework are 7.284581268763444e-05\n","are multifarious 0.006596589070656341\n","multifarious within 0.043542218134981825\n","within studies 0.00022977422771186773\n","studies on 0.0014068980875529236\n","on all 0.0007381617129962384\n","all aspects 0.003198722805488506\n","aspects of 0.0021155605655521024\n","of spoken 0.0007793151078063971\n","spoken language 0.007534478294667332\n","language perception 0.0001843092805894959\n","perception . 0.0005236444252227399\n","As the 0.0003231492117227231\n","the uses 1.5743905973586284e-05\n","uses of 0.00016818035972508317\n","of Games-With-A-Purpose 0.00462018420265445\n","Games-With-A-Purpose ( 0.015556736455046273\n","( GWAPs 0.009334032352099965\n","GWAPs ) 0.009011083593758257\n",") broadens 0.002503076132982613\n","broadens , 0.0005563694537291665\n",", the 0.0001475034206944482\n","the systems 8.417300769982837e-05\n","systems that 0.0005924696522242669\n","that incorporate 0.001103022006214097\n","incorporate its 0.0005141948278382049\n","its usages 0.0013991510224038016\n","usages have 0.0007196247561662576\n","have expanded 0.0016880082955058099\n","expanded in 0.0003493787375345438\n","in complexity 7.842336142467668e-05\n","complexity . 0.000460890275916891\n",". The 0.0029164815609491533\n","The types 5.822964432373278e-05\n","types of 0.0023933418207908235\n","of annotations 0.00018754018724738945\n","annotations required 0.0014903645196307038\n","required within 0.0011807624822617482\n","within the 0.0012317277275674654\n","the NLP 0.00016414182584609336\n","NLP paradigm 0.0006953101544869584\n","paradigm set 0.00029326506143152357\n","set such 3.231653718469534e-05\n","such an 0.0004299357151034078\n","an example 0.004272803286363152\n","example , 0.0015908871572738177\n",", where 0.0014392745688938742\n","where tasks 0.00010294678616273013\n","tasks can 0.000290113114629151\n","can involve 0.000244744122867023\n","involve varying 0.012134910686265576\n","varying complexity 0.014873192081975361\n","complexity of 0.0013717413858660588\n","of annotations 0.00018753739710267257\n","annotations . 0.0005701195103192174\n",". Assigning 0.0025308209673793156\n","Assigning more 0.02151394831218713\n","more complex 0.011103971275077479\n","complex tasks 0.0013357896125751375\n","tasks to 9.460868690811921e-05\n","to more 0.00019175698722825494\n","more skilled 0.019123452552513177\n","skilled players 1.3806688007769852\n","players through 0.002978432208834912\n","through a 0.0010513409099997739\n","a progression 0.00018750783858519643\n","progression mechanism 0.013677301517665079\n","mechanism can 0.0005861854385678885\n","can achieve 0.005590425465415759\n","achieve higher 0.010766085073541463\n","higher accuracy 0.010576088281613544\n","accuracy in 0.00032464465436617764\n","in the 0.0007001880515473122\n","the collected 0.000218789037005052\n","collected data 0.0019055049722078276\n","data while 0.0002200667436561597\n","while acting 0.010092810237606435\n","acting as 0.014858547654659483\n","as a 0.0010678541961294029\n","a motivating 0.0001992249307764644\n","motivating factor 0.09025535490508041\n","factor that 0.0006935310050657874\n","that rewards 0.00088180820027411\n","rewards the 3.3758009187723754e-05\n","the more 9.123697975750796e-05\n","more skilled 0.019123079917971828\n","skilled players 1.380641897429706\n","players . 0.00031389322096536605\n",". In 0.002735770968295256\n","In this 0.014340229587565034\n","this paper 0.013142330684863434\n","paper , 0.0013878247298380155\n",", we 0.0025181519778906996\n","we present 0.006388784188946853\n","present the 0.00043392513341503243\n","the progression 0.0009630757254327868\n","progression technique 0.016403652621259077\n","technique implemented 0.0010914367383046879\n","implemented in 0.002012040204750613\n","in Wormingo 0.00943257628689978\n","Wormingo , 0.003337949191464161\n",", an 0.0001837128649361832\n","an NLP 0.0003272137463383187\n","NLP GWAP 0.01653194654997855\n","GWAP that 0.0015839439713209048\n","that currently 0.0002505846072033336\n","currently includes 0.006202351616438644\n","includes two 0.003146239066203698\n","two layers 0.0016829952632630332\n","layers of 0.0009156927274695812\n","of task 1.9897611530766353e-05\n","task complexity 0.00026794717937307926\n","complexity . 0.0004608544488423607\n",". For 0.0031483737875508113\n","For the 0.0005939772584175495\n","the experiment 0.0002473102979913825\n","experiment , 0.0003395028438040022\n",", we 0.0025180717891848583\n","we have 0.001637355428762469\n","have implemented 0.002943001477576966\n","implemented four 0.0008756852139377036\n","four different 0.007627083610784787\n","different progression 0.0020603474590161763\n","progression scenarios 0.02723443683544466\n","scenarios on 7.200022056149026e-05\n","on 192 0.0034620066391342886\n","192 players 1.5531527922699344\n","players and 0.0004912611369548257\n","and compared 0.00023772208959870364\n","compared the 8.209207153787141e-05\n","the accuracy 0.0003801981811596201\n","accuracy and 0.00038728575803573663\n","and engagement 0.0006789856041168962\n","engagement achieved 0.004921052613061579\n","achieved with 0.00034085535634408265\n","with each 0.0007130922838957498\n","each scenario 0.0006015155900683643\n","scenario . 0.000849227071528767\n","While playing 0.012280844745004708\n","playing the 0.0007484171304967253\n","the communication 0.00021158701658219314\n","communication game 0.016163425110103068\n","game { 5.848491174111922e-05\n","{ `` 0.007361130429723694\n","`` } 0.0073611298679898554\n","} Are 0.0013012839008514575\n","Are You 4.240264571608194\n","You a 0.0009106771458768348\n","a Werewolf 0.006374724157072616\n","Werewolf { 0.007807683968968554\n","{ '' 0.0062131784671495965\n","'' } 0.006213177872735053\n","} , 5.6044878303559074e-05\n",", a 0.0001849859426124985\n","a player 0.0006594467059543684\n","player always 0.05962123821483342\n","always guesses 0.13300122248973117\n","guesses other 0.009389394044656904\n","other players 0.00425798046686784\n","players { 0.0008170737752658764\n","{ ' 0.007641767997112634\n","' } 0.007641764578149978\n","} roles 9.267131921710226e-05\n","roles through 0.0007599958819814268\n","through discussions 0.001883224973266365\n","discussions , 0.0004908423018179928\n",", based 0.00021448282410563825\n","based on 0.01064476700941356\n","on his 0.0008145525444097148\n","his own 0.021120340191506944\n","own role 0.0014052672379248066\n","role and 0.00011367704582567546\n","and other 0.0005646365607617984\n","other players 0.004257921394475512\n","players { 0.0008170624397271473\n","{ ' 0.0076416619805157095\n","' } 0.007641658561647918\n","} crucial 1.1883486039037588e-05\n","crucial utterances 0.0009953182990235338\n","utterances . 0.0006070760096342255\n",". The 0.002916051780023409\n","The underlying 0.0012132960287858567\n","underlying goal 0.0019665644345434334\n","goal of 0.002025193682128992\n","of this 0.0003271068712748237\n","this paper 0.013140994612893964\n","paper is 0.00032679669105770804\n","is to 0.0003422479441675864\n","to construct 0.002526495887628869\n","construct an 0.0015379946747390769\n","an agent 0.003718105253547087\n","agent that 0.0007217338625160741\n","that can 0.0014430538785023747\n","can analyze 0.00041950157567744513\n","analyze the 0.0015518633895036436\n","the participating 0.0007578994956924675\n","participating players 0.028759994353025172\n","players { 0.0008170429040248841\n","{ ' 0.007641479271186923\n","' } 0.0076414758524826175\n","} utterances 5.733608559248209e-05\n","utterances and 0.0004847596750313894\n","and play 0.000165594758814123\n","play the 0.0001483042303581788\n","the werewolf 0.0016370426117276072\n","werewolf game 2.000895258613896\n","game as 0.00027821124064958673\n","as if 0.0004805175085021123\n","if it 0.00446339088222201\n","it is 0.003881661351669289\n","is a 0.0007164116694143352\n","a human 0.0003940784039126365\n","human . 9.805253481032622e-06\n",". For 0.0031480402513625735\n","For a 0.00021171587513305116\n","a step 0.0005156125834905391\n","step of 0.00021484819250041658\n","of this 0.00032709434592493046\n","this underlying 4.4447798881596736e-05\n","underlying goal 0.0019664809568011436\n","goal , 0.00022336489755373062\n",", this 0.0001622877793584913\n","this paper 0.013140419472792297\n","paper studies 0.0014368982897261177\n","studies how 0.0007796305033372778\n","how to 0.001259173844381158\n","to accumulate 0.0033627488058662557\n","accumulate werewolf 19.07951612752465\n","werewolf game 2.000848133104747\n","game log 0.0370527429119153\n","log data 0.003370496845741369\n","data annotated 0.00025236987507251576\n","annotated with 0.0029399553814652866\n","with identification 1.4786414119074805e-05\n","identification of 0.001216102029799353\n","of players 0.00010742223751839612\n","players revealing 0.09860167798595434\n","revealing oneselves 8.479744290930904\n","oneselves as 0.024760098735297827\n","as seer/medium 0.02476008287413951\n","seer/medium , 0.0033374787470111\n",", the 0.00014747081245408895\n","the acts 1.6963517603403342e-05\n","acts of 7.179961986239178e-05\n","of the 0.0005976693971296817\n","the divination 0.003273931776136431\n","divination and 0.005280359748384033\n","and the 0.00018248142563424038\n","the medium 0.0002888740660302316\n","medium and 0.0002329552189736234\n","and declaring 0.005280302436451176\n","declaring the 0.0032738962412663035\n","the results 0.000376038415427181\n","results of 0.0004206610955479684\n","of the 0.0005976548889931554\n","the divination 0.0032738523034056676\n","divination and 0.005280231570817337\n","and the 0.00018247699602035703\n","the medium 0.0002888670538405244\n","medium . 0.00024806231377101266\n",". In 0.0027351996658478406\n","In this 0.014337234963919733\n","this paper 0.013139586216419983\n","paper , 0.001387534915216341\n",", we 0.0025176261234516905\n","we divide 0.0026184116707266107\n","divide the 0.0012276713200245826\n","the whole 0.0019257496044705496\n","whole task 0.0001326866021826102\n","task into 0.0003209376791038819\n","into four 0.002666641690796785\n","four sub 0.01789574573187598\n","sub tasks 0.006010554258228096\n","tasks and 0.00024803853646500665\n","and apply 0.0007583708719158883\n","apply CNN/SVM 0.5768750300939248\n","CNN/SVM classifiers 0.669406363221949\n","classifiers to 0.0005899115520679188\n","to each 0.00044473788085040293\n","each sub 0.016057663592205285\n","sub task 0.009022632240028562\n","task and 0.00021225518619074707\n","and evaluate 0.0008349451757324464\n","evaluate their 0.0014043819467895646\n","their performance 0.0019300938363725102\n","performance . 0.00045871257370203933\n","Framenets as 0.024758198051465192\n","as an 0.0010946959427776384\n","an incarnation 0.006183022917622963\n","incarnation of 0.002771277767593386\n","of frame 0.00011722792945888104\n","frame semantics 0.02408400621900485\n","semantics have 0.00014283066681756767\n","have been 0.025612886932516798\n","been set 0.00011959853394874554\n","set up 0.005298161229441282\n","up to 0.003651379364557334\n","to deal 0.003593017138326645\n","deal with 0.018006159112437088\n","with lexicographic 0.0009998123913099448\n","lexicographic issues 0.00853511057390666\n","issues ( 0.00013642062310319488\n","( cf 0.012219377637745287\n","cf . 0.002891595166290418\n",". Fillmore 0.0001606435490478452\n","Fillmore and 0.0012571385842924626\n","and Baker 0.0016246049833119705\n","Baker 2010 0.5628811167747918\n","2010 , 0.00025143063994650537\n",", among 0.0002147161392995138\n","among others 0.05311323613128403\n","others ) 0.0004279432858522719\n",") . 0.0005717449124397776\n",". They 0.003258727526220663\n","They are 0.008254155961617961\n","are thus 0.0005404149339297108\n","thus concerned 0.004870468399641718\n","concerned with 0.013869416838391147\n","with lexical 0.00020269072858826082\n","lexical units 0.025491492713545522\n","units ( 0.0014116115082728339\n","( LUs 0.006911838378641263\n","LUs ) 0.0050045214431154355\n",") and 0.00037901159258028953\n","and the 0.00018246538661851965\n","the conceptual 0.00041513275879389444\n","conceptual structure 0.0031495079373661887\n","structure which 0.0002637330483261218\n","which categorizes 0.012004732358333077\n","categorizes these 0.00387276383825895\n","these together 0.0001333246307310058\n","together . 0.00016037619742287693\n",". These 0.0033409564831292683\n","These lexically-evoked 0.3953814590040542\n","lexically-evoked frames 1.3767019358211852\n","frames , 0.0003612307309298495\n",", however 0.002422536272653757\n","however , 0.002714408057453877\n",", do 8.914308781449173e-05\n","do not 0.04857502208724017\n","not reflect 0.006990499812243808\n","reflect pragmatic 0.011873807625411294\n","pragmatic properties 0.006641489887779931\n","properties of 0.0019065268640207874\n","of constructions 0.00032566666783124745\n","constructions ( 0.0010965602786916753\n","( LUs 0.0069116456181560395\n","LUs and 0.0005866393289240779\n","and other 0.0005645230597216524\n","other types 0.0029335547404583887\n","types of 0.0023925296451507106\n","of constructions 0.00032566417001900585\n","constructions ) 0.0002887124189999465\n",") , 0.00046464346607784726\n",", such 0.0007818396242281716\n","such as 0.014357359313685142\n","as expressing 0.0010002613994779415\n","expressing illocutions 2.6977049864637497\n","illocutions or 0.03299231545942615\n","or being 0.0005587181631019217\n","being considered 0.0064381827355799925\n","considered polite 0.05280203947130478\n","polite or 0.0036658117562713755\n","or very 0.0004380389579481989\n","very informal 0.0020022163693784035\n","informal . 2.0952309580429416e-05\n",". From 0.0031471650813919756\n","From the 0.0009875356561407911\n","the viewpoint 0.0006546963933667042\n","viewpoint of 0.0012315829371023438\n","of a 0.0002850039858371358\n","a multilingual 0.0008503675847645675\n","multilingual annotation 0.00011668806563185091\n","annotation effort 0.009235841603947659\n","effort , 0.00034173482599960043\n",", the 0.0001474470102346462\n","the Global 0.0010723306723525033\n","Global FrameNet 0.10083039195227272\n","FrameNet Shared 0.003425988685216547\n","Shared Annotation 0.01771171479044548\n","Annotation Task 0.006574015449479471\n","Task , 0.00010665915326840958\n",", we 0.002517335053962299\n","we discuss 0.004284859282035521\n","discuss two 0.0007267845611449284\n","two phenomena 0.00045966548505287156\n","phenomena , 0.00042390433652790067\n",", greetings 0.0005561453545418206\n","greetings and 0.001759831463772754\n","and tag 0.00021194986823289597\n","tag questions 0.0027442900893367077\n","questions , 0.00031630506671394775\n",", which 0.0016191532328871413\n","which highlight 0.00037382142440034135\n","highlight the 0.0015177554743150928\n","the necessity 0.0024550142210888882\n","necessity both 0.001205469530197054\n","both to 3.921459251142988e-05\n","to investigate 0.0010303823953613318\n","investigate the 0.0013901982160599105\n","the role 0.0006373701293017406\n","role between 8.323265922167949e-05\n","between construction 0.00011134000244741994\n","construction and 0.0003662400371075334\n","and frame 0.0002813876788190094\n","frame annotation 0.001525910204175081\n","annotation on 8.414613349543139e-05\n","on the 0.0008110744359921088\n","the one 0.00013170455930155178\n","one hand 0.024152790160313395\n","hand and 0.0001921597318337621\n","and to 4.9514601643123146e-05\n","to develop 0.0027058586124432283\n","develop pragmatic 0.0027785105198577737\n","pragmatic frames 0.016585129722376513\n","frames describing 0.0054843257016055445\n","describing social 0.0009878915303475825\n","social interactions 0.007601716676675478\n","interactions which 0.0001051400921342503\n","which are 0.0018920120244256006\n","are not 0.0030364137336896845\n","not explicitly 0.010853536849921729\n","explicitly lexicalized 0.011432080067163715\n","lexicalized . 8.432696841521491e-05\n","This paper 0.01879327591093153\n","paper reports 0.012808286197769471\n","reports on 0.00377547509979472\n","on an 0.0003124557676704975\n","an effort 0.002653476397796152\n","effort to 0.0012658085279703278\n","to search 0.0003526714199670294\n","search for 0.0008503668574677454\n","for corresponding 4.3536848842568675e-05\n","corresponding constructions 0.006411498545536682\n","constructions in 0.000997311509807589\n","in English 0.001211867518999089\n","English and 0.0007244752399007972\n","and Japanese 0.0005741350482887898\n","Japanese in 6.252706412768827e-05\n","in a 0.0005363061512332975\n","a TED 0.00020556954096731596\n","TED Talk 3.132543412055319\n","Talk parallel 0.07367894214112854\n","parallel corpus 0.013364819209475704\n","corpus , 0.00031463996523369096\n",", using 0.00027219321973257133\n","using frames-and-constructions 0.029639056220259422\n","frames-and-constructions analysis 0.1088880312043563\n","analysis ( 0.00033920512528318414\n","( Ohara 0.011106805361715663\n","Ohara , 0.001430004312485726\n",", 2019 0.0009991393987961832\n","2019 ; 0.0025850967511047112\n","; Ohara 0.022096419966880818\n","Ohara and 0.0007541656432715834\n","and Okubo 0.005279143649577269\n","Okubo , 0.003336650442386229\n",", 2020 0.0005929703219672644\n","2020 ; 0.0010907879908793631\n","; cf 0.011048122947667412\n","cf . 0.0028911106342476476\n",". Czulo 0.0033729492470328084\n","Czulo , 0.0033366211582346868\n",", 2013 0.0016575394098832736\n","2013 , 0.0001937383716924154\n",", 2017 0.0010060708610110688\n","2017 ) 0.005614852208513055\n",") . 0.0005716481300820478\n",". The 0.0029150945929735045\n","The purpose 0.004158447302071032\n","purpose of 0.0018523767038324966\n","of the 0.000597512573044553\n","the paper 6.673050479181385e-05\n","paper is 0.00032668807025970056\n","is two-fold 0.008954446437996817\n","two-fold : 0.03110830482672386\n",": ( 0.0014305627829970117\n","( 1 0.006357832538817897\n","1 ) 0.00918236456641609\n",") to 0.0001795264860664437\n","to demonstrate 0.0003044260409575656\n","demonstrate the 0.0009273840840463353\n","the validity 0.0019226227450432607\n","validity of 0.0030677740683161155\n","of frames-and-constructions 0.0023088957149647064\n","frames-and-constructions analysis 0.10888196738131553\n","analysis to 0.00015078134615283635\n","to search 0.0003526453208995557\n","search for 0.0008503039269897422\n","for corresponding 4.353362694963161e-05\n","corresponding constructions 0.006411024070191362\n","constructions in 0.0009972377049528793\n","in typologically 0.00020060453954931856\n","typologically unrelated 0.1915119692111522\n","unrelated languages 0.0062368164022793444\n","languages ; 0.0006200433089078474\n","; and 0.0006940646404014541\n","and ( 0.00015307251758693105\n","( 2 0.006397750508244287\n","2 ) 0.009369063184624955\n",") to 0.00017952257516124046\n","to assess 0.003617897031184574\n","assess whether 0.017172538481228534\n","whether the 0.0006218931964799638\n","the { 1.3966385955627409e-05\n","{ `` 0.007358109278513613\n","`` } 0.007358108717240774\n","} Do 0.0019511247446012587\n","Do schools 0.6577020445190244\n","schools kill 9.207828615340798\n","kill creativity 7.216946752136\n","creativity ? 0.02141527223801323\n","? { 0.0008800325061126608\n","{ '' 0.006210643790159695\n","'' } 0.006210643196230037\n","} TED 0.00012587848980057198\n","TED Talk 3.132268786440868\n","Talk parallel 0.07367248281332701\n","parallel corpus 0.013363647534900748\n","corpus , 0.00031461238120576615\n",", annotated 9.582893432444449e-05\n","annotated in 0.0002178843370333363\n","in various 0.0010063756339559883\n","various languages 0.0010500469031072958\n","languages for 0.0001311593155298633\n","for Multilingual 0.00021326817610816485\n","Multilingual FrameNet 0.02384192893561496\n","FrameNet , 0.0004018264941336351\n",", is 6.227825463319432e-05\n","is a 0.0007161517729398744\n","a good 0.0015111713880503034\n","good starting 0.019832881579132936\n","starting place 0.005400918611295744\n","place for 0.0006537947828599392\n","for building 0.0027831451161640865\n","building a 0.0014316792264753163\n","a multilingual 0.000850213661513374\n","multilingual constructicon 0.06477932075115124\n","constructicon . 0.0008431653870830118\n",". The 0.0029148767810492963\n","The analysis 0.00045014689675661944\n","analysis showed 0.002449716824623004\n","showed that 0.009404088915216617\n","that similar 0.00018358974010797358\n","similar to 0.0011405723707225842\n","to our 9.008261876381397e-05\n","our previous 0.0005915976739199547\n","previous findings 0.005209600206622114\n","findings involving 0.0028704211143394213\n","involving texts 0.0006574467463045307\n","texts in 0.00046575137602404126\n","in a 0.0005362410645353817\n","a Japanese 0.0002450723992105411\n","Japanese to 0.00014266466957379742\n","to English 0.00027734002571721736\n","English bilingual 0.00011714629820363246\n","bilingual children 0.002883233299410808\n","children { 0.0012741523983543397\n","{ ' 0.007638430025186659\n","' } 0.0076384266092102\n","} s 0.006425747017893802\n","s book 0.0008041142414120846\n","book , 0.0003270852269025932\n",", the 0.00014741737236501637\n","the TED 0.0004222932250968981\n","TED Talk 3.1321104544871026\n","Talk bilingual 0.051373240242011356\n","bilingual transcripts 0.002522792976842745\n","transcripts include 0.003338990682446841\n","include pairs 0.000959299568643108\n","pairs of 0.0006218434612605642\n","of constructions 0.00032558720995066326\n","constructions that 0.0007306774586388044\n","that share 0.001321571672508922\n","share similar 0.01108010598122314\n","similar pragmatic 0.002302793015078296\n","pragmatic functions 0.018140987741814916\n","functions . 0.000519828393124424\n",". While 0.0028163219995199027\n","While the 0.00042653623151148596\n","the TED 0.00042228750954284474\n","TED Talk 3.1320680627424404\n","Talk parallel 0.07366776169447482\n","parallel corpus 0.013362791158650078\n","corpus constitutes 0.001267730258926449\n","constitutes a 0.003360889631798186\n","a good 0.0015110956848195245\n","good resource 0.0012749844536234659\n","resource for 0.002120094770593456\n","for frame 0.00014749263774942986\n","frame semantic 0.0033289760724213233\n","semantic annotation 0.0020458414165618857\n","annotation in 0.00012309746515773293\n","in multiple 0.0005223985271293781\n","multiple languages 0.00588739292103445\n","languages , 0.0005587528019039077\n",", it 0.000842584322950946\n","it may 0.0018503003874872647\n","may not 0.008353730674711377\n","not be 0.0025713520045622562\n","be the 4.311878211983716e-05\n","the ideal 0.0004007354198822353\n","ideal place 0.022704444355153997\n","place to 0.00012605163871423045\n","to start 0.0008255994300859035\n","start aligning 0.021838108290775596\n","aligning constructions 0.011968963147033076\n","constructions among 0.0013267920264941257\n","among typologically 0.004403820174658092\n","typologically unrelated 0.19149195537507752\n","unrelated languages 0.00623616462776474\n","languages . 0.000978435945109872\n",". Finally 0.0033638767425265845\n","Finally , 0.003206281715053781\n",", this 0.00016221915842784762\n","this work 0.007787469688138025\n","work shows 0.00175471791917816\n","shows that 0.007910636719018392\n","that the 0.00035957006295950596\n","the proposed 0.0011759219910954298\n","proposed method 0.011184481093025465\n","method , 0.00019612483387680744\n",", which 0.0016187707744053912\n","which focuses 0.0017969390631266534\n","focuses on 0.013248434929575947\n","on heads 8.000305147763854e-05\n","heads of 0.00010675558878249454\n","of sentences 0.00036620979540399933\n","sentences , 0.0003630526432583201\n",", seems 8.203339635865186e-05\n","seems valid 0.020453148973200455\n","valid for 0.00032584727954597137\n","for searching 0.001684329051666201\n","searching for 0.0037897403507282886\n","for corresponding 4.352754517465882e-05\n","corresponding constructions 0.00641012843140486\n","constructions in 0.0009970983879342768\n","in transcripts 0.0001262557524664518\n","transcripts of 0.0007420371931971008\n","of spoken 0.0007787887051237432\n","spoken data 0.0009863409601689912\n","data , 0.0003450560978831685\n",", as 0.00030521072474315556\n","as well 0.01627896760850803\n","well as 0.014963428323238282\n","as in 6.903347380126585e-05\n","in written 0.0004637875275095829\n","written data 0.00046696967505597756\n","data of 5.065053912587808e-05\n","of typologically-unrelated 0.004617048204252971\n","typologically-unrelated languages 0.07928434936879383\n","languages . 0.0009783862205791334\n","In this 0.014331470931728481\n","this paper 0.013134303681338392\n","paper , 0.0013869770819097873\n",", we 0.0025166139622307183\n","we introduce 0.0074598112862745\n","introduce the 0.0004109618234595737\n","the task 0.0006513962797168281\n","task of 0.0006897605481100159\n","of using 0.00015936502173342524\n","using FrameNet 0.00043258613127879463\n","FrameNet to 0.00014720334160095634\n","to link 0.0011447343988684635\n","link structured 0.0021544255973215166\n","structured information 0.002474604650373044\n","information about 0.009905285475647514\n","about real-world 0.0010617082396133892\n","real-world events 0.006024096270657317\n","events to 9.813528786138209e-05\n","to the 0.0003216687833459614\n","the conceptual 0.00041497978011490594\n","conceptual frames 0.014958706949846327\n","frames used 0.00041614787527668686\n","used in 0.0016746855721778257\n","in texts 0.000363378847885249\n","texts describing 0.0023884922245961653\n","describing these 0.0006477924458570407\n","these events 0.0006675925410432387\n","events . 0.0006399761943950839\n",". We 0.0028939726037086836\n","We show 0.0036438173153310804\n","show that 0.010693185281069746\n","that frames 0.00022030782993443147\n","frames made 0.0012320407894462735\n","made relevant 0.0003564741315880389\n","relevant by 6.500570131773725e-05\n","by the 0.0004083355131482083\n","the knowledge 0.00016828613011007137\n","knowledge of 0.0001880366064669689\n","of the 0.0005973786644053877\n","the real-world 0.00012138507075870202\n","real-world event 0.001657182428510311\n","event can 0.00014078586408659792\n","can be 0.018603787987698902\n","be captured 0.006873074655224404\n","captured by 0.014669615770824063\n","by complementing 0.0061172858317758505\n","complementing standard 0.013071074928061926\n","standard lexicon-driven 0.08278346926268874\n","lexicon-driven FrameNet 0.6495783171494003\n","FrameNet annotations 0.00534485720132499\n","annotations with 0.000289821591136437\n","with frame 0.00015472794279654037\n","frame annotations 0.0037169787688669286\n","annotations derived 0.0011031936389842719\n","derived through 0.0014461567393419226\n","through pragmatic 0.0030839726146800143\n","pragmatic inference 0.016002902593364062\n","inference . 0.0004793328304348026\n",". We 0.0028939075592018513\n","We propose 0.0050665859021614135\n","propose a 0.0035136753716684807\n","a two-layered 0.003716339864120768\n","two-layered annotation 0.010017050242519839\n","annotation scheme 0.047144795416998626\n","scheme with 0.0003465175298673458\n","with a 0.0007603525804889433\n","a { 2.97924986388756e-05\n","{ ` 0.007774336733153931\n","` } 0.007774336544209939\n","} strict 0.0007224910028733321\n","strict { 0.00028899640068623676\n","{ ' 0.007637176605517174\n","' } 0.007637173190661702\n","} FrameNet-compatible 0.007802867652324477\n","FrameNet-compatible lexical 0.19717224171531286\n","lexical layer 0.0003061680525944944\n","layer and 0.0003196107517315007\n","and a 0.00015081544932404615\n","a { 2.9792076150721757e-05\n","{ ` 0.007774226485317923\n","` } 0.007774226296379289\n","} loose 0.0019506980445818702\n","loose { 0.0004876745109139261\n","{ ' 0.00763706831180251\n","' } 0.007637064897043883\n","} layer 1.2116082312758064e-05\n","layer capturing 0.002246860110394882\n","capturing frames 0.003729324472294573\n","frames that 0.0004773097462013059\n","that are 0.00110092075886984\n","are inferred 0.0016478628293385565\n","inferred from 0.009529348254471756\n","from referential 0.001102853445864965\n","referential data 0.0004605242244638768\n","data . 0.0005551044391133374\n","Multimodal aspects 0.004614968483679697\n","aspects of 0.002113913594836668\n","of human 0.00046959100320164603\n","human communication 0.0036462713154885315\n","communication are 0.00017037598977363014\n","are key 0.0005364327278467799\n","key in 5.753459601060662e-05\n","in several 0.0005899179605401431\n","several applications 0.001700119403402139\n","applications of 0.0002727895566028123\n","of Natural 0.0005495953137428361\n","Natural Language 0.2258994057157669\n","Language Processing 0.24235865776276344\n","Processing , 0.0001623306999695405\n",", such 0.000781517141567863\n","such as 0.014351437385177333\n","as Machine 0.00023545440943260694\n","Machine Translation 0.34421457564892866\n","Translation and 0.00010545424376791531\n","and Natural 0.00020760311526851497\n","Natural Language 0.2258974193336042\n","Language Generation 0.10583022971241471\n","Generation . 0.00015905291482940423\n",". Despite 0.0023648120349629965\n","Despite recent 0.013213000509597127\n","recent advances 0.09739156139222402\n","advances in 0.00673279556511728\n","in integrating 7.008102563550629e-05\n","integrating multimodality 0.1102685995669804\n","multimodality into 0.005703182708647292\n","into Computational 0.0006297992203220368\n","Computational Linguistics 1.5475208168748456\n","Linguistics , 0.00044649567344297184\n",", the 0.0001473871495583324\n","the merge 0.0001924765605611707\n","merge between 0.0025009384676314647\n","between NLP 7.103750380670025e-05\n","NLP and 0.00014255316322609596\n","and Computer 0.0007036520582731629\n","Computer Vision 3.295762895619579\n","Vision techniques 0.008891446653720651\n","techniques is 8.050155160615909e-05\n","is still 0.005268857863237252\n","still timid 0.473747088413088\n","timid , 0.0033355384500693918\n",", especially 0.002266768199393283\n","especially when 0.021605896757474496\n","when it 0.0018931407570398368\n","it comes 0.013969707433087416\n","comes to 0.0010950785741621304\n","to providing 0.0001527613846144277\n","providing fine-grained 0.0033117616841152418\n","fine-grained accounts 0.007850101609729659\n","accounts for 0.005047116263777777\n","for meaning 8.635904326613e-05\n","meaning construction 0.0005770707996461618\n","construction . 0.00024273606665929268\n",". This 0.002098805246462222\n","This paper 0.0187862153539288\n","paper reports 0.012803474178953585\n","reports on 0.0037740566698304012\n","on research 3.297948509798481e-05\n","research aiming 0.0026929403930233335\n","aiming to 0.0032006911621055628\n","to determine 0.0045745052296846085\n","determine appropriate 0.0070545676156129385\n","appropriate methodology 0.0014628302339186577\n","methodology and 0.0003838031515580482\n","and develop 0.00031900641142175267\n","develop a 0.0027230604539847237\n","a computational 0.0005035135701382593\n","computational tool 0.0013384813434990125\n","tool to 0.00045062298822319986\n","to annotate 0.002853578966436378\n","annotate multimodal 0.0016471313631693236\n","multimodal corpora 0.0018901579025427497\n","corpora according 0.0010277716954918848\n","according to 0.006624391033556409\n","to a 0.00018822163215567638\n","a principled 0.003573553677300933\n","principled structured 0.0072504867463820035\n","structured semantic 0.0018774944127207492\n","semantic representation 0.004050784652635982\n","representation of 0.0010260322875338143\n","of events 0.000627582459354157\n","events , 0.0004382310501551451\n",", relations 3.5506686614038556e-05\n","relations and 0.00028966554685777\n","and entities 0.00011092304272586786\n","entities : 0.00014855816254859147\n",": FrameNet 0.00025794354011354206\n","FrameNet . 0.0003322490787573928\n",". Taking 0.0031055129032002736\n","Taking a 0.0005029063705479167\n","a Brazilian 0.0004304142918282804\n","Brazilian television 0.34355668698299185\n","television travel 0.5777998822931868\n","travel show 0.0013373523104062324\n","show as 2.727265749754493e-06\n","as corpus 2.6727658572440757e-06\n","corpus , 0.00031451506612683775\n",", a 0.00018485322002566293\n","a pilot 0.0031419978614843657\n","pilot study 0.06508610831191115\n","study was 0.0012607477384772136\n","was conducted 0.012452367394220157\n","conducted to 0.0007080646317613444\n","to annotate 0.0028534812036721405\n","annotate the 0.0005857607822331843\n","the frames 0.00014335541491203234\n","frames that 0.00047725950060510853\n","that are 0.0011008048669174542\n","are evoked 0.009586556287303438\n","evoked by 0.023770626921352754\n","by the 0.00040827143269038513\n","the audio 0.0003643654766630762\n","audio and 0.0007555715062864055\n","and the 0.00018236557189476365\n","the ones 0.0003446658035529448\n","ones that 0.0005654165284339285\n","that are 0.0011007883814805465\n","are evoked 0.009586412721010601\n","evoked by 0.023770270937339206\n","by visual 0.00017860166512944397\n","visual elements 0.00721632388605519\n","elements . 0.0003736315179744706\n",". We 0.0028934554564090035\n","We also 0.005897763045164398\n","also implemented 0.0022688918206773575\n","implemented a 0.00053601804808137\n","a Multimodal 0.0002843693329034545\n","Multimodal Annotation 0.044968449209003725\n","Annotation tool 0.0035543163694908442\n","tool which 0.0006858449987733135\n","which allows 0.005372698755726276\n","allows annotators 0.004088758821223486\n","annotators to 0.0005574496024917855\n","to choose 0.0037482127133337765\n","choose frames 0.01764022864558475\n","frames and 0.0005712107183716925\n","and locate 0.0004947077978988968\n","locate frame 0.021171568451096946\n","frame elements 0.07865321380124286\n","elements both 0.0002416136432965558\n","both in 0.0002408756742194164\n","in the 0.0006995918237619816\n","the text 0.0002616378800165937\n","text and 0.0002673539181682626\n","and in 3.866048741030253e-05\n","in the 0.0006995849755796819\n","the images 0.00023298627920320702\n","images , 0.000449742162786624\n",", while 0.0015384280619395558\n","while keeping 0.0999155942164324\n","keeping track 0.1238186222076728\n","track of 0.0009854234330507548\n","of the 0.0005972581873832165\n","the time 0.0002745186885612888\n","time span 0.008721208128135463\n","span in 0.00037983228936321593\n","in which 0.0007161201181168345\n","which those 0.00010973415857011429\n","those elements 0.0010735995887580228\n","elements are 0.0009042177857860764\n","are active 0.00018829578841211405\n","active in 0.0002692732137177018\n","in each 0.0004296762246363047\n","each modality 0.012274591668730097\n","modality . 0.000488611434006143\n",". Results 0.0033221881449468026\n","Results suggest 0.03111102554646956\n","suggest that 0.008486324766510279\n","that adding 0.001894437498929896\n","adding a 0.0008158088643539734\n","a multimodal 0.0009296958015899227\n","multimodal domain 0.00019046331568636952\n","domain to 9.74004452861615e-05\n","to the 0.00032159047528427674\n","the linguistic 0.0003426606247185913\n","linguistic layer 0.0007097070461013008\n","layer of 0.0005805589338428625\n","of annotation 0.00018912045523032366\n","annotation and 0.0002304505225188881\n","and analysis 0.00014307568938204092\n","analysis contributes 0.000980489425536489\n","contributes both 0.0008249229187681904\n","both to 3.919351600646684e-05\n","to enrich 0.0039463515121048115\n","enrich the 0.0016548131808761175\n","the kind 0.0003216430040959699\n","kind of 0.0034877337155895076\n","of information 0.0002674095275511318\n","information that 0.0002636142405203538\n","that can 0.001441912378407461\n","can be 0.018599319826541814\n","be tagged 0.001648501545172828\n","tagged in 0.00023414208685969184\n","in a 0.0005360295958418037\n","a corpus 0.0006226268629139264\n","corpus , 0.0003144777353664764\n",", and 0.0005693860160019683\n","and to 4.94877315054612e-05\n","to enhance 0.004317982968660955\n","enhance FrameNet 0.004551962001253548\n","FrameNet as 9.02966129356443e-05\n","as a 0.0010668635330905649\n","a model 0.00027380677054660187\n","model of 6.922190967127001e-05\n","of linguistic 0.0006388617012566012\n","linguistic cognition 0.003109063897170977\n","cognition . 0.0006880058530699158\n","We introduce 0.00659450494668641\n","introduce an 0.001975763957943466\n","an annotation 0.0012101558147786303\n","annotation tool 0.010770713200498437\n","tool whose 0.001026486430203248\n","whose purpose 0.010956920947353862\n","purpose is 0.0007022980223500559\n","is to 0.0003419672196900065\n","to gain 0.0013878816680903741\n","gain insights 0.03834115580958818\n","insights into 0.03797993310333361\n","into variation 0.00023648901372001418\n","variation of 0.0005530195748966643\n","of framing 0.00018968098419509412\n","framing by 0.00039793506926769984\n","by combining 0.00808537297456581\n","combining FrameNet 0.003247012787620084\n","FrameNet annotation 0.0026315276768151923\n","annotation with 0.00013720476641630066\n","with referential 0.0005143148500638243\n","referential annotation 0.003042354112697892\n","annotation . 0.00025348464433014355\n",". English 1.3976703302589694e-05\n","English FrameNet 0.004846263046024217\n","FrameNet enables 0.0031221117446560273\n","enables researchers 0.01304384145666141\n","researchers to 0.0011613063164331702\n","to study 0.0005055556207358535\n","study variation 0.0003761361107430726\n","variation in 0.0022148230485478132\n","in framing 0.0007745621174620383\n","framing at 0.0012090579940063448\n","at the 0.0007989227231487849\n","the conceptual 0.000414851744590689\n","conceptual level 0.0021218458839930734\n","level as 0.0001492844770904982\n","as well 0.016273401270643872\n","well through 6.700624462507733e-05\n","through its 0.0010821507837266038\n","its packaging 0.04426208364001562\n","packaging in 0.0031412607904990238\n","in language 0.0001334779079565211\n","language . 0.00028527820202121614\n",". We 0.0028930769384366284\n","We enrich 0.0008734176149789832\n","enrich FrameNet 0.011326536951051971\n","FrameNet annotations 0.005343292209942697\n","annotations in 0.0004600717350066505\n","in two 0.0005066639617606034\n","two ways 0.007075450673100305\n","ways . 0.0005102539353103612\n",". First 0.0030685678805693747\n","First , 0.00289656828164862\n",", we 0.0025157407938910664\n","we introduce 0.007457223020764042\n","introduce the 0.0004108192356391124\n","the referential 0.0003312731668334605\n","referential aspect 0.010331518549668756\n","aspect . 0.00017525424573087555\n",". Secondly 0.0033340092697545618\n","Secondly , 0.003261455010249227\n",", we 0.0025157043228791537\n","we annotate 0.0018290354727758128\n","annotate on 0.00010615112359102783\n","on complete 0.00022469860226905986\n","complete texts 0.0004635009282452994\n","texts to 0.00013077665429248926\n","to encode 0.002811110302603794\n","encode connections 0.006483592373443666\n","connections between 0.030064703715044123\n","between mentions 0.0006757593458203616\n","mentions . 0.0005091368426426062\n",". As 0.0030488376939302913\n","As a 0.0024000684942710186\n","a result 0.0015535069671976196\n","result , 0.0007034117977332006\n",", we 0.0025156642143009003\n","we can 0.0007393091828509505\n","can analyze 0.000419129501932458\n","analyze the 0.001550486976061951\n","the variation 0.0002261213277254929\n","variation of 0.0005529789366717081\n","of framing 0.0001896670456793355\n","framing for 0.0001591380826713038\n","for one 0.00022436390087382646\n","one particular 0.0009980752459800993\n","particular event 0.001132050731056907\n","event across 0.00030354731936056267\n","across multiple 0.013067223402140256\n","multiple mentions 0.0028213502585153886\n","mentions and 0.0005619863875859905\n","and ( 0.0001529883003453933\n","( cross-lingual 3.7748916089994e-05\n","cross-lingual ) 3.644283769518489e-05\n",") documents 4.031066464921347e-05\n","documents . 0.0006702607307824738\n",". We 0.0028928965665166225\n","We can 5.473571216301617e-05\n","can examine 9.10212219546856e-05\n","examine how 0.01214121390171604\n","how an 0.0002391992808206332\n","an event 0.0023984873328458675\n","event is 0.000229621581477866\n","is framed 0.002663464895577383\n","framed over 0.00435824209757491\n","over time 0.012465242095814316\n","time and 0.00047048941222972896\n","and how 0.0005062861802373434\n","how core 0.00036719118643327014\n","core frame 0.010109872951591518\n","frame elements 0.07863827206583306\n","elements are 0.0009040748268161015\n","are expressed 0.002165325877585607\n","expressed throughout 0.015429280044971794\n","throughout a 0.0007148452379813526\n","a complete 0.001448081965181304\n","complete text 0.00027702054975387144\n","text . 0.00043884435350295345\n",". The 0.002913339316010501\n","The data 0.00033635025970514597\n","data model 7.285682600158757e-05\n","model starts 0.0014449272873329874\n","starts with 0.005078361806472345\n","with a 0.0007600782814531292\n","a representation 0.00017834644122459296\n","representation of 0.0010257587388902624\n","of an 0.00027056406903562145\n","an event 0.0023984380383943677\n","event type 0.0065831755098363295\n","type . 0.00020067364511219273\n",". Each 0.003270949965633956\n","Each event 0.002646388593611517\n","event type 0.006583149221769804\n","type has 0.00015513788094534238\n","has many 0.0010048039807616551\n","many incidents 0.01029431453914319\n","incidents linked 0.09242422756840546\n","linked to 0.003176490113724172\n","to it 3.487156578779764e-05\n","it , 5.3689401332333164e-05\n",", and 0.0005693064928697164\n","and each 0.00010043952237294106\n","each incident 0.008356468153934801\n","incident has 0.004560358284986332\n","has several 0.0007081679334288189\n","several reference 0.0009291507957281921\n","reference texts 0.005195176860600782\n","texts describing 0.002387504395905258\n","describing it 0.0005659269637046595\n","it as 0.0003489305116852724\n","as well 0.01627165642994774\n","well as 0.014956707977907858\n","as structured 0.00041321529106622975\n","structured data 0.004171435844931421\n","data about 0.00032750296809577154\n","about the 0.0009075326585138922\n","the incident 0.00020443932727939442\n","incident . 0.00042134560065428293\n",". The 0.0029132374968283024\n","The user 0.0003922114680982713\n","user can 0.000816500758778469\n","can apply 0.0005449214675348088\n","apply two 0.000837555790089685\n","two types 0.00437843660975676\n","types of 0.0023906779871013225\n","of annotations 0.00018733145226824076\n","annotations : 0.0003100649107999525\n",": 1 0.007395126335821632\n","1 ) 0.009176574773095093\n",") mappings 0.00010417643096152748\n","mappings from 0.003427271404020483\n","from expressions 2.6343910165074163e-05\n","expressions to 0.00017072649442268447\n","to frames 5.1953282461821994e-05\n","frames and 0.0005710711726412196\n","and frame 0.00028118648482270347\n","frame elements 0.07863399917262429\n","elements , 0.0002903309405288137\n",", 2 0.00017823025554628794\n","2 ) 0.009363248881288132\n",") reference 3.901489881509119e-05\n","reference relations 0.0013853403725335067\n","relations from 0.0010526752335744164\n","from mentions 9.230728917204205e-05\n","mentions to 0.0003204718718475592\n","to events 5.51760034052536e-05\n","events and 0.0006305626272792472\n","and participants 0.00012978875194063848\n","participants of 0.00010685607081322634\n","of the 0.0005971178948173218\n","the structured 0.00020033417761149757\n","structured data 0.004171262343622924\n","data . 0.0005548834172485767\n","This paper 0.01877969481017205\n","paper presents 0.028584934185244944\n","presents an 0.002809511319461441\n","an approach 0.0018797953703384922\n","approach to 0.0012125732802529556\n","to project 0.00010099921827268543\n","project FrameNet 0.0015410431433921738\n","FrameNet annotations 0.005342508275964131\n","annotations into 0.0005629098975203505\n","into other 0.0003985638095214167\n","other languages 0.006010854575233632\n","languages using 0.0004133782896534799\n","using attention-based 0.0011966911827859374\n","attention-based neural 0.012647232749738026\n","neural machine 0.01323396025943794\n","machine translation 0.04109735796385695\n","translation ( 0.0016360332137620927\n","( NMT 0.0049762021429567566\n","NMT ) 0.004850761800798706\n",") models 0.00029567211423284613\n","models . 0.00047575180956027126\n",". The 0.002913107701252022\n","The idea 0.00283957081571422\n","idea is 0.004047578593050108\n","is to 0.0003419039665275325\n","to use 0.0007341555668808263\n","use an 0.00036512871957061993\n","an NMT 0.0006350783213782382\n","NMT encoder-decoder 0.0008820446742768735\n","encoder-decoder attention 0.007249284421861864\n","attention matrix 0.004808198461314087\n","matrix to 0.0002647889821928679\n","to propose 5.6719454551928124e-05\n","propose a 0.0035120986723040067\n","a word-to-word 0.001061334921946723\n","word-to-word correspondence 0.3901383382088403\n","correspondence between 0.037280299750561074\n","between the 0.0006412488185253043\n","the source 0.0009695473036650226\n","source and 0.0005961861015541071\n","and the 0.00018230807847147406\n","the target 0.001198072179657194\n","target language 0.005409800694035253\n","language . 0.00028522607416602317\n",". We 0.0028925482990008696\n","We combine 0.0018333681019823758\n","combine this 0.00025389534797858645\n","this word 2.5693064374518926e-05\n","word alignment 0.009710142994653835\n","alignment along 0.0005916892589910395\n","along with 0.015721153557172497\n","with a 0.000759996866969773\n","a set 0.001748657611014899\n","set of 0.002472611395520686\n","of simple 0.00011951445683680514\n","simple rules 0.0013248911256992216\n","rules to 0.00047991503868024404\n","to securely 0.006718794711765838\n","securely project 0.4222262819017929\n","project the 3.1051376418841224e-05\n","the FrameNet 0.00041779386992691394\n","FrameNet annotations 0.005342234996448522\n","annotations into 0.0005628811036041099\n","into the 0.0005766694836172746\n","the target 0.0011980416113328096\n","target language 0.005409662665182163\n","language . 0.00028521879674294235\n",". We 0.002892474497137737\n","We successfully 0.00039571689793056086\n","successfully implemented 0.014675340375878651\n","implemented , 5.219756807960586e-05\n",", evaluated 4.193841298390052e-05\n","evaluated and 0.00012796773255291255\n","and analyzed 0.0006035070294380231\n","analyzed this 0.0004957607698486176\n","this technique 0.001465546503624768\n","technique on 0.0003032152650406454\n","on the 0.0008104179179754043\n","the English-to-French 0.00021804318540777362\n","English-to-French configuration 0.32344129666844057\n","configuration . 0.0005208756276449338\n",". First 0.003067890492211748\n","First , 0.002895928862326281\n",", we 0.0025151854451195606\n","we analyze 0.003611430824221823\n","analyze the 0.0015501924787808341\n","the obtained 0.00014156330722472503\n","obtained FrameNet 0.0014535091271350522\n","FrameNet lexicon 0.0030769387275397324\n","lexicon qualitatively 0.005774528628008552\n","qualitatively . 0.000554025657598432\n",". Then 0.0033642260200338154\n","Then , 0.002052161327047056\n",", we 0.002515148915163049\n","we use 0.002106039919818477\n","use existing 0.0003214183214902778\n","existing French 0.0006482733747128593\n","French FrameNet 0.010197292576176622\n","FrameNet corpora 0.0005196592492587326\n","corpora to 0.0001810462712645605\n","to assert 0.0007464904577284753\n","assert the 0.0010901866100725256\n","the quality 0.0008199248727552061\n","quality of 0.0014180152496130187\n","of the 0.0005970459481071219\n","the translation 0.0002427894452222146\n","translation . 0.00029679659908938004\n",". Finally 0.0033616687587122987\n","Finally , 0.0032041771736497507\n",", we 0.002515083952290415\n","we trained 0.0003577429214094807\n","trained a 0.00011069749498082089\n","a BERT-based 0.002035449666241846\n","BERT-based FrameNet 0.007982082666239954\n","FrameNet parser 0.0030479313845624923\n","parser using 0.00023172396440507558\n","using the 0.00042759373863987765\n","the projected 0.0006651781429434182\n","projected annotations 0.02480765399306089\n","annotations and 0.00028645151555485793\n","and compared 0.0002374385687844248\n","compared it 0.00020230980026107013\n","it to 0.0003414965643190135\n","to a 0.00018813214394511324\n","a BERT 0.0001281790380499847\n","BERT baseline 0.0017821335922544694\n","baseline . 0.000378634708761158\n",". Results 0.003320937204368192\n","Results show 0.026188307131049606\n","show substantial 0.0020051027076379503\n","substantial improvements 0.07124373247938559\n","improvements in 0.001849033681688096\n","in the 0.0006993015795308346\n","the French 0.000332176436780542\n","French language 0.0009390173431250837\n","language , 0.00016366941277883777\n",", giving 0.0013467661022292683\n","giving evidence 0.0074243751028460325\n","evidence to 0.00028932751478587504\n","to support 0.002191656471791607\n","support that 2.425521182575686e-05\n","that our 0.002721415823742355\n","our approach 0.006619608723144541\n","approach could 0.0008946718574612785\n","could help 0.01393477282709055\n","help to 0.0006141269904361144\n","to propagate 0.002985777522618136\n","propagate FrameNet 0.05409882885513477\n","FrameNet data-set 0.06282444590189933\n","data-set on 0.00044616396665010736\n","on other 0.00024331555170100516\n","other languages 0.006009859331168961\n","languages . 0.000977745060927326\n","Large coverage 0.009442941288189732\n","coverage lexical 0.0009519656293480331\n","lexical resources 0.015937474379152692\n","resources that 0.00040093136820478495\n","that bear 0.0010169792670014987\n","bear deep 0.01981100982920493\n","deep linguistic 0.0015039178977958667\n","linguistic information 0.003135559569295404\n","information have 4.48629560579731e-05\n","have always 0.0015466585208581617\n","always been 0.0062546967724165506\n","been considered 0.006696941100717183\n","considered useful 0.0022047623062174033\n","useful for 0.0034698201426717075\n","for many 0.0014535131677441318\n","many natural 0.006920018809053424\n","natural language 0.026917238077582107\n","language processing 0.016888096915275756\n","processing ( 0.0015264807023909954\n","( NLP 0.0021977830769272446\n","NLP ) 0.002130094933149465\n",") applications 0.00041307447066112404\n","applications including 0.0023113185355141813\n","including Machine 0.0004879694291742144\n","Machine Translation 0.34402198249011795\n","Translation ( 0.0065896977453939265\n","( MT 0.0031326542935719006\n","MT ) 0.0030676777439974465\n",") . 0.0005711608398988275\n",". In 0.0027322736059934814\n","In this 0.014321897294577831\n","this respect 0.0008880170826202\n","respect , 7.701674433763953e-05\n",", Frame-based 0.0011112363218331103\n","Frame-based resources 0.05824275558197675\n","resources have 0.00062594562765822\n","have been 0.025586020181295216\n","been developed 0.011885531731041312\n","developed for 0.0020618963798645713\n","for many 0.001453488528804152\n","many languages 0.0037224652049419053\n","languages following 0.00016929643302046816\n","following Frame 0.022804229211351695\n","Frame Semantics 3.006304015517556\n","Semantics and 0.0005200189795471024\n","and the 0.00018227816935798143\n","the Berkeley 0.001250393327096814\n","Berkeley FrameNet 0.9164702066853053\n","FrameNet project 0.023111145253733235\n","project . 0.0004479075929505172\n",". However 0.003367391667863592\n","However , 0.0032854216642398226\n",", to 5.527188990166928e-05\n","to a 0.00018811962409393616\n","a great 0.0018251480337761745\n","great extent 0.02483339093590537\n","extent , 0.00044506578104508453\n",", all 0.00014037303372690943\n","all those 0.000598808963513365\n","those efforts 0.0008134220643876888\n","efforts have 0.006417458730124633\n","have been 0.025585290797055866\n","been kept 0.005085824052873209\n","kept fragmented 0.9528654624901435\n","fragmented . 0.0007221230703490122\n",". Consequentially 0.0033698945258645694\n","Consequentially , 0.003333599337704444\n",", the 0.00014729939655303933\n","the Global 0.001071257134732182\n","Global FrameNet 0.10072944807214582\n","FrameNet initiative 0.022129954320880604\n","initiative has 0.0041446394177014996\n","has been 0.03402553869781407\n","been conceived 0.009246824566378981\n","conceived of 0.0008388614599335289\n","of as 2.9937018390763454e-06\n","as a 0.0010664193437431113\n","a joint 0.0014630390779503497\n","joint effort 0.006087916984087618\n","effort to 0.001264595675090632\n","to bring 0.001972976845444513\n","bring together 0.09175612944941376\n","together FrameNets 0.17494835031880956\n","FrameNets in 0.0037680422879263504\n","in different 0.0005632045355503455\n","different languages 0.004445916335308267\n","languages . 0.0009776741312643622\n",". The 0.002912424030453093\n","The proposed 0.0037637737469710937\n","proposed paper 4.0435115402356855e-05\n","paper is 0.0003263914473146485\n","is aimed 0.002460482969884067\n","aimed at 0.06493167307377364\n","at describing 0.002460472976061523\n","describing ongoing 0.008894764165245705\n","ongoing work 0.01474580958726782\n","work towards 0.0017066575433326966\n","towards developing 0.008245116834363502\n","developing the 0.0001397476334754182\n","the Greek 0.0003528862014363494\n","Greek ( 0.0004470412408169165\n","( EL 0.004363670588520426\n","EL ) 0.004212691671461216\n",") counterpart 0.00016663518374404276\n","counterpart of 0.0004613625009031666\n","of the 0.0005969598383392617\n","the Global 0.0010712216222112986\n","Global FrameNet 0.10072610885833234\n","FrameNet and 0.00046196416062583336\n","and our 9.4102650933071e-05\n","our efforts 0.003064024250848724\n","efforts to 0.0010075999826640977\n","to contribute 0.0009238717334047298\n","contribute to 0.004369143360675642\n","to the 0.0003214319254115411\n","the Shared 0.00016666092977063975\n","Shared Annotation 0.017693166410983355\n","Annotation Task 0.0065671308911966495\n","Task . 0.0002461885969662248\n",". In 0.002732004457504185\n","In the 0.00021762540883695744\n","the paper 6.666728953730951e-05\n","paper , 0.0013859082571480357\n",", we 0.002514674632023687\n","we will 0.001954665192863722\n","will elaborate 0.004943525609996128\n","elaborate on 0.0037716453181789824\n","on the 0.0008102406619681374\n","the annotation 0.0004770185074297715\n","annotation methodology 0.002495945282872736\n","methodology employed 0.0037969139361985816\n","employed , 7.305954217530569e-05\n",", the 0.00014728800865252136\n","the current 0.0013964601604165069\n","current status 0.0412827678218037\n","status and 0.00033168916222843433\n","and progress 0.00013547483671759683\n","progress made 0.009640994954687862\n","made so 0.0015777620527795066\n","so far 0.17794553539426028\n","far , 0.0003644877363757671\n",", as 0.00030496390742608935\n","as well 0.0162658031676369\n","well as 0.014951327731246175\n","as the 0.0002652147176436903\n","the problems 0.00026137497830839126\n","problems raised 0.0071343386433705535\n","raised during 0.020776007508197625\n","during annotation 0.0006657177602999657\n","annotation . 0.0002533612843532288\n","This paper 0.01877363415320836\n","paper presents 0.028575709146693145\n","presents the 0.0008767739689368531\n","the first 0.0015394042661453949\n","first investigation 0.0015165039148759764\n","investigation on 0.0008576044052729093\n","on using 9.515971401511849e-05\n","using semantic 0.00019738984436759748\n","semantic frames 0.010132427059215526\n","frames to 0.00019042798252362896\n","to assess 0.003614404951353201\n","assess text 0.00041344429814515424\n","text difficulty 0.0013936551096812263\n","difficulty . 0.0002432349737369639\n",". Based 0.003019342233877677\n","Based on 0.012635688962801732\n","on Mandarin 0.0003073053885843054\n","Mandarin VerbNet 0.11185336293130603\n","VerbNet , 0.0007546894555811474\n",", a 0.00018473344966827012\n","a verbal 0.00023872314587418015\n","verbal semantic 0.0008775274293428184\n","semantic database 0.0007983190616305513\n","database that 0.000257994646879764\n","that adopts 0.0012941107477381286\n","adopts a 0.0025319088103786798\n","a frame-based 0.0015914815828755495\n","frame-based approach 0.004133108046576029\n","approach , 0.00024209718229234608\n",", we 0.0025145184560936198\n","we examine 0.004943557397274922\n","examine usage 0.0021830436427506387\n","usage patterns 0.01801010977019133\n","patterns of 0.0006260729669135966\n","of ten 0.0007543521299149692\n","ten verbs 0.005537182078057033\n","verbs in 0.0007771449179004818\n","in a 0.0005357325464288015\n","a corpus 0.0006222818250914657\n","corpus of 0.000647273659810853\n","of graded 0.00035485515441704697\n","graded Chinese 0.00892959486107267\n","Chinese texts 0.0013033464961156953\n","texts . 0.0007729964198651756\n",". We 0.0028915908372651134\n","We identify 0.0008943454362058534\n","identify a 0.00025277770053386603\n","a number 0.0016619449239813411\n","number of 0.004277766031670481\n","of characteristics 8.717606460147526e-05\n","characteristics in 0.00016316065285151054\n","in texts 0.0003630751420941182\n","texts at 0.00042918520340842607\n","at advanced 0.0003542732955996877\n","advanced grades 0.11277078294566298\n","grades : 0.003717225964740637\n",": more 6.824990579232858e-05\n","more frequent 0.006018206538203398\n","frequent use 0.0009948627292587187\n","use of 0.001409708314752152\n","of non-core 0.0015376796982297335\n","non-core frame 0.4513675324955392\n","frame elements 0.07860226067726513\n","elements ; 0.002446038255968751\n","; more 4.9769066633355535e-05\n","more frequent 0.00601818287556246\n","frequent omission 0.07158409423299258\n","omission of 0.0012725621043498654\n","of some 0.00022287633459917985\n","some core 0.000994120003126766\n","core frame 0.010105204479861678\n","frame elements 0.0786019589937013\n","elements ; 0.002446028867822359\n","; increased 0.0006205119451624892\n","increased preference 0.018005275048966502\n","preference for 0.0027321212962426434\n","for noun 0.00041969275987922085\n","noun phrases 0.11876811507937615\n","phrases rather 0.0020321074604358765\n","rather than 0.09300464006298191\n","than clauses 0.0009019746256780016\n","clauses as 0.0005261100582453008\n","as verb 0.0002169048852502379\n","verb arguments 0.009176352402086616\n","arguments ; 0.0005049252605729728\n","; and 0.0006933501010601782\n","and more 0.0003023572203025125\n","more frequent 0.006018127685881256\n","frequent metaphoric 0.06290665743585314\n","metaphoric usage 0.030561457363318282\n","usage . 0.0003948917596156359\n",". These 0.0033368841232309034\n","These characteristics 0.0024875560347133244\n","characteristics can 0.0002647670479494727\n","can potentially 0.004597222989102609\n","potentially be 0.0027630356952044696\n","be useful 0.007014515354046703\n","useful for 0.0034690394465331166\n","for automatic 0.0012118579810150407\n","automatic prediction 0.0013219397106188763\n","prediction of 0.00041134732608499034\n","of text 0.00034464277113955096\n","text readability 0.004430912190212213\n","readability . 0.00031691293531920694\n","We propose 0.005062366244729716\n","propose an 0.0030532558619023507\n","an approach 0.0018790384173954756\n","approach for 0.0008692655057733936\n","for generating 0.0021638163970618294\n","generating an 0.00040874383299012543\n","an accurate 0.0026724086729226913\n","accurate and 0.0004401244056987322\n","and consistent 0.000586960381051888\n","consistent PropBank-annotated 0.9425838214994122\n","PropBank-annotated corpus 0.05762610098864215\n","corpus , 0.0003142887545363233\n",", given 0.0002630911621596396\n","given a 0.0006556286823301341\n","a FrameNet-annotated 0.0031827469032985585\n","FrameNet-annotated corpus 0.028812832220408018\n","corpus which 0.00020203166995851909\n","which has 0.0015636728118807956\n","has an 0.00029123855910830905\n","an underlying 0.0014646817977268982\n","underlying dependency 0.0011260616857606798\n","dependency annotation 0.001202366116802707\n","annotation layer 0.0016784580694209925\n","layer , 0.00035710103174093824\n",", namely 0.0028747178486607993\n","namely , 0.0006764041874312177\n",", a 0.00018471767234140495\n","a parallel 0.0005022779319406311\n","parallel Universal 0.0008303040095920585\n","Universal Dependencies 1.4731509074385527\n","Dependencies ( 0.0036506696567582615\n","( UD 0.00327964932811411\n","UD ) 0.003106437337030935\n",") treebank 0.0002856098716433591\n","treebank . 0.0004299719611584402\n",". The 0.002911870641670304\n","The PropBank 0.0004464966456292298\n","PropBank annotation 0.005543112105451841\n","annotation layer 0.0016784265811644935\n","layer of 0.0005801799901480179\n","of such 0.000340257346363513\n","such a 0.0003580913752088553\n","a multi-layer 0.0026440579490330175\n","multi-layer corpus 0.0026595753863246177\n","corpus can 0.00026785022738017234\n","can be 0.01858741185591655\n","be semi-automatically 0.0017682588290835965\n","semi-automatically derived 0.026791396738140745\n","derived from 0.02082034674298697\n","from the 0.0006940810402520464\n","the existing 0.0004731582878090956\n","existing FrameNet 0.0028233904394799184\n","FrameNet and 0.00046187842987010705\n","and UD 0.00016806681298252077\n","UD annotation 0.00526329673491569\n","annotation layers 0.004832591420809843\n","layers , 0.0004470260694242256\n",", by 0.00010736964694352865\n","by providing 0.005443338877767218\n","providing a 0.0016636394672940116\n","a mapping 0.0005988179900136533\n","mapping configuration 0.016898159497283224\n","configuration from 0.0005275791960889149\n","from lexical 0.00023573503666907212\n","lexical units 0.025458392818198807\n","units in 0.0005361029028531438\n","in [ 0.00025569252902347354\n","[ a 0.00014400901551413033\n","a non-English 0.0004325852299397543\n","non-English language 0.0021759332970690643\n","language ] 0.0002884440397539404\n","] FrameNet 0.008770172552458107\n","FrameNet to 0.00014706569996065694\n","to [ 3.0389070052873475e-05\n","[ English 0.0005003928245633122\n","English language 0.001015471141341859\n","language ] 0.0002884431705858024\n","] PropBank 0.03696953905129015\n","PropBank predicates 0.04610807670544917\n","predicates , 0.00046808621492705217\n",", and 0.0005690098796540062\n","and a 0.00015068143420236553\n","a mapping 0.0005988068582191726\n","mapping configuration 0.016897845367049063\n","configuration from 0.0005275693885962175\n","from FrameNet 0.0010589890714111155\n","FrameNet frame 0.024707504974279405\n","frame elements 0.07859463759640277\n","elements to 0.00015948064966429847\n","to PropBank 0.0003099642250156115\n","PropBank semantic 0.0028796945307856072\n","semantic arguments 0.001223399430516231\n","arguments for 0.00037942502379993345\n","for the 0.00043332970464097245\n","the given 0.00037537044131847897\n","given pair 0.003729155290070413\n","pair of 0.0008931308630376135\n","of a 0.00028464081252572873\n","a FrameNet 0.00011614958822738473\n","FrameNet frame 0.024707139745804792\n","frame and 0.00016059518540133854\n","and a 0.00015067829228843416\n","a PropBank 0.00019584499698293426\n","PropBank predicate 0.048276367677958086\n","predicate . 0.0004161650681319645\n",". The 0.002911673620651611\n","The latter 0.006543797259438704\n","latter mapping 0.0030371176117695315\n","mapping generally 0.002458618977212013\n","generally depends 0.007312162057535544\n","depends on 0.011820526279585339\n","on the 0.0008100688576566692\n","the underlying 0.0017493840923337996\n","underlying UD 0.0040328119670879565\n","UD syntactic 0.0008975050214696823\n","syntactic relations 0.00314753734933378\n","relations . 0.0005357097444354346\n",". To 0.003217245885068083\n","To demonstrate 0.002386151559810121\n","demonstrate our 0.0006578848322582312\n","our approach 0.006617295136513696\n","approach , 0.00024205684784484212\n",", we 0.002514099528329084\n","we use 0.0021051612249049175\n","use Latvian 0.0032156400045151136\n","Latvian FrameNet 0.036733405833275556\n","FrameNet , 0.00040137145638422666\n",", annotated 9.571963318277692e-05\n","annotated on 0.0001415022057618691\n","on top 0.006059255366906325\n","top of 0.0021040503587844736\n","of Latvian 0.0005221535759621724\n","Latvian UD 0.04009903690534001\n","UD Treebank 0.011426069570798466\n","Treebank , 0.00037625766576373463\n",", for 7.100721459279595e-05\n","for generating 0.0021635319549717625\n","generating Latvian 0.010249283414895194\n","Latvian PropBank 0.15484301996327335\n","PropBank in 0.00028976343627707004\n","in compliance 0.002140294514709097\n","compliance with 0.008305020411851098\n","with the 0.0004567297330526816\n","the Universal 0.0008952383921053067\n","Universal Propositions 0.4103310042391965\n","Propositions approach 0.016529198854973323\n","approach . 0.00019123860114915526\n","The Emirati 0.00967304767673846\n","Emirati Arabic 0.3511717492652909\n","Arabic FrameNet 0.0012816486622307891\n","FrameNet ( 0.0011902675656342379\n","( EAFN 0.003882535486773699\n","EAFN ) 0.0037482034032403663\n",") project 0.0005930695335733809\n","project aims 0.015217829831878159\n","aims to 0.004677783767213378\n","to initiate 0.0024419777863518076\n","initiate a 0.0026037193691623812\n","a FrameNet 0.00011614291118556977\n","FrameNet for 0.0002542247402521503\n","for Emirati 0.0038698602192346007\n","Emirati Arabic 0.3511687968611424\n","Arabic , 0.0003597950876612814\n",", utilizing 0.00021812573404941758\n","utilizing the 0.0005111657357017263\n","the Emirati 0.00108968912662667\n","Emirati Arabic 0.3511654130759319\n","Arabic Corpus 0.004083318572819161\n","Corpus . 0.000302179597473645\n",". The 0.0029114900569107076\n","The goal 0.006952053069465835\n","goal is 0.003946920359854834\n","is to 0.000341714101773441\n","to create 0.003960462306559172\n","create a 0.0022616214687539637\n","a resource 0.0005318228914698136\n","resource comparable 0.0003488912836097543\n","comparable to 0.0016245341954126663\n","to the 0.0003213343170424691\n","the initial 0.0009775894755131155\n","initial stages 0.03783055264638094\n","stages of 0.0015946179390532713\n","of the 0.0005967658465317403\n","the Berkeley 0.0012499049981552054\n","Berkeley FrameNet 0.9161122881680701\n","FrameNet . 0.0003319477978748874\n",". The 0.0029114144428316465\n","The project 0.0014233382716567476\n","project is 0.0013207300163153063\n","is divided 0.006097649645535682\n","divided into 0.08297863045917858\n","into manual 0.00010980497008395203\n","manual and 0.00017499223723849722\n","and automatic 0.00023030188536812235\n","automatic tracks 0.0012888394552091282\n","tracks , 0.0005553907010662671\n",", based 0.00021413688479142762\n","based on 0.010627598069927217\n","on the 0.0008099892284428486\n","the predominant 0.0014301514953965534\n","predominant techniques 0.007494883080211906\n","techniques being 0.00040615790476182057\n","being used 0.004233769217652381\n","used to 0.0019544072843040235\n","to collect 0.0029082094891451937\n","collect frames 0.00334483669548162\n","frames in 0.0005339314499802603\n","in each 0.0004293136268088009\n","each track 0.0011259741109639666\n","track . 0.00042580286624682365\n",". Work 0.0014035669420586487\n","Work on 0.005184328529980177\n","on the 0.0008099775951997293\n","the EAFN 0.001634435377538471\n","EAFN is 0.007452469943332148\n","is progressing 0.0021292748678712167\n","progressing , 0.0004760366244710237\n",", and 0.0005689215300478862\n","and we 0.00013007234000479397\n","we here 0.00032578836914677796\n","here report 0.0011982903300648434\n","report on 0.00353592227711024\n","on initial 0.00015799591675190388\n","initial results 0.0026291555936534586\n","results for 0.0004852820689382179\n","for annotations 1.2735737287870778e-05\n","annotations and 0.0002863087875186167\n","and evaluation 0.0003063338547040446\n","evaluation . 0.0002806017094914297\n",". The 0.0029112691676117957\n","The EAFN 0.007254071786487927\n","EAFN project 0.10549394585712826\n","project aims 0.015216364531955368\n","aims to 0.0046773333510871885\n","to provide 0.0015246266324959728\n","provide a 0.0017006725578338378\n","a general 0.001222197041424022\n","general semantic 0.0005615874571887493\n","semantic resource 0.000868796505808848\n","resource for 0.0021175480931166077\n","for the 0.00043325763485198694\n","the Arabic 0.00024747085467912924\n","Arabic language 0.0015594678459342406\n","language , 0.00016358785178754637\n",", sure 0.00015867317812007792\n","sure to 0.0006394948764245142\n","to be 0.001536230739354941\n","be of 3.248247567908094e-05\n","of interest 0.0016400382333903133\n","interest to 0.0005000274880427459\n","to researchers 0.00033274577146217183\n","researchers from 0.0007003882634826253\n","from general 0.0003699802373143445\n","general linguistics 0.0041143725354617165\n","linguistics to 0.0001898739937803861\n","to natural 0.0001427065050199398\n","natural language 0.02690383125023559\n","language processing 0.016879685365506714\n","processing . 0.0004830219104548693\n",". As 0.0030464438556529133\n","As we 0.0003656417230403863\n","we report 0.0037539182846741776\n","report here 0.010784135454421842\n","here , 0.0003814764390308996\n",", the 0.00014723225927833986\n","the EAFN 0.0016343314348545952\n","EAFN is 0.0074519960000132696\n","is well 0.0001443956844848012\n","well on 0.0006008816610205871\n","on target 0.00010975294391410622\n","target for 4.1891029924647506e-05\n","for the 0.00043324008082460443\n","the first 0.0015388390506060419\n","first release 0.0026217293670987396\n","release of 0.0006238723067004743\n","of data 0.00021273100477857922\n","data in 0.00025855546614836735\n","in the 0.000698917438976087\n","the coming 0.0001894840806301822\n","coming year 0.06441353404448379\n","year . 0.0005192737054946937\n","The FrameNet 0.00031767654776564775\n","FrameNet ( 0.001190075456364534\n","( FN 0.004086219837798039\n","FN ) 0.003944840465151984\n",") project 0.0005929738120432097\n","project at 0.0005581326579065225\n","at the 0.0007982323100488932\n","the International 0.001293062740904741\n","International Computer 0.13024174875913225\n","Computer Science 1.5980223512984617\n","Science Institute 0.06585256924791764\n","Institute in 0.00010346865770877672\n","in Berkeley 0.0011077214168001752\n","Berkeley ( 0.0009133831936356476\n","( ICSI 0.0017252776092390543\n","ICSI ) 0.0016655846231297784\n",") , 0.0004639383120884759\n",", which 0.0016167679772614207\n","which documents 6.441274271078637e-05\n","documents the 1.756495728512107e-05\n","the core 0.0009756805949582998\n","core vocabulary 0.016532061489464317\n","vocabulary of 0.0002945006574396835\n","of contemporary 0.0014807355309595393\n","contemporary English 0.0010142915573579032\n","English , 0.0004869381986431084\n",", was 6.666108420175702e-05\n","was the 8.489616691699712e-05\n","the first 0.0015387693050510603\n","first lexical 0.00014442268356700363\n","lexical resource 0.010648549418141945\n","resource based 0.00024736769186000393\n","based on 0.010626128068959177\n","on Fillmore 0.0006582450741760353\n","Fillmore { 0.0011134056366183334\n","{ ' 0.007628305758477343\n","' } 0.007628302351550211\n","} s 0.006417230108594939\n","s theory 0.0018861071925976448\n","theory of 0.0009556435089417256\n","of Frame 0.0003689074677316512\n","Frame Semantics 3.0046214232257857\n","Semantics . 0.0004269437546121871\n",". Berkeley 0.00019812345412780828\n","Berkeley FrameNet 0.9159608814247825\n","FrameNet has 0.0007982570843008452\n","has inspired 0.001719515673554533\n","inspired related 0.0014375150940587732\n","related projects 0.004724855567953996\n","projects in 0.00043791913604387695\n","in roughly 0.00031384152058479135\n","roughly a 0.00042422209639284046\n","a dozen 0.0037431268462110928\n","dozen other 0.007167356727204134\n","other languages 0.006006318651866229\n","languages , 0.000558022006382049\n",", which 0.0016167004238790614\n","which have 0.0007406682060210025\n","have evolved 0.018764203595393675\n","evolved somewhat 0.21163205646135455\n","somewhat independently 0.038831569920399406\n","independently ; 0.000708482157306125\n","; the 0.00014955358107333682\n","the current 0.001395816137107234\n","current Multilingual 0.0019108585696194019\n","Multilingual FrameNet 0.023809018589243863\n","FrameNet project 0.023097950006294284\n","project ( 0.0006141879260129567\n","( MLFN 0.015526654962670566\n","MLFN ) 0.014989447276258516\n",") is 0.00043142889463762963\n","is an 0.0010746442469926276\n","an attempt 0.006359449027142039\n","attempt to 0.005072342326653571\n","to find 0.0011759186167612915\n","find alignments 0.0007232534560068235\n","alignments between 0.00767484906824842\n","between all 0.0005502266113273102\n","all of 0.0002444112525525233\n","of them 0.0006127271976756744\n","them . 0.0006214331194618305\n",". The 0.00291086013667122\n","The alignment 0.00047822324808007943\n","alignment problem 0.0012869346210570788\n","problem is 0.0006080970259676344\n","is complicated 0.0014692631640134803\n","complicated by 0.002861289255882452\n","by the 0.00040783101881629227\n","the fact 0.0012738342904268793\n","fact that 0.005435260545657101\n","that these 0.0007495724221271686\n","these projects 0.002203004238770203\n","projects have 0.0013223179311301855\n","have adhered 0.04548772629338085\n","adhered to 0.006713810419047225\n","to the 0.0003212641160068435\n","the Berkeley 0.0012496422917162276\n","Berkeley FrameNet 0.9159197386566743\n","FrameNet model 0.00010537787920083473\n","model to 0.0004191100944259203\n","to varying 0.00028848112741732397\n","varying degrees 0.5375934597131999\n","degrees , 0.0002507703697674996\n",", and 0.0005688203059283132\n","and they 0.0001844569694728826\n","they were 0.004447598527084592\n","were also 0.0011878439878773323\n","also founded 0.005147322893757104\n","founded at 0.005878399340505069\n","at different 0.0017572765299450155\n","different times 0.0011338555530562774\n","times , 0.0004682297969539694\n",", when 0.0003092221495248028\n","when different 0.00016747485759142773\n","different versions 0.006335859685080501\n","versions of 0.0027308113151934084\n","of the 0.0005966256296731568\n","the Berkeley 0.001249611320315173\n","Berkeley FrameNet 0.9158970383065665\n","FrameNet data 0.0009283161852121528\n","data were 0.0005745583267988029\n","were available 0.000695163502051928\n","available . 0.0004468794815574857\n",". We 0.0028902648817018457\n","We describe 0.006962636777432834\n","describe several 0.0010157015724357784\n","several new 0.0006058743081768177\n","new methods 0.0005347494662499832\n","methods for 0.001314858172379231\n","for finding 0.0012241077273579864\n","finding relations 0.0006929853489988823\n","relations of 7.97639523430793e-05\n","of similarity 0.00015221135124772283\n","similarity between 0.008410822121854874\n","between semantic 0.00019369989144447908\n","semantic frames 0.010127307561952872\n","frames across 0.0005838658862253359\n","across languages 0.0075345134912136675\n","languages . 0.0009770943817077131\n",". We 0.002890233534425346\n","We will 0.0015066141326846398\n","will demonstrate 0.0007689989068816846\n","demonstrate ViToXF 0.18863541605336148\n","ViToXF , 0.0033315361044496594\n",", a 0.00018464058171355078\n","a new 0.003124803663522307\n","new tool 0.0006692043316123119\n","tool which 0.0006850779713434169\n","which provides 0.0021646979393705156\n","provides interactive 0.000854267732592462\n","interactive visualizations 0.14243271436057434\n","visualizations of 0.0007093679712353669\n","of these 0.0009568550547731938\n","these cross-lingual 0.00013148430375173553\n","cross-lingual relations 0.0001436441908570697\n","relations , 0.0003247267664583304\n",", between 1.008092773559666e-05\n","between frames 0.0010944341023479672\n","frames , 0.00036062419632171527\n",", lexical 0.00010579984437183434\n","lexical units 0.025448195400718073\n","units , 0.00044077838468606774\n",", and 0.0005687863392664082\n","and frame 0.0002809361804732685\n","frame elements 0.0785640013852521\n","elements , 0.0002900724960464235\n",", based 0.0002140786949662391\n","based on 0.010624710113725833\n","on resources 4.525639159466331e-05\n","resources such 0.0017532748649923874\n","such as 0.014333366266147488\n","as multilingual 0.00014390159141994168\n","multilingual dictionaries 0.0014535676717387035\n","dictionaries and 0.0006909367226424468\n","and on 3.319722083047744e-05\n","on shared 5.829010016280775e-05\n","shared distributional 0.0004422710918880149\n","distributional vector 0.01557738874063599\n","vector spaces 0.10785814178677716\n","spaces , 0.00045608462001599833\n",", making 0.0012581516371389632\n","making clear 0.007117756821120113\n","clear the 2.3177392113936595e-05\n","the strengths 0.0011427963167924685\n","strengths and 0.0024252087380657165\n","and weaknesses 0.002403471661568882\n","weaknesses of 0.0019918150089253146\n","of different 0.0004606435454434492\n","different alignment 0.0003292701798717683\n","alignment methods 0.0014348192106963282\n","methods . 0.00046561722256637146\n","The methodology 0.0013939340609267713\n","methodology developed 0.0015712342489925533\n","developed within 0.009464278814962503\n","within the 0.001229217565299529\n","the FrameNet 0.0004174396383455942\n","FrameNet project 0.023094866675530606\n","project is 0.0013203212466906871\n","is being 0.0013878770370933091\n","being used 0.004232519293083135\n","used to 0.0019538302898863876\n","to compile 0.0023893873252136726\n","compile resources 0.0029593694931213197\n","resources in 0.000376059947806794\n","in an 0.0004995929319173531\n","an increasing 0.004208227551985414\n","increasing number 0.014334293728722216\n","number of 0.004275474798932298\n","of specialized 0.0005808624800871402\n","specialized fields 0.022985539039571234\n","fields of 0.0007235606647174129\n","of knowledge 0.00019729959721214232\n","knowledge . 0.0002837897270149684\n",". The 0.002910462362880652\n","The methodology 0.0013939027212094425\n","methodology along 0.0009781157137483062\n","along with 0.01570732599762676\n","with the 0.0004565552763258313\n","the theoretical 0.0006391437730812791\n","theoretical principles 0.008853172051060641\n","principles on 0.00021822006565633368\n","on which 0.00012486238131996\n","which it 0.0003492249275971164\n","it is 0.003874301348701376\n","is based 0.0015691271325657902\n","based , 9.43026159604281e-06\n",", i.e 0.0018243699068888333\n","i.e . 0.003258689063491377\n",". Frame 0.00013469921023362348\n","Frame Semantics 3.0040579878351577\n","Semantics , 0.000563021587359342\n",", are 6.596263385366743e-05\n","are especially 0.0007466094126191308\n","especially appealing 0.012292346943826164\n","appealing as 0.001808301314066549\n","as they 0.0014718297718271324\n","they allow 0.004923870703466387\n","allow domain-specific 0.0017554874613578582\n","domain-specific resources 0.0023458232544830916\n","resources to 0.0002505756221635546\n","to account 0.0007199819872920966\n","account for 0.0028737975324404042\n","for the 0.0004331285759995883\n","the conceptual 0.0004143941040293506\n","conceptual background 0.006211990279250023\n","background of 0.00031131387869601327\n","of specialized 0.0005808371020693656\n","specialized knowledge 0.00315203330035176\n","knowledge and 0.0001671742612531855\n","and to 4.9431174531060944e-05\n","to explain 0.0019631397343846347\n","explain the 0.0011817386219388582\n","the linguistic 0.0003422556103313115\n","linguistic properties 0.009265275509055284\n","properties of 0.0019031356504622095\n","of terms 0.0002431018783579735\n","terms against 0.0006120824838623901\n","against this 8.626908830507378e-05\n","this background 0.00022552000447017035\n","background . 0.00016241331198370022\n",". This 0.0020960320386480863\n","This paper 0.018761392629567785\n","paper presents 0.028557076096144475\n","presents a 0.0022176465837974433\n","a methodology 0.0010245170760860708\n","methodology for 0.0023058592758335526\n","for building 0.0027787470978549497\n","building a 0.0014294168394954222\n","a multilingual 0.000848870129034045\n","multilingual resource 0.0009890522819638436\n","resource that 0.0004040200694135629\n","that accounts 0.0021554776383589875\n","accounts for 0.005040378364773541\n","for terms 1.6245323726947004e-05\n","terms of 0.002491220389911459\n","of the 0.0005965249102305393\n","the environment 0.00035441024956293476\n","environment . 0.0009568670144964358\n",". After 0.003203024585182954\n","After listing 0.3096335290615822\n","listing some 0.03328270517785555\n","some lexical 0.00036871531622481964\n","lexical and 0.00036977219513901544\n","and conceptual 0.00036280438496003064\n","conceptual differences 0.002283496287558326\n","differences that 0.00023541700731567302\n","that need 0.0005786435725620531\n","need to 0.0028075685341849873\n","to be 0.001535706996067067\n","be managed 0.003681811245805377\n","managed in 0.0009805120147584273\n","in such 0.000167221953761304\n","such a 0.00035788907025327007\n","a resource 0.0005315917389995552\n","resource , 0.00024013596232375873\n",", we 0.002512860836376853\n","we explain 0.0015888933564869039\n","explain how 0.02364766760650712\n","how the 0.0003976053774325698\n","the FrameNet 0.0004173912845135347\n","FrameNet methodology 0.007581436377403515\n","methodology is 0.0010835667311619283\n","is adapted 0.0009429763768620479\n","adapted for 0.0008813289826906077\n","for describing 0.001063328407227361\n","describing terms 0.0009912302940513838\n","terms in 0.00037773984480816384\n","in different 0.0005627647474745373\n","different languages 0.004442444664849317\n","languages . 0.0009769106977071758\n",". We 0.002889690201457405\n","We first 0.0012562405043246253\n","first applied 0.00018343146980417428\n","applied our 0.0005130011699284458\n","our methodology 0.00279282379500568\n","methodology to 0.0008368482919456437\n","to French 0.00020382611666129125\n","French and 0.0005076902962670198\n","and then 0.0016814114693185004\n","then extended 0.0017016681017159537\n","extended it 0.0009072543954600567\n","it to 0.00034119144648553807\n","to English 0.0002768907911217874\n","English . 0.00030991150372022965\n",". Extensions 0.002693706794991438\n","Extensions to 0.002684855845070252\n","to Spanish 0.00013225859732682375\n","Spanish , 0.000689143473656396\n",", Portuguese 0.0004616115776137316\n","Portuguese and 0.0005441803769603918\n","and Chinese 0.0005366259006009128\n","Chinese were 0.000297449265282148\n","were made 0.004283588674636974\n","made more 0.0006918710512544985\n","more recently 0.0015694049186196237\n","recently . 0.00024615861462478164\n",". Up 0.0028860729248848527\n","Up to 0.005273748149711404\n","to now 0.0002964302278374374\n","now , 0.0003992771815426522\n",", we 0.002512736668853828\n","we have 0.0016338863138811246\n","have defined 0.0011261591089343041\n","defined 190 0.6287185346319207\n","190 frames 0.6870532439900433\n","frames : 0.0005457134520184374\n",": 112 0.002075850705786093\n","112 frames 0.04041488715389303\n","frames are 0.0015606360866623034\n","are new 2.553563185928233e-05\n","new ; 2.4960034076481157e-05\n","; 38 0.005324232931490016\n","38 are 0.00090783547956335\n","are used 0.002042010935180973\n","used as 0.001632666908561166\n","as such 0.00013678526150707326\n","such ; 1.9876737605862267e-05\n","; and 0.0006928805653857403\n","and 40 0.0003040294925705508\n","40 are 0.00025314522466855777\n","are slightly 0.0009016125879474203\n","slightly different 0.00430908634996259\n","different ( 1.4245970182389018e-05\n","( a 5.926472195040396e-05\n","a different 0.00016097195226564043\n","different number 0.0002104596817586504\n","number of 0.004274769326742792\n","of obligatory 0.0009219670339288643\n","obligatory participants 0.15431139699885915\n","participants ; 0.0006703362026038885\n","; a 0.00010132371158791559\n","a significant 0.0016647020723677152\n","significant alternation 0.011273459793542011\n","alternation , 0.0003568647120379211\n",", etc 0.0026540660597588832\n","etc . 0.003134550436200147\n",". ) 1.6087728020244367e-05\n",") when 0.0002422669886915249\n","when compared 0.010732454656958858\n","compared to 0.004478364403925136\n","to Berkeley 0.000197405712333603\n","Berkeley FrameNet 0.9156512661020928\n","FrameNet . 0.0003317807492932811\n","This paper 0.018759186170520335\n","paper presents 0.02855371760383707\n","presents the 0.000876099213635861\n","the results 0.00037528476471226224\n","results and 0.00010490081882563805\n","and findings 0.00015499049521144382\n","findings of 0.000331418175051367\n","of the 0.000596455283472068\n","the Financial 0.0006534562198430215\n","Financial Narrative 6.397545490671349\n","Narrative Summarisation 9.329753833651543\n","Summarisation shared 0.015614128111575525\n","shared task 0.027702185848117476\n","task ( 0.000216284832845145\n","( FNS 0.011641027724317634\n","FNS 2020 0.3759718102304515\n","2020 ) 0.003973281926894269\n",") on 0.0001122407128444143\n","on summarising 0.0019740028458801063\n","summarising UK 1.620446335828727\n","UK annual 0.9722678001432853\n","annual reports 0.40388397211543303\n","reports . 0.0003188342831090823\n",". The 0.002909888672965621\n","The shared 0.0006659437057784534\n","shared task 0.02770196127057037\n","task was 0.0015961326409693894\n","was organised 0.02847581468882955\n","organised as 0.005371550130959256\n","as part 0.004805520913567082\n","part of 0.0034327834395153816\n","of the 0.0005964448148964711\n","the 1st 0.000683146785012356\n","1st Financial 0.12116350743558273\n","Financial Narrative 6.3974331850168555\n","Narrative Processing 0.133907549561029\n","Processing and 0.0001654486841871658\n","and Financial 0.0001317381198938977\n","Financial Narrative 6.397413875457635\n","Narrative Summarisation 9.329561894798495\n","Summarisation Workshop 0.2135888711166607\n","Workshop ( 0.0006964584096453096\n","( FNP-FNS 0.011640793000272487\n","FNP-FNS 2020 0.5639463440422748\n","2020 ) 0.003973201812034386\n",") . 0.0005706164055819665\n",". The 0.0029098333795789812\n","The shared 0.0006659310515958067\n","shared task 0.02770143488124889\n","task included 0.0007034801184020252\n","included one 0.0005586043467554336\n","one main 0.00023177098375049373\n","main task 0.0007297037377173994\n","task which 0.00021855681956880235\n","which is 0.0018320264200815234\n","is the 0.00017483536975893037\n","the use 0.0005647087325159089\n","use of 0.0014086305765547383\n","of either 9.323918857960711e-05\n","either abstractive 0.0036332496170032864\n","abstractive or 0.0001775108348029392\n","or extractive 0.0005897597365691385\n","extractive summarisation 0.06539869890605722\n","summarisation methodologies 0.05705355230766713\n","methodologies and 0.00045284177453110326\n","and techniques 7.581893184501353e-05\n","techniques to 0.0007333051131066206\n","to automatically 0.0016800965628611783\n","automatically summarise 0.026479186792192248\n","summarise UK 1.2602966135816858\n","UK financial 0.05587521924354852\n","financial annual 0.22509731045588174\n","annual reports 0.4038677750329756\n","reports . 0.0003188214968005147\n",". FNS 0.0004208427368769479\n","FNS summarisation 0.9128482508527223\n","summarisation shared 0.0034221115234428657\n","shared task 0.027700865355293727\n","task is 0.0006114289314513028\n","is the 0.00017483194040561747\n","the first 0.0015381187301030423\n","first to 0.0001378026225873665\n","to target 0.00012109888175794731\n","target financial 0.0009477090330007751\n","financial annual 0.22509443268434795\n","annual reports 0.4038626117585551\n","reports . 0.0003188174208059561\n",". The 0.002909734777238641\n","The data 0.00033593410945272284\n","data for 0.00047118000622551064\n","for the 0.00043303137262856164\n","the shared 0.0006414705723323737\n","shared task 0.0277003125298662\n","task was 0.0015960376437991152\n","was created 0.014081679099220866\n","created and 0.00021629859396662284\n","and collected 7.041325104842389e-05\n","collected from 0.007200557417488263\n","from publicly 0.0004571433692075099\n","publicly available 0.11643356479059129\n","available UK 0.0028831597972559506\n","UK annual 0.9722014822809405\n","annual reports 0.40385642340752115\n","reports published 0.0051776463729480285\n","published by 0.0015620848476246838\n","by firms 0.007252532844061129\n","firms listed 2.0823834923574918\n","listed on 0.0008635668406545029\n","on the 0.0008095202176648839\n","the London 0.00017194867897572544\n","London Stock 2.8057209830033343\n","Stock Exchange 10.661739732253924\n","Exchange ( 0.0007760087724129811\n","( LSE 0.0025866932757575003\n","LSE ) 0.002497196115802334\n",") . 0.0005705834151765459\n",". A 0.002181476798669335\n","A total 0.026689407773957882\n","total number 0.016037953618976322\n","number of 0.004274256319326851\n","of 24 0.0009833134834073265\n","24 systems 0.0021880124934064967\n","systems from 0.00011603545122427387\n","from 9 0.0011276011563733663\n","9 different 0.0027180997966920807\n","different teams 0.0005089367401360204\n","teams participated 0.15127532510194855\n","participated in 0.008029854089633949\n","in the 0.0006985686367382579\n","the shared 0.0006414527093190117\n","shared task 0.027699541160074802\n","task . 0.0006826414580333759\n",". In 0.002729469344531876\n","In addition 0.02535346503101599\n","addition we 0.00022174909995064797\n","we had 0.002127585703516595\n","had 2 0.0011328712624171506\n","2 baseline 0.0001323282456354362\n","baseline summarisers 0.07922050445842543\n","summarisers and 0.0017563664276411509\n","and additional 0.00013911771590663198\n","additional 2 0.000489787681406625\n","2 topline 0.14840565956975219\n","topline summarisers 88.84552152382734\n","summarisers to 0.0022369927238284058\n","to help 0.0017497580764586557\n","help evaluate 0.00034719009629023147\n","evaluate and 7.770852991849378e-05\n","and compare 0.0011664623509669394\n","compare against 0.007456590976839093\n","against the 0.0005384594567078518\n","the results 0.00037523735242021616\n","results of 0.0004197649744922546\n","of the 0.0005963817280075097\n","the participants 0.0004869586862633556\n","participants . 0.00046283107880114\n","This paper 0.018756550346238944\n","paper presents 0.028549705566729963\n","presents the 0.0008759761143461181\n","the FinTOC-2020 0.002450144875674446\n","FinTOC-2020 Shared 0.23420916204228134\n","Shared Task 0.3098344244064803\n","Task on 0.001288799197106284\n","on structure 5.6209406444818875e-06\n","structure extraction 0.0003890690996650603\n","extraction from 0.001373247684328367\n","from financial 0.0009998012569666064\n","financial documents 0.0023519091547292608\n","documents , 0.00041310628901032164\n",", its 0.00016153895142199883\n","its participants 0.00038379477972157736\n","participants results 7.678187433546852e-05\n","results and 0.00010488536003848385\n","and their 0.000783614720036217\n","their findings 0.00041922227820639566\n","findings . 0.00022002832758088217\n",". This 0.0020954494719452287\n","This shared 0.00045978359023088116\n","shared task 0.027698211406614343\n","task was 0.0015959165812434151\n","was organized 0.016064266073621034\n","organized as 0.004367184350081473\n","as part 0.004804870395379048\n","part of 0.003432318747339526\n","of The 3.259550795262735e-06\n","The 1st 0.0005272445907490055\n","1st Joint 0.2307573817991713\n","Joint Workshop 0.24407030736851748\n","Workshop on 0.008767856628723378\n","on Financial 0.004490200232171139\n","Financial Narrative 6.396587267163238\n","Narrative Processing 0.13388984327421863\n","Processing and 0.00016542680728919452\n","and MultiLing 0.0007526897172474964\n","MultiLing Financial 1.903740466558661\n","Financial Summarisation 2.498659359397359\n","Summarisation ( 0.008729449254266932\n","( FNP-FNS 0.011639253828832229\n","FNP-FNS 2020 0.5638717778072714\n","2020 ) 0.003972676466499409\n",") , 0.00046368453331761936\n",", held 0.000247986747176137\n","held at 0.008438508192272905\n","at The 2.3973320154776803e-05\n","The 28th 0.009666037422689075\n","28th International 3.90507911807227\n","International Conference 0.7257226660039087\n","Conference on 0.005257360543131056\n","on Computational 0.0013561559420299139\n","Computational Linguistics 1.544976623112415\n","Linguistics ( 0.002688310891287634\n","( COLING 0.0020242004098195198\n","COLING { 0.0016934154532217737\n","{ ' 0.007624265077823743\n","' } 0.0076242616745049235\n","} 2020 7.690794395196792e-05\n","2020 ) 0.003972623956333418\n",") . 0.00057053341619172\n",". This 0.0020953890787133896\n","This shared 0.0004597703387498561\n","shared task 0.027697413113009533\n","task aimed 0.00237709259724277\n","aimed to 0.001505991226102795\n","to stimulate 0.0037578822049220855\n","stimulate research 0.04573230819594442\n","research in 0.0008454890803273293\n","in systems 1.158720054995216e-05\n","systems for 0.0006481743367513854\n","for extracting 0.003028071960679923\n","extracting table-of-contents 0.30114896092123905\n","table-of-contents ( 0.0051728743137225\n","( TOC 0.005172869050204153\n","TOC ) 0.004993892636241355\n",") from 0.0001434197734306206\n","from investment 0.0007835886578256389\n","investment documents 0.006451534911927397\n","documents ( 0.00022238898424592447\n","( such 0.00036359151956645076\n","such as 0.014327406182421351\n","as financial 0.00048679450895449896\n","financial prospectuses 2.6257695655101405\n","prospectuses ) 0.007490812301736666\n",") by 0.00013044500498761202\n","by detecting 0.0007097930728985558\n","detecting the 0.00034105609023059774\n","the document 0.0004396226258512147\n","document titles 0.0038974273270675638\n","titles and 0.0012467760949265373\n","and organizing 0.0007747968640417329\n","organizing them 0.011903750745049579\n","them hierarchically 0.005621215191158684\n","hierarchically into 0.002846814227595049\n","into a 0.0010100446818332466\n","a TOC 0.0010599736375320758\n","TOC . 0.0005610408384240058\n",". For 0.0031408990761259362\n","For the 0.0005925670674294981\n","the second 0.0013519994964277882\n","second edition 0.06342688223469084\n","edition of 0.0030244900784981393\n","of this 0.0003263515832625474\n","this shared 0.0007775664172969861\n","shared task 0.027696419382824865\n","task , 0.00042998779676196516\n",", two 5.674515642544388e-05\n","two subtasks 0.01101408715656627\n","subtasks were 0.0021208953614390473\n","were presented 0.002578688445612373\n","presented to 0.0003376223887148956\n","to the 0.0003210935055433958\n","the participants 0.0004869111953378268\n","participants : 0.0004084521384638911\n",": one 0.001665581550288633\n","one with 0.00015235578178842993\n","with English 0.00022707166007839615\n","English documents 0.0004948113389729366\n","documents and 0.0003208736629809964\n","and the 0.0001820702728687051\n","the other 0.0005156892759058643\n","other one 0.0004899716908494364\n","one with 0.00015235440176686001\n","with French 0.00016992573453912475\n","French documents 0.0012497112837099097\n","documents . 0.0006693054912073631\n","We present 0.007328439650836491\n","present the 0.00043287151882056755\n","the FinCausal 0.001959904072408085\n","FinCausal 2020 0.5262365468870893\n","2020 Shared 0.05416946514997329\n","Shared Task 0.30980093429119865\n","Task on 0.0012886598903400694\n","on Causality 0.0017268473375061874\n","Causality Detection 0.6618399731113414\n","Detection in 0.0009035783277323623\n","in Financial 0.0004704830984554194\n","Financial Documents 1.4805580331850845\n","Documents and 0.0005853743332591293\n","and the 0.00018206704431776414\n","the associated 0.00030441969863254147\n","associated FinCausal 0.07884566677873171\n","FinCausal dataset 0.009178520875948726\n","dataset , 0.0004719195205768629\n",", and 0.0005685044868320517\n","and discuss 0.0011034006658196547\n","discuss the 0.0012673248434297987\n","the participating 0.0007561180281029998\n","participating systems 0.026585840561661273\n","systems and 0.00018487965435808612\n","and results 6.659370186838629e-05\n","results . 0.00026601121577085217\n",". Two 0.0031482988362913\n","Two sub-tasks 0.013810788673692752\n","sub-tasks are 0.0004112342858561797\n","are proposed 0.00036244355994892354\n","proposed : 0.00010463922230558513\n",": a 0.0002887562959474117\n","a binary 0.0014742147079926365\n","binary classification 0.035324591352550526\n","classification task 0.0034831130126515048\n","task ( 0.0002162261233092056\n","( Task 0.0005567147034463147\n","Task 1 0.024510596031703528\n","1 ) 0.009163639873941599\n",") and 0.00037817125595503295\n","and a 0.00015054438233704\n","a relation 0.00024051837303150359\n","relation extraction 0.0484121911608716\n","extraction task 0.0013081885344554975\n","task ( 0.00021622436285347507\n","( Task 0.0005567101708275091\n","Task 2 0.018777306820107014\n","2 ) 0.009350085351674026\n",") . 0.0005704683285677097\n",". A 0.0021810367967644993\n","A total 0.026684024544877172\n","total of 0.0022683678521277623\n","of 16 0.0007617111712653199\n","16 teams 0.04276453567270782\n","teams submitted 0.06731068296909794\n","submitted runs 0.14527240344371212\n","runs across 0.006543706065981472\n","across the 0.0003177530276515653\n","the two 0.0003719374941504489\n","two Tasks 0.0010667540460382512\n","Tasks and 0.00015963878842982434\n","and 13 0.0005316395095841529\n","13 of 0.00021139059377333133\n","of them 0.0006123404347488331\n","them contributed 0.01064956618303707\n","contributed with 0.0005337752567846374\n","with a 0.0007589562264318918\n","a system 0.0004096320380002517\n","system description 0.004231136125974037\n","description paper 0.0006462069506459614\n","paper . 3.9969612072799606e-05\n",". This 0.0020951017269851347\n","This workshop 0.0006855453198344725\n","workshop is 0.00037466873364379566\n","is associated 0.0010354646740656573\n","associated to 0.00041686833076622494\n","to the 0.0003210624052971419\n","the Joint 0.0006221415588634118\n","Joint Workshop 0.2440290440302194\n","Workshop on 0.008766374305713878\n","on Financial 0.004489441104849957\n","Financial Narrative 6.395505840077584\n","Narrative Processing 0.13386720743781774\n","Processing and 0.00016539883971537493\n","and MultiLing 0.000752562465549337\n","MultiLing Financial 1.903418615201358\n","Financial Summarisation 2.4982369294919\n","Summarisation ( 0.008727973430678744\n","( FNP-FNS 0.011637286068052232\n","FNP-FNS 2020 0.5637764482624825\n","2020 ) 0.003972004836093176\n",") , 0.00046360614175814703\n",", held 0.0002479448221726563\n","held at 0.008437081565695571\n","at The 2.3969267190819936e-05\n","The 28th 0.009664403269401298\n","28th International 3.9044189201442427\n","International Conference 0.7255999743542287\n","Conference on 0.005256471726691835\n","on Computational 0.0013559266687457137\n","Computational Linguistics 1.5447154275878403\n","Linguistics ( 0.002687856402358596\n","( COLING 0.0020238581965199183\n","COLING { 0.0016931291627504163\n","{ ' 0.007622976114731625\n","' } 0.007622972712563443\n","} 2020 7.689494187829678e-05\n","2020 ) 0.003971952343680561\n",") , 0.00046360001493296463\n",", Barcelona 0.001109833584408466\n","Barcelona , 0.0011098335843098492\n",", Spain 0.0003699427742101366\n","Spain on 0.0007674044885010837\n","on September 0.0013813265007778235\n","September 12 0.20737096901643876\n","12 , 0.00010364143610861205\n",", 2020 0.0005916962399396958\n","2020 . 0.0005269314750814799\n","Identifying causal 0.03802374764260296\n","causal relationships 0.054722006716080754\n","relationships in 0.0004480286512919\n","in a 0.0005351413173398171\n","a text 0.0003250171462072229\n","text is 0.00038475989932436496\n","is essential 0.004164344449615997\n","essential for 0.0022123962425056047\n","for achieving 0.0006798676885039277\n","achieving comprehensive 0.004342212977869024\n","comprehensive natural 0.0003373814345805343\n","natural language 0.026882384594462436\n","language understanding 0.009208708640016711\n","understanding . 0.00035741840469780085\n",". The 0.0029088515127751523\n","The present 0.000471593610353989\n","present work 0.0007378996026360824\n","work proposes 0.008336129125165173\n","proposes a 0.003840105167710317\n","a combination 0.002107714437198651\n","combination of 0.0030496352109692653\n","of features 0.00021379038380025685\n","features derived 0.007504184058608697\n","derived from 0.02079877649476666\n","from pre-trained 0.000624127508467453\n","pre-trained BERT 0.01044554797707714\n","BERT with 0.00044628063104828363\n","with linguistic 0.0003472947704932038\n","linguistic features 0.008835095482014076\n","features for 0.0005848321476003259\n","for training 0.000900729904694362\n","training a 0.00017011714026448268\n","a supervised 0.000893512641841842\n","supervised classifier 0.006621013482138746\n","classifier for 0.0005935181224167354\n","for the 0.00043289477579356477\n","the task 0.0006501166893267562\n","task of 0.0006884055961599642\n","of Causality 0.0005759880400054241\n","Causality Detection 0.6617415969834768\n","Detection . 0.0005349297962426965\n",". The 0.00290877372088681\n","The Linguistic 0.001723813884315225\n","Linguistic features 0.002209087189616573\n","features help 0.0007309478542315409\n","help to 0.0006133075737427479\n","to inject 0.00394649162470943\n","inject knowledge 0.003922443302703091\n","knowledge about 0.005704530172331313\n","about the 0.0009061311715325759\n","the semantic 0.00038160372972687154\n","semantic and 0.00015248114315353242\n","and syntactic 0.0006873572704537585\n","syntactic structure 0.013092721494548541\n","structure of 0.0009523105885133634\n","of the 0.0005962104760958614\n","the input 0.0009913778352808908\n","input sentences 0.0033421294435597272\n","sentences . 0.0006119127014534354\n",". Experiments 0.0033566579218225674\n","Experiments on 0.006645903440920132\n","on the 0.0008092432514112944\n","the FinCausal 0.0019595442828737038\n","FinCausal Shared 0.09365634435570314\n","Shared Task1 0.07204334059800818\n","Task1 datasets 0.008527701432178849\n","datasets indicate 0.0022416564914889874\n","indicate that 0.00969671921398537\n","that the 0.0003588310645867068\n","the combination 0.0005632101029957885\n","combination of 0.003049454544603006\n","of Linguistic 0.0004234107181877575\n","Linguistic features 0.0022090013953889533\n","features with 0.0002799502782119243\n","with BERT 0.0004842332192512875\n","BERT improves 0.00019030094171691424\n","improves overall 0.0020765404634724787\n","overall performance 0.007416485155690316\n","performance for 0.00033141496938485736\n","for causality 0.0006627487911538116\n","causality detection 0.0195774458585689\n","detection . 0.00043520817883058524\n",". The 0.002908639843730656\n","The proposed 0.0037588833825403897\n","proposed system 0.0014593627479193144\n","system achieves 0.00480824042866046\n","achieves a 0.0007657805634520121\n","a weighted 0.001734077302219468\n","weighted average 0.06929364155222714\n","average F1 0.024756914779431472\n","F1 score 0.11306220246167259\n","score of 0.001459686856539963\n","of 0.952 0.0046076417502262145\n","0.952 on 0.01381193818269868\n","on the 0.0008092195851557053\n","the post-evaluation 0.0021772077515903993\n","post-evaluation dataset 0.005098147968732057\n","dataset . 0.0005041996126454635\n","This document 0.0007046531386306472\n","document describes 0.001888615528300021\n","describes a 0.0009222083990486827\n","a system 0.00040957126591325264\n","system for 0.0008573576048945684\n","for causality 0.0006627344028373531\n","causality extraction 0.008194471890215407\n","extraction from 0.0013728023573242537\n","from financial 0.0009994770341325697\n","financial documents 0.0023511464605021374\n","documents submitted 0.00038803471540369\n","submitted as 0.00032127604883745215\n","as part 0.0048033749151014694\n","part of 0.003431250463628275\n","of the 0.0005961784618323809\n","the FinCausal 0.0019594588346698695\n","FinCausal 2020 0.5261170000311328\n","2020 Workshop 0.014453763433627712\n","Workshop . 0.0004098811160402892\n",". The 0.0029085487719546312\n","The main 0.007423483597260253\n","main contribution 0.039718274625343206\n","contribution of 0.001451686665821188\n","of this 0.00032626522610078274\n","this paper 0.013107182888962305\n","paper is 0.0003259558448926424\n","is a 0.0007145852291979066\n","a description 0.0007294765852773765\n","description of 0.001880240385865698\n","of the 0.000596166030848709\n","the robust 2.984318069148511e-05\n","robust post-processing 0.00621118797107752\n","post-processing used 0.0007193483474545644\n","used to 0.001952481630216619\n","to detect 0.005010880329343442\n","detect the 0.00043748731666988375\n","the number 0.0008118531743756811\n","number of 0.004272512678993494\n","of cause 9.214803298409882e-05\n","cause and 0.0004634977713007515\n","and effect 0.00012142949505288759\n","effect clauses 0.005445488369686801\n","clauses in 0.0006671790326922867\n","in a 0.0005350629786474394\n","a document 0.0009530735792779754\n","document and 0.00027495260356971414\n","and extract 0.00031260770633516203\n","extract them 0.0014553977408910826\n","them . 0.0006209135257533007\n",". The 0.0029084263128553154\n","The proposed 0.003758607433208341\n","proposed system 0.0014592556123478612\n","system achieved 0.003743486352452561\n","achieved a 0.0005164970320830951\n","a weighted-average 0.003178908366974881\n","weighted-average F1 0.3955864723515246\n","F1 score 0.1130539072269049\n","score of 0.0014595797611163368\n","of more 0.00012020988862809233\n","more than 0.013563361014324607\n","than 95 0.011341157357774375\n","95 { 0.006674525250330358\n","{ \\ 0.007785744398195094\n","\\ % 0.08111138745394653\n","% } 0.007784254323684954\n","} for 5.57575893064319e-05\n","for the 0.0004328319877315912\n","the official 0.001696988302735278\n","official blind 0.12268541090594048\n","blind test 0.08352895632909497\n","test set 0.02591780701676303\n","set during 0.00019821004261107086\n","during the 0.00086616067752174\n","the post-evaluation 0.0021770261683492424\n","post-evaluation phase 0.5103875197593825\n","phase and 0.00030874484472694395\n","and exact 0.00017390403913882815\n","exact clauses 0.01782560438624146\n","clauses match 0.010645149617194125\n","match for 0.00026133868679044934\n","for 83 0.00041417458739547315\n","83 { 0.005283893017205145\n","{ \\ 0.007785588006831072\n","\\ % 0.0811097581790299\n","% } 0.007784097962309269\n","} of 4.739892900443255e-05\n","of the 0.0005961271291456063\n","the documents 0.0001433127569796902\n","documents . 0.0006690918526394631\n","In this 0.014300752613951149\n","this paper 0.013106151411156407\n","paper , 0.0013840042149554502\n",", we 0.002511219840640405\n","we describe 0.006200249510866856\n","describe the 0.0011429108470510595\n","the results 0.00037506938661185636\n","results of 0.00041957707704059566\n","of team 0.00011335517278185027\n","team LIORI 0.9364270665725355\n","LIORI at 0.08809970252380858\n","at the 0.0007974605773237876\n","the FinCausal 0.00195924912681767\n","FinCausal 2020 0.5260606932263674\n","2020 Shared 0.05415136321729239\n","Shared task 0.0010280795877985324\n","task held 0.0014361139454889154\n","held as 0.0021017210704733368\n","as a 0.0010648778486700484\n","a part 0.00045859811227730616\n","part of 0.0034308427157799374\n","of the 0.000596107616041586\n","the 1st 0.0006827605712423224\n","1st Joint 0.23065715852559593\n","Joint Workshop 0.24396430198326885\n","Workshop on 0.008764048545612165\n","on Financial 0.004488250036489492\n","Financial Narrative 6.39380908440837\n","Narrative Processing 0.1338316918822615\n","Processing and 0.00016535495868071016\n","and MultiLingual 0.001755513218755466\n","MultiLingual Financial 4.440131808142821\n","Financial Summarisation 2.4975741391220243\n","Summarisation . 0.00021030786455975174\n",". The 0.0029081910911134957\n","The shared 0.0006655552052338738\n","shared task 0.027685800405521155\n","task consisted 0.008264813346219306\n","consisted of 0.003666763250666088\n","of two 0.00034019567196624437\n","two subtasks 0.011009878773232862\n","subtasks : 0.005779294419032391\n",": classifying 0.0004864389355509452\n","classifying whether 0.009028402180828154\n","whether a 0.0011714914529905782\n","a sentence 0.0009301874422550749\n","sentence contains 0.002353174555300572\n","contains any 0.0009199041843006988\n","any causality 0.004433061843716144\n","causality and 0.0002257062519486996\n","and labelling 0.00030618973458597647\n","labelling phrases 0.003468893089618269\n","phrases that 0.0005890084055115769\n","that indicate 0.00039197927441803195\n","indicate causes 0.00576132665432558\n","causes and 0.00037174992794416627\n","and consequences 0.0004328582083903593\n","consequences . 0.0005531281873413603\n",". Our 0.0033180491911688308\n","Our team 0.009250662159928336\n","team ranked 0.014942331214434617\n","ranked 1st 0.2834062106340254\n","1st in 0.0023943004280614004\n","in the 0.0006982047418396283\n","the first 0.00153725371358975\n","first subtask 0.005485807789403192\n","subtask and 0.00018896543863865367\n","and 4th 0.0006929432683265998\n","4th in 0.0021039937064476493\n","in the 0.0006981980062326869\n","the second 0.0013514289031237546\n","second one 0.006907242503644537\n","one . 0.00011823713896661172\n",". We 0.0028876323653060374\n","We used 0.0003825708733915823\n","used Transformer-based 0.0004217551877925606\n","Transformer-based models 0.00811809690035608\n","models with 0.0005538925524349227\n","with joint-task 0.01013837165214501\n","joint-task learning 0.03506131265016926\n","learning and 0.0002412045874704738\n","and their 0.0007832256736058715\n","their ensembles 0.0018186996895551717\n","ensembles . 0.0006085208068148053\n","This paper 0.018746939416007642\n","paper describes 0.031852184538279996\n","describes the 0.001469099116702381\n","the approach 9.065464017564098e-05\n","approach we 7.768554968412515e-05\n","we built 0.0013874435771533647\n","built for 0.0007452230615277687\n","for the 0.0004327797619958385\n","the Financial 0.0006530323694517792\n","Financial Document 0.3288783878456171\n","Document Causality 0.8221959676420774\n","Causality Detection 0.6615682784427074\n","Detection Shared 0.015502478559503492\n","Shared Task 0.3096733805829565\n","Task ( 0.00031366669079353244\n","( FinCausal-2020 0.003102261599610638\n","FinCausal-2020 ) 0.0029949262600874253\n",") Task 3.907270850494494e-05\n","Task 2 0.018770361884896627\n","2 : 0.0018064414688581424\n",": Cause 0.017632435610811917\n","Cause and 0.0026331010890243665\n","and Effect 0.0006582733003063786\n","Effect Detection 0.22052155555039993\n","Detection . 0.0005347867716751323\n",". Our 0.003317888425489342\n","Our approach 0.005193258455413756\n","approach is 0.0011643844383081435\n","is based 0.0015678092847001381\n","based on 0.0106152060681622\n","on a 0.0005222982086238304\n","a multi-class 0.0013679328000492789\n","multi-class classifier 0.02924539963184295\n","classifier using 0.001230838530756607\n","using BiLSTM 0.0017598666261858856\n","BiLSTM with 0.0006034555572788319\n","with Graph 0.0004121156697018797\n","Graph Convolutional 0.48009242031474997\n","Convolutional Neural 0.3457878909229813\n","Neural Network 0.23815320438859403\n","Network ( 0.008842489983168039\n","( GCN 0.0068154670754301495\n","GCN ) 0.0065796582998227686\n",") trained 0.00016140672759490921\n","trained by 0.00038307715522813656\n","by minimizing 0.006040182560200622\n","minimizing the 0.002040688253532994\n","the binary 0.00034134984047099994\n","binary cross 0.010175143486241272\n","cross entropy 0.5306148880795468\n","entropy loss 0.023702391677577445\n","loss . 0.00035364080251874325\n",". In 0.002727896554561186\n","In our 0.0009297828889357618\n","our approach 0.006608886949331878\n","approach , 0.00024174928127010178\n",", we 0.0025109050336364737\n","we have 0.0016326953092529453\n","have not 0.001376098040941827\n","not used 0.00019777911633021936\n","used any 4.69138166658097e-05\n","any extra 0.012089141596871696\n","extra data 0.0014143537767226054\n","data source 0.0004696571003015872\n","source apart 0.0032178744480342567\n","apart from 0.015639027312723427\n","from combining 0.00028978181893093974\n","combining the 0.0005931497769810981\n","the trial 0.0003265028869418572\n","trial and 0.0007372400967172715\n","and practice 0.00021003928441669291\n","practice dataset 0.0002613800362074838\n","dataset . 0.0005040772491394047\n",". We 0.002887440545481619\n","We achieve 0.0012657692226740555\n","achieve weighted 0.001963371933133494\n","weighted F1 0.032103390076399806\n","F1 score 0.11303312854355771\n","score to 0.00011091676665568405\n","to 75.61 0.006706982459564763\n","75.61 percent 9.865846994348773\n","percent and 0.0001950350297237257\n","and are 8.607095152962835e-05\n","are ranked 0.0012127585469107036\n","ranked at 0.0020303879731617706\n","at 7-th 0.08808758552317998\n","7-th place 1.1099035775592054\n","place . 0.00022430224054670163\n","Financial causality 0.1902691817949801\n","causality detection 0.019572142098521855\n","detection is 0.0006070831045405993\n","is centered 0.004720329592492978\n","centered on 0.006398983538050732\n","on identifying 0.0007748775041506252\n","identifying connections 0.0036458930581028787\n","connections between 0.030006762309564176\n","between different 0.0015018098000841229\n","different assets 0.007761202967284647\n","assets from 0.0032197277680558116\n","from financial 0.0009992253151244169\n","financial news 0.027351003436444473\n","news in 6.0679140364134296e-05\n","in order 0.004842151780419193\n","order to 0.004935600106086922\n","to improve 0.0031898704157321935\n","improve trading 0.009554334683893625\n","trading strategies 0.08908857887264773\n","strategies . 0.0003600811458862631\n",". FinCausal 0.00033644951131705256\n","FinCausal 2020 0.5259849503942119\n","2020 - 0.0009025910585202445\n","- Causality 0.03999606430897503\n","Causality Identification 0.4773723803488874\n","Identification in 0.001348414310869235\n","in Financial 0.0004702587022356938\n","Financial Documents 1.4798518832161942\n","Documents { 0.00043252184963320087\n","{ -- 0.007490834979404723\n","-- } 0.004676317074203385\n","} is 2.8281924898015433e-05\n","is a 0.0007144081318999134\n","a competition 0.00019189361130893625\n","competition targeting 0.02792151967226842\n","targeting to 0.00018630047202995733\n","to boost 0.002094106709975012\n","boost results 0.00022373382551497717\n","results in 0.0006580839156024228\n","in financial 0.000787615961118222\n","financial causality 0.037490519523896024\n","causality by 0.0004141584375042283\n","by obtaining 0.0009544386782587914\n","obtaining an 0.0017763087444077634\n","an explanation 0.003221214955721868\n","explanation of 0.0005843842947996268\n","of how 0.00025986911734628896\n","how different 0.0012942142328151696\n","different individual 0.00026128024764947544\n","individual events 0.0018182339429775778\n","events or 0.0007205551945392943\n","or chain 0.0006452028626949639\n","chain of 0.0006773977326001738\n","of events 0.0006262199201612868\n","events interact 0.003057061174968041\n","interact and 0.00033117910763981917\n","and generate 0.0002186954514412134\n","generate subsequent 0.00179589428634739\n","subsequent events 0.006075890494735828\n","events in 0.000780885554804563\n","in a 0.0005349336918292201\n","a financial 0.00025049645583794346\n","financial environment 0.0056926134311421715\n","environment . 0.0009560429628412878\n",". The 0.002907732713591505\n","The competition 0.0010936241408300745\n","competition is 0.00028087798020193504\n","is divided 0.006089938845613281\n","divided into 0.08287369960017288\n","into two 0.001961965928604068\n","two tasks 0.0018014076496370495\n","tasks : 0.002340838357343676\n",": ( 0.0014269615670873704\n","( a 5.9217895095925194e-05\n","a ) 4.644970659752378e-05\n",") a 7.628463014323278e-05\n","a binary 0.0014734972341445584\n","binary classification 0.03530739950795562\n","classification task 0.0034814178442827276\n","task for 0.00026143778673976334\n","for determining 0.00279033173345645\n","determining whether 0.05069059271351045\n","whether sentences 0.0005748368342901397\n","sentences are 0.0009427198126490547\n","are causal 9.008913033803398e-05\n","causal or 0.00045074613037759996\n","or not 0.002486573541922702\n","not , 2.3606800132382284e-05\n",", and 0.0005682127360382392\n","and ( 0.0001526877842428893\n","( b 0.009860309533567535\n","b ) 0.012940097243228647\n",") a 7.62834578522888e-05\n","a sequence 0.0013830543995658332\n","sequence labeling 0.12680103603685844\n","labeling task 0.002197413393753524\n","task aimed 0.0023756637434315117\n","aimed at 0.06482558502602288\n","at identifying 0.00395427090954989\n","identifying elements 0.0015775349492141825\n","elements related 0.0016063812845463646\n","related to 0.0026059481777327203\n","to cause 0.00029508461440484805\n","cause and 0.00046336771775292454\n","and effect 0.00012139542306739333\n","effect . 0.00013088691270403806\n",". Various 0.0026651972102104\n","Various Transformer-based 0.036221610685048124\n","Transformer-based language 0.0033478141963476553\n","language models 0.003078951245155059\n","models were 0.0010088516718326766\n","were fine-tuned 0.004402151875495222\n","fine-tuned for 0.001056913910693806\n","for the 0.0004327183473351329\n","the first 0.0015369858979901287\n","first task 0.0002969166604370404\n","task and 0.00021166806181143944\n","and we 0.000129907014349278\n","we obtained 0.000792432069591454\n","obtained the 0.00011207124501166084\n","the second 0.001351194007058816\n","second place 0.036083022737049865\n","place in 0.002390283207176816\n","in the 0.0006980720733199116\n","the competition 0.0014167335694104993\n","competition with 0.00038251346603978843\n","with an 0.0009796932849717676\n","an F1-score 0.006695236957944557\n","F1-score of 0.002279720024548813\n","of 97.55 0.004605949176905652\n","97.55 { 0.0077846592247450015\n","{ \\ 0.007783457855114802\n","\\ % 0.08108756639892874\n","% } 0.007781968219053352\n","} using 2.5919949521129e-05\n","using an 0.0008725183111103322\n","an ensemble 0.008704648815658177\n","ensemble of 0.0009753723807705756\n","of five 0.00047259330577416585\n","five such 0.00022337185193007907\n","such language 7.408189712518506e-05\n","language models 0.003078838445506275\n","models . 0.0004748377792595654\n",". Subsequently 0.003364125796061157\n","Subsequently , 0.0029245117986601296\n",", a 0.0001844386597213969\n","a BERT 0.00012794899752340826\n","BERT model 0.0024708911923280487\n","model was 0.0008402125915180142\n","was fine-tuned 0.0020659955077460715\n","fine-tuned for 0.001056862785220654\n","for the 0.00043269741573424017\n","the second 0.0013511411357152\n","second task 0.0009092114288734443\n","task and 0.00021165784148885795\n","and a 0.0001504592822429157\n","a Conditional 0.0009454776676441447\n","Conditional Random 2.5459093500103385\n","Random Field 2.97410851977141\n","Field model 0.000703419504862525\n","model was 0.0008402019186796145\n","was used 0.0034770693779980085\n","used on 7.723561275169292e-05\n","on top 0.006050601218858523\n","top of 0.002101045243150332\n","of the 0.0005959453882070636\n","the generated 0.0006957105558298271\n","generated language 7.486962970018749e-05\n","language features 8.580721349542446e-05\n","features ; 0.0005323518321305954\n","; the 0.0001493740776268517\n","the system 0.00039092729770817247\n","system managed 0.0011854811802065725\n","managed to 0.003772069686753702\n","to identify 0.0039232317222863125\n","identify the 0.0007626437395007254\n","the cause 0.00045702231545676094\n","cause and 0.00046332520468452215\n","and effect 0.00012138428531163241\n","effect relationships 0.0027073297823365537\n","relationships with 0.0002502703581562861\n","with an 0.0009796297532869158\n","an F1-score 0.006694802781786196\n","F1-score of 0.0022795721880977636\n","of 73.10 0.004605650488978461\n","73.10 { 0.007784154402907341\n","{ \\ 0.007782953112206377\n","\\ % 0.08108230802059513\n","% } 0.007781463572930208\n","} . 6.729942599847027e-05\n",". We 0.0028868983098700018\n","We open-sourced 0.001629423853109331\n","open-sourced the 0.0002838599907885502\n","the code 0.00027040224795215457\n","code and 0.001056141569315305\n","and made 0.00014140363755169323\n","made it 0.0019670879205145\n","it available 0.0002702539545818139\n","available at 0.011506972347808559\n","at : 0.000676212572839627\n",": https 0.009148140459765211\n","https : 0.07051263177479389\n",": //github.com/avramandrei/FinCausal2020 0.07051261600680592\n","//github.com/avramandrei/FinCausal2020 . 0.0033638931775100043\n","FinCausal-2020 is 0.00297687532119155\n","is the 0.0001746840818568805\n","the shared 0.00064093681950055\n","shared task 0.027677263741723895\n","task which 0.0002183662262142132\n","which focuses 0.001792405767999308\n","focuses on 0.013215011946155854\n","on the 0.0008088498236830408\n","the causality 0.0002797988029753564\n","causality detection 0.019568167480493847\n","detection of 0.0005938771017357418\n","of factual 0.0005066041966311209\n","factual data 0.0005444027485532155\n","data for 0.00047077995256185177\n","for financial 0.00045684908826695283\n","financial analysis 0.0005349348861483781\n","analysis . 0.0003524994176561682\n",". The 0.002907245724732913\n","The financial 0.0001427398323759886\n","financial data 0.0003575693081177639\n","data facts 7.561097710398372e-05\n","facts don 0.0217581702786433\n","don { 0.007631210245642582\n","{ ' 0.007618513269296686\n","' } 0.0076185098711109124\n","} t 0.004443647707740934\n","t provide 0.0002992803351350746\n","provide much 0.0007580227184613771\n","much explanation 0.001918391391900442\n","explanation on 0.00010302490623823274\n","on the 0.0008088328582969367\n","the variability 0.0005699485698835461\n","variability of 0.0011330776118434521\n","of these 0.0009557179320327399\n","these data 0.00028733603385086077\n","data . 0.0005537516368346816\n",". This 0.002093789231860448\n","This paper 0.018741317475286377\n","paper aims 0.0028149334319387295\n","aims to 0.004670777179625634\n","to propose 5.660444277001578e-05\n","propose an 0.0030482360599493486\n","an efficient 0.007175123881025912\n","efficient method 0.002640419520817507\n","method to 0.0007374146109724254\n","to classify 0.004092240574483689\n","classify the 0.00070404400587632\n","the data 0.00021952533479566166\n","data into 0.0003837856916712328\n","into one 0.002005306134895124\n","one which 0.0001830199837045675\n","which is 0.0018303422335648563\n","is having 0.0001463956387541455\n","having any 0.002034134840286201\n","any financial 0.0015281061447162542\n","financial cause 0.010495032938846254\n","cause or 0.0010527444945371439\n","or not 0.002486104855330178\n","not . 7.937814086856335e-05\n",". Many 0.0018079849799705627\n","Many models 0.00022017849937525726\n","models were 0.0010086805372281063\n","were used 0.004267360863358415\n","used to 0.0019515659216179312\n","to classify 0.004092181101768228\n","classify the 0.0007040337739733889\n","the data 0.00021952214443959192\n","data , 0.0003441706918966748\n",", out 3.1603727674955925e-05\n","out of 0.0013153345043824729\n","of which 0.00010960367149449694\n","which SVM 0.00013202067840542445\n","SVM model 0.0009541513902053685\n","model gave 0.002059754256040933\n","gave an 0.003669546951191173\n","an F-Score 0.008562271840127656\n","F-Score of 0.0020467600954120133\n","of 0.9435 0.004605194449192798\n","0.9435 , 0.003327393100502151\n",", BERT 0.00017914250635572905\n","BERT with 0.0004460092867136344\n","with specific 0.00039962134926126787\n","specific fine-tuning 0.0003729912619224193\n","fine-tuning achieved 0.00043912445422636067\n","achieved best 0.0002588732057593387\n","best results 0.003916413500449798\n","results with 0.00035709486213098414\n","with F-Score 0.0011260920556763754\n","F-Score of 0.0020467390554023018\n","of 0.9677 0.004605147109495074\n","0.9677 . 0.003363586140212311\n","The FinCausal 0.0028974180937425464\n","FinCausal 2020 0.5258425609081421\n","2020 shared 0.024993185862199216\n","shared task 0.027674884869607005\n","task aims 0.0023340325410003884\n","aims to 0.004670544109164846\n","to detect 0.005008369697906806\n","detect causality 0.010295802834149883\n","causality on 0.00019720590634678768\n","on financial 0.0006120176311016966\n","financial news 0.027343266949374185\n","news and 0.00025603624274397066\n","and identify 0.00029962849064957113\n","identify those 0.0016119582943208966\n","those parts 0.002580157720476362\n","parts of 0.00264403659482588\n","of the 0.0005958583006887877\n","the causal 0.0003912341885357703\n","causal sentences 0.00048062085044229316\n","sentences related 0.00032078005157611\n","related to 0.0026053559547593585\n","to the 0.0003208406220289472\n","the underlying 0.0017465736576095618\n","underlying cause 0.008084876947912474\n","cause and 0.0004632601692224221\n","and effect 0.00012136724704268608\n","effect . 0.00013085653369371574\n",". We 0.002886519523857524\n","We apply 0.00495712671776994\n","apply ensemble-based 0.05751480825188464\n","ensemble-based and 0.000526427917618672\n","and sequence 0.00011141298226683816\n","sequence tagging 0.035674165556881896\n","tagging methods 0.0004274264451241595\n","methods for 0.0013131506972875646\n","for identifying 0.0023026453956016065\n","identifying causality 0.017078231670941496\n","causality , 0.0003802560008816481\n",", and 0.0005680650822222548\n","and extracting 0.0002587495041743921\n","extracting causal 0.003091365951839267\n","causal subsequences 0.1519921579851194\n","subsequences . 0.0008408599653139581\n",". Our 0.0033166457197084057\n","Our models 0.0004441569117630494\n","models yield 0.0017155396940653359\n","yield promising 0.01596840024510463\n","promising results 0.01993069917199053\n","results on 0.0018396812874270053\n","on both 0.001259624205729446\n","both sub-tasks 0.005709446982994503\n","sub-tasks , 0.0003639117906949988\n",", with 0.00020361173027953088\n","with the 0.0004559945780257727\n","the prospect 0.000932533421424391\n","prospect of 0.0006578386541284103\n","of further 2.8029551052330254e-05\n","further improvement 0.007098677686254194\n","improvement given 0.0003281866573465669\n","given more 0.00014482708617767623\n","more time 0.000382453216976877\n","time and 0.00046943794620652346\n","and computing 0.00031427406931392425\n","computing resources 0.006073066792821882\n","resources . 0.0005627627254344019\n",". With 0.0026479274407499257\n","With respect 0.005699232457278445\n","respect to 0.0061624721140615705\n","to task 8.493256749833528e-06\n","task 1 0.0009512342898478148\n","1 , 0.0001461476202057233\n",", we 0.002509946524592405\n","we achieved 0.0006956644241326923\n","achieved an 0.0025038588123784647\n","an F1 0.003134756692543916\n","F1 score 0.1129915640741566\n","score of 0.0014587748813822442\n","of 0.9429 0.004604763022909411\n","0.9429 on 0.013803308865165169\n","on the 0.00080871400768438\n","the evaluation 0.00030686077949007215\n","evaluation data 0.0003752942706190558\n","data , 0.0003441316701499015\n",", and 0.0005680342718799021\n","and a 0.00015042308097566317\n","a corresponding 0.0003569765042422071\n","corresponding ranking 0.0009525169400750076\n","ranking of 0.0003496394488873807\n","of 12/14 0.0046046698102471485\n","12/14 . 0.003363237522221\n",". For 0.003138092909825143\n","For task 0.0001608704474727909\n","task 2 0.001001665146158185\n","2 , 0.00014078617271323987\n",", we 0.00250986039685584\n","we were 0.0005280164964641233\n","were ranked 0.006859757142077701\n","ranked 6/10 0.9442253790891746\n","6/10 , 0.003326980494480676\n",", with 0.00020359869429117426\n","with an 0.000979402020825802\n","an F1 0.0031346317115209525\n","F1 score 0.11298705916290308\n","score of 0.0014587167208367454\n","of 0.76 0.0008633586439105984\n","0.76 and 0.0006579742384471858\n","and an 0.0001574959952364955\n","an ExactMatch 0.030819848529165687\n","ExactMatch score 0.2752124871936397\n","score of 0.00145870652976235\n","of 0.1912 0.004604547265194823\n","0.1912 . 0.0033631480156689304\n","In order 0.007716129491742677\n","order to 0.004933589966419433\n","to provide 0.0015222160341669796\n","provide an 0.0025559974948933073\n","an explanation 0.003219961764650398\n","explanation of 0.00058415694419285\n","of machine 0.0003264862447827691\n","machine learning 0.016330959000885662\n","learning models 0.002100004757817594\n","models , 0.00030960698517123287\n",", causality 0.00014258074425650035\n","causality detection 0.019563838068892198\n","detection attracts 0.01504910499493129\n","attracts lots 2.2142520962401284\n","lots of 0.0046044894993913605\n","of attention 0.0002593595302785534\n","attention in 0.00043670877617743705\n","in the 0.0006978405573403116\n","the artificial 0.0001756178047066637\n","artificial intelligence 1.1033129227607994\n","intelligence research 0.0016064796259004479\n","research community 0.02568131083195426\n","community . 0.001076615002457437\n",". In 0.0027266252783975393\n","In this 0.014292290182816532\n","this paper 0.013098395887241786\n","paper , 0.0013831852351489742\n",", we 0.0025097338418448426\n","we explore 0.0062845371170984765\n","explore the 0.0011545270337047137\n","the cause-effect 0.0005439185993962872\n","cause-effect detection 0.03260554212680801\n","detection in 0.0005146061774242983\n","in financial 0.0007872796090416131\n","financial news 0.027338723807893567\n","news and 0.00025599370178470136\n","and propose 0.00030045062545295214\n","propose an 0.003047564630523098\n","an approach 0.0018755359146839527\n","approach , 0.00024163258004645055\n",", which 0.0016142600289648084\n","which combines 0.005496029544275005\n","combines the 0.0007980268773375366\n","the BIO 0.0004079325009143491\n","BIO scheme 0.08107571499108113\n","scheme with 0.00034558075271621033\n","with the 0.00045593515276287617\n","the Viterbi 0.00046620594707728045\n","Viterbi decoder 0.0894260463167605\n","decoder for 0.00032697620078681706\n","for addressing 0.001590712120990763\n","addressing this 0.002746842738078242\n","this challenge 0.0027701191966492057\n","challenge . 0.00043970628335873014\n",". Our 0.003316151628275355\n","Our approach 0.0051905399689528816\n","approach is 0.0011637749245313343\n","is ranked 0.0012400060944615246\n","ranked the 0.00013308261054147626\n","the first 0.0015363751902297987\n","first in 0.00015625775680296736\n","in the 0.000697801099811253\n","the official 0.0016958574156434563\n","official run 0.012675970834175465\n","run of 0.00011705549964850642\n","of cause-effect 0.0011510418069306974\n","cause-effect detection 0.03260411194991126\n","detection ( 0.0003645038424536816\n","( Task 0.0005561991885217461\n","Task 2 0.018760071871650078\n","2 ) 0.009341503277999519\n",") of 4.309239270365934e-05\n","of the 0.0005957347549659487\n","the FinCausal-2020 0.001958000511035601\n","FinCausal-2020 shared 0.049904851505689934\n","shared task 0.027668721344862247\n","task . 0.0006818819192343959\n",". We 0.002885949832643972\n","We not 2.81072700855362e-05\n","not only 0.015721372130925056\n","only report 0.000543237963015499\n","report the 0.0004079145280557787\n","the implementation 0.000592104002192447\n","implementation details 0.06782365241071586\n","details and 0.00032514056703640743\n","and ablation 0.0005407394353981238\n","ablation analysis 0.004461293247124444\n","analysis in 0.00018590031835842303\n","in this 0.0006388666511963747\n","this paper 0.013097333147127332\n","paper , 0.001383073010275053\n",", but 0.001965479775197218\n","but also 0.014287973239477926\n","also publish 0.0021414402689484448\n","publish our 0.002527845212283313\n","our code 0.0027609507013586746\n","code for 0.0004973261479365505\n","for academic 0.0010808244617409473\n","academic usage 0.005215358250365389\n","usage . 0.0003941227697097293\n","This paper 0.018735768863156196\n","paper describes 0.03183320510085107\n","describes our 0.0056747295858245944\n","our system 0.0034530460472781\n","system developed 0.0024843492461452447\n","developed for 0.0020574596312110073\n","for the 0.0004325241516371786\n","the sub-task 0.00015405822785205297\n","sub-task 1 0.022307424653492846\n","1 of 6.990853489547666e-05\n","of the 0.0005957113990872848\n","the FinCausal 0.001957923747675463\n","FinCausal shared 0.0249514474899375\n","shared task 0.02766763658932194\n","task in 0.00031119581826206255\n","in the 0.0006977591881474344\n","the FNP-FNS 0.0008157959174455239\n","FNP-FNS workshop 0.8372015812934869\n","workshop held 0.07125119807270924\n","held in 0.0023000285169399535\n","in conjunction 0.007289874082744215\n","conjunction with 0.015094681520067803\n","with COLING-2020 0.02026407724299513\n","COLING-2020 . 0.0033626717376167297\n",". The 0.00290624296066067\n","The system 0.0013460413760419492\n","system classifies 0.006094316679894767\n","classifies whether 0.01557436482281097\n","whether a 0.0011707114202016695\n","a financial 0.00025036630084521785\n","financial news 0.02733575981363833\n","news text 0.0008570768606222701\n","text segment 0.002280716765222323\n","segment contains 0.002238898600795576\n","contains causality 0.007516302220438053\n","causality or 0.0009396538395709713\n","or not 0.002485320363495854\n","not . 7.935309305543807e-05\n",". To 0.0032112536235373457\n","To address 0.04355970863125983\n","address this 0.008414920364781218\n","this task 0.002218076457727709\n","task , 0.00042953144910067557\n",", we 0.002509416136310284\n","we fine-tune 0.002435881494025842\n","fine-tune and 0.0001315728212750328\n","and ensemble 0.00020432424016288616\n","ensemble the 1.5355785831746366e-05\n","the generic 0.00013739320983046198\n","generic and 0.0005650662006950939\n","and domain-specific 0.000464666723963546\n","domain-specific BERT 0.0023922372037653815\n","BERT language 0.0002543277346599924\n","language models 0.0030774013768313916\n","models pre-trained 0.00046337874858732973\n","pre-trained on 0.0008501370919396471\n","on financial 0.000611834537955562\n","financial text 0.0014667635985038785\n","text corpora 0.0025580694774120804\n","corpora . 0.0006953005890547664\n",". The 0.002906149298144502\n","The task 0.0007852248817105736\n","task data 0.00011949191850802062\n","data is 0.0006032063818599474\n","is highly 0.0027002210368104727\n","highly imbalanced 0.10689959548169845\n","imbalanced with 0.0002226735871876607\n","with the 0.0004558778673532038\n","the majority 0.0015635359752032116\n","majority non-causal 0.9243669926509003\n","non-causal class 0.4159651466682878\n","class ; 0.00024092970680742793\n","; therefore 0.004566138851933705\n","therefore , 0.0006213147559280772\n",", we 0.0025093379769939423\n","we train 0.0023715014511546846\n","train the 0.00039771975066670647\n","the models 8.006956950819353e-05\n","models using 0.0005354695786831555\n","using strategies 6.587865798976559e-05\n","strategies such 0.0005348824471074939\n","such as 0.014311215164320308\n","as under-sampling 0.003525271870024058\n","under-sampling , 0.0004751809596664189\n",", cost-sensitive 0.000415781370409566\n","cost-sensitive learning 0.0525559034348227\n","learning , 0.0001724852046033287\n",", and 0.0005678960940493604\n","and data 8.857141384708083e-05\n","data augmentation 0.024123589001832133\n","augmentation . 0.0002758923387101085\n",". Our 0.0033156576805012197\n","Our best 0.006182575899838878\n","best system 0.0016293266309777524\n","system achieves 0.004803945869159211\n","achieves a 0.0007650965939091422\n","a weighted 0.0017325284863119485\n","weighted F1-score 0.06111319176891598\n","F1-score of 0.0022785211044992616\n","of 96.98 0.0023017634428169147\n","96.98 securing 88.7360497332645\n","securing 4th 2.3351592032996704\n","4th position 0.03213521832176604\n","position on 0.00031650466032686096\n","on the 0.0008084968983072798\n","the evaluation 0.00030677839927853364\n","evaluation leaderboard 0.0015891313362897568\n","leaderboard . 0.0009606805907888088\n",". The 0.0029059926098173117\n","The code 0.002428094302772512\n","code is 0.0021232537150357015\n","is available 0.001974100526127143\n","available at 0.011501730963690398\n","at https 0.04953333309979938\n","https : 0.07048052929429155\n",": //github.com/sarthakTUM/fincausal 0.07048051354065779\n","This paper 0.018733565364650682\n","paper introduces 0.021072087410614847\n","introduces our 0.0010204653281444588\n","our efforts 0.0030573127998183344\n","efforts at 0.0029930410153673707\n","at the 0.0007968352392826536\n","the FinCasual 0.0032628546101859056\n","FinCasual shared 0.24948758783388056\n","shared task 0.02766465519311084\n","task for 0.00026128164368841164\n","for modeling 0.0006170796629392339\n","modeling causality 0.009000939001671385\n","causality in 0.0006713679684219812\n","in financial 0.0007871197471795593\n","financial utterances 0.003210138288676825\n","utterances . 0.0006049723030658969\n",". Our 0.003315550375385694\n","Our approach 0.0051895988701146254\n","approach uses 0.0020848292963870875\n","uses the 0.00032045639377605744\n","the commonly 0.0003120176350359814\n","commonly and 1.4378215166194095e-05\n","and successfully 0.00019718636057482962\n","successfully applied 0.03860710456023811\n","applied strategy 0.0009216836047239404\n","strategy of 0.0001302306212039737\n","of fine-tuning 0.00024135229854462586\n","fine-tuning a 0.000300558442712865\n","a transformer-based 0.0014053822013170671\n","transformer-based language 0.004948418006293546\n","language model 0.0018285828872174198\n","model with 0.0006132239542980995\n","with a 0.0007581390953580445\n","a twist 0.0031761501067134493\n","twist , 0.0016630201949312524\n",", i.e 0.0018215223782754267\n","i.e . 0.003253602808699792\n",". we 3.296082821804868e-07\n","we modified 0.00045305075153301717\n","modified the 0.000118954003306404\n","the training 0.0004463536662996234\n","training and 0.00036299458534537677\n","and inference 0.00034408550842601754\n","inference mechanism 0.0016465702424731665\n","mechanism such 5.96478952729004e-05\n","such that 0.0002523449145271641\n","that our 0.0027150411806307786\n","our model 0.003574448429403503\n","model produces 0.0017259968394289248\n","produces multiple 0.0035329694838432805\n","multiple predictions 0.0002720174534559139\n","predictions for 0.0006250179119283938\n","for the 0.0004324513123836748\n","the same 0.0031258563498165193\n","same instance 0.0007202997864523841\n","instance . 0.00022778893682620448\n",". By 0.0032787113726022127\n","By designing 0.010093402734684167\n","designing such 0.001335088988495494\n","such a 0.0003573468800728095\n","a model 0.00027306768260166014\n","model that 0.0006113897224684211\n","that returns 0.0013857776532003741\n","returns k 0.1603053900584689\n","k { 0.0014407314829923792\n","{ \\textgreater 0.007779934256667203\n","\\textgreater } 0.007779934208097766\n","} 1 0.00010547611806412121\n","1 predictions 0.0003993153465920872\n","predictions at 0.0003652467675418725\n","at the 0.0007967795968002989\n","the same 0.003125801681649455\n","same time 0.026934127764843264\n","time , 0.0005662244213769901\n",", we 0.0025090266802352203\n","we not 2.6102985324408723e-05\n","not only 0.015717954208993898\n","only obtain 0.00030205596480361056\n","obtain a 0.0009986540579619994\n","a more 0.0005647102727726974\n","more resource 4.68273098338812e-05\n","resource efficient 0.0006570638440849956\n","efficient training 0.001391854076087015\n","training ( 6.275863396160131e-05\n","( as 6.680657524912696e-05\n","as opposed 0.02136707079848114\n","opposed to 0.006494766226035606\n","to fine-tuning 0.0001285555674235253\n","fine-tuning some 0.00025497014109589363\n","some pre-trained 0.00011925037953718137\n","pre-trained language 0.00701309858505209\n","language model 0.0018284708057524275\n","model k 0.000355840124258732\n","k independent 0.016188096254453187\n","independent times 0.0035438804888106\n","times ) 0.0002830801382541451\n",") , 0.00046308862121961666\n",", but 0.001965055491855252\n","but our 7.457606718230082e-05\n","our results 0.0007099680090312222\n","results indicate 0.01977264046282592\n","indicate that 0.009686778165118126\n","that we 0.0002557441161008269\n","we are 0.00034790589724477324\n","are also 0.0010732872496736242\n","also capable 0.001414219249886295\n","capable of 0.0040645856495342665\n","of obtaining 0.0005303863162829926\n","obtaining comparable 0.00787099639637074\n","comparable or 0.0033078602938773805\n","or even 0.006239325083934405\n","even better 0.005794368163899267\n","better evaluation 0.00026222473588607644\n","evaluation scores 0.0031261349289091788\n","scores that 0.0002121393822811552\n","that way 7.863629156152825e-05\n","way . 0.0004699832267943094\n",". We 0.002885241818483595\n","We compare 0.005068810587623473\n","compare multiple 0.00029819154229458625\n","multiple strategies 0.00043847782919310626\n","strategies for 0.00167915999713041\n","for combining 0.0006951712938031126\n","combining the 0.0005926896500422544\n","the k 8.055545889000071e-05\n","k predictions 0.009090176560511235\n","predictions of 0.0004711165816850806\n","of our 0.0007287642923021462\n","our model 0.003574204417265017\n","model . 0.00029833496837703674\n",". Our 0.0033151830992499552\n","Our submissions 0.0026112612269391304\n","submissions got 0.02813505938143819\n","got ranked 0.018507281941070684\n","ranked third 0.0421660746466794\n","third on 0.00033190379984262733\n","on both 0.0012590696498262007\n","both subtasks 0.006801559324396562\n","subtasks of 0.0003000021957938315\n","of the 0.000595570941260667\n","the shared 0.0006405628108340626\n","shared task 0.027661113107939762\n","task . 0.0006816944180732952\n","This paper 0.01873104340738504\n","paper presents 0.028510880981207853\n","presents our 0.0020855388026715937\n","our participation 0.011776899257660343\n","participation to 0.00029307800215254925\n","to the 0.00032068543150261643\n","the FinCausal-2020 0.0019574448958083325\n","FinCausal-2020 Shared 0.37422401697901564\n","Shared Task 0.30941222018010184\n","Task whose 0.0009461841859615098\n","whose ultimate 0.05920409556448456\n","ultimate aim 0.039032161150899865\n","aim is 0.0019061981022252623\n","is to 0.0003410195299725579\n","to extract 0.003911302019580358\n","extract cause-effect 0.03989296906782441\n","cause-effect relations 0.05902991561899099\n","relations from 0.0010499200811171507\n","from a 0.0006197833435267895\n","a given 0.0023754652429830415\n","given financial 0.0011069293749853422\n","financial text 0.0014664554711952592\n","text . 0.0004376693837985913\n",". Our 0.003315085235699551\n","Our participation 0.0014121710161343803\n","participation includes 0.004612112722088884\n","includes two 0.0031346525076200887\n","two systems 0.0007360022842913847\n","systems for 0.00064731476054374\n","for the 0.0004324071011917169\n","the two 0.0003714855926377803\n","two sub-tasks 0.015016297946949126\n","sub-tasks of 0.0004315059061002501\n","of the 0.0005955500877627461\n","the FinCausal-2020 0.0019573935685502576\n","FinCausal-2020 Shared 0.3742142042416377\n","Shared Task 0.30940410690925524\n","Task . 0.0002456104113330076\n",". The 0.002905483072834594\n","The first 0.0020457200081618675\n","first sub-task 0.005026059755399415\n","sub-task ( 0.0004655981012099665\n","( Task-1 0.004649324886212567\n","Task-1 ) 0.004488462608403409\n",") consists 0.00023068432605178265\n","consists of 0.004131242878475183\n","of the 0.000595541174386181\n","the binary 0.00034105589607227746\n","binary classification 0.035280028141502565\n","classification of 0.0003212852207116117\n","of the 0.000595536177474096\n","the given 0.0003745593183890643\n","given sentences 0.0006512957035042995\n","sentences as 0.000507077346772194\n","as causal 0.000422452867643959\n","causal meaningful 0.012317355396686989\n","meaningful ( 5.2356290676859296e-05\n","( 1 0.0063368082997794575\n","1 ) 0.009152000096153029\n",") or 0.0002716863741839323\n","or causal 0.0011259816410647383\n","causal meaningless 0.22787053264204363\n","meaningless ( 0.003874356324428163\n","( 0 0.0020861897471512337\n","0 ) 0.002014009537224727\n",") . 0.0005697470301593482\n",". Our 0.0033149265692739775\n","Our approach 0.0051886224703048675\n","approach for 0.0008673055079608444\n","for the 0.00043238649506964467\n","the Task-1 0.0006524389618678188\n","Task-1 includes 0.07909344001069957\n","includes applying 0.0014539235006616376\n","applying linear 0.0020863336891219996\n","linear support 0.006767256028915781\n","support vector 0.024651207513258715\n","vector machines 0.0437961815053424\n","machines after 0.00677328329043495\n","after transforming 0.020670191838964215\n","transforming the 0.0009561604180423227\n","the input 0.0009902360385145956\n","input sentences 0.0033382802226410665\n","sentences into 0.0019417224619631354\n","into vector 0.0008942761661860752\n","vector representations 0.020666027846170238\n","representations using 0.0005016239170007198\n","using term 0.0001843317832837856\n","term frequency-inverse 0.6643317343572552\n","frequency-inverse document 0.25493093969109976\n","document frequency 0.007466596098784568\n","frequency scheme 0.0013563818021407536\n","scheme with 0.0003454447708734045\n","with 3-grams 0.0067526175340689035\n","3-grams . 0.0011205462683110672\n",". The 0.0029053502354999066\n","The second 0.004809251791879255\n","second sub-task 0.008029944225282826\n","sub-task ( 0.0004655768549675013\n","( Task-2 0.0034437872057309833\n","Task-2 ) 0.0033246354004034524\n",") consists 0.00023067379945857432\n","consists of 0.004131054361497013\n","of the 0.0005955139987294211\n","the identification 0.000474490898267107\n","identification of 0.0012116897169315369\n","of the 0.0005955090625044504\n","the cause-effect 0.0005436829809852814\n","cause-effect relations 0.05902451054014511\n","relations in 0.0005720721242489376\n","in the 0.0006975224182948236\n","the sentences 0.00015128470180294305\n","sentences , 0.0003618874036847606\n",", which 0.0016135592777504562\n","which are 0.0018855276459001846\n","are detected 0.001577037366006644\n","detected as 0.0006578623070343368\n","as causal 0.0004224284378092683\n","causal meaningful 0.012316643101951612\n","meaningful . 0.00011356476261425881\n",". Our 0.0033147495915474712\n","Our approach 0.005188345459523382\n","approach for 0.0008672592041642994\n","for the 0.00043236341080591035\n","the Task-2 0.0007248934773380339\n","Task-2 is 0.001652633969327543\n","is a 0.0007137724295599828\n","a CRF-based 0.002339775679239662\n","CRF-based model 0.0030334741951046243\n","model which 0.00041190761133402375\n","which uses 0.0036724471486194745\n","uses linguistically 0.0014114620964028708\n","linguistically informed 0.2680550604283487\n","informed features 0.0033275623304255863\n","features . 0.00047496168982185815\n",". For 0.0031364396455767062\n","For the 0.0005917257441440504\n","the Task-1 0.0006523953193254288\n","Task-1 , 0.0003325232058046547\n",", the 0.0001469296792343633\n","the obtained 0.00014118868174562213\n","obtained results 0.0020162820466135623\n","results show 0.009102925336716843\n","show that 0.010659054452053719\n","that there 0.0020711706522016328\n","there is 0.006943261055291045\n","is a 0.0007137547071881929\n","a small 0.002929964520856068\n","small difference 0.0013177245826173497\n","difference between 0.02936391494360626\n","between the 0.000639501922153464\n","the proposed 0.0011720767129126237\n","proposed approach 0.006079937752949688\n","approach based 0.001223237820997082\n","based on 0.010604750852533618\n","on linear 0.0003235580280168216\n","linear support 0.006766657088919451\n","support vector 0.024649025743637216\n","vector machines 0.04379230529865422\n","machines ( 0.0008754721133001386\n","( F-score 0.0005275179799541905\n","F-score 94 0.07809994590307155\n","94 { 0.006973515162554135\n","{ \\ 0.0077769511753067394\n","\\ % 0.08101978023777533\n","% } 0.007775462786909986\n","} ) 0.00015084462373284865\n",") , 0.000462990378112232\n",", which 0.0016134645595418924\n","which requires 0.0028792223994505138\n","requires less 0.002249624681960159\n","less time 0.001863896523703008\n","time compared 0.0007375931859182941\n","compared to 0.004470856209375008\n","to the 0.00032062994139958097\n","the BERT-based 0.00016041854009328773\n","BERT-based baseline 0.0029175004274607495\n","baseline ( 0.0002694279358965866\n","( F-score 0.0005275079072239404\n","F-score 95 0.04044384257004191\n","95 { 0.006666859714950942\n","{ \\ 0.00777680267184037\n","\\ % 0.08101823313845559\n","% } 0.007775314311919313\n","} ) 0.00015084174331152635\n",") . 0.0005696759554190591\n",". For 0.0031362637161903646\n","For the 0.0005916925530252062\n","the Task-2 0.0007248430281127329\n","Task-2 , 0.0003694495044347542\n",", although 0.002194519666079468\n","although a 0.00015240888159068124\n","a minor 0.0006457987447397736\n","minor modifications 0.25591034752211533\n","modifications such 0.0019437062874398741\n","such as 0.01430584525195919\n","as the 0.0002645575008226495\n","the learning 9.65904294169105e-05\n","learning algorithm 0.003140982938816228\n","algorithm type 0.00023169409843708198\n","type and 0.0002908119221903516\n","and the 0.00018180246872832173\n","the feature 0.0002051118901769743\n","feature representations 0.004284550342112102\n","representations are 0.0011229468425661374\n","are made 0.0025881276042824556\n","made in 0.0007234091475479654\n","in the 0.0006974421906815551\n","the conditional 0.00047223283179223513\n","conditional random 0.34478618171171993\n","random fields 0.21636889022121134\n","fields based 0.0005498662481163053\n","based baseline 0.0003358522525619172\n","baseline ( 0.0002694161065357266\n","( F-score 0.0005274847467634981\n","F-score 52 0.08387984238840077\n","52 { 0.0031686769118480632\n","{ \\ 0.007776461235355069\n","\\ % 0.08101467607566733\n","% } 0.007774972940904805\n","} ) 0.00015083512069889192\n",") , 0.0004629612102933956\n",", we 0.002508298187042229\n","we have 0.0016310002302687375\n","have obtained 0.0010671224447325646\n","obtained better 0.00039015009343483075\n","better results 0.002931335462596104\n","results ( 6.0146571131586153e-05\n","( F-score 0.0005274780021613042\n","F-score 60 0.02242303749047232\n","60 { 0.004774345018342905\n","{ \\ 0.007776361786016542\n","\\ % 0.0810136400190166\n","% } 0.007774873510635789\n","} ) 0.00015083319174731744\n",") . 0.0005696436592172217\n",". The 0.0029048729213623954\n","The source 0.0007491523910602605\n","source codes 0.015141196461841453\n","codes for 0.0006495157036888374\n","for the 0.00043230797644981846\n","the both 3.916160708321281e-06\n","both tasks 0.001900203624804126\n","tasks are 0.00048420772325900977\n","are available 0.002645395360668891\n","available online 0.007431014061467012\n","online ( 5.455773950558264e-05\n","( https 0.0004969043879335227\n","https : 0.07045287684620251\n",": //github.com/ozenirgokberk/FinCausal2020.git/ 0.07045286110492795\n","//github.com/ozenirgokberk/FinCausal2020.git/ ) 0.014958287550827298\n",") . 0.0005696355976255299\n","Automatic identification 0.007217805078107834\n","identification of 0.001211491439372361\n","of cause-effect 0.00115041093289293\n","cause-effect relationships 0.15643695001243446\n","relationships from 0.0005615903468952007\n","from data 0.00012229079213626706\n","data is 0.0006029333720419627\n","is a 0.0007136752632644493\n","a challenging 0.0017190264448149967\n","challenging but 0.0013795897677842815\n","but important 0.00025947313104083587\n","important problem 0.0027391310369780635\n","problem in 0.0005514378508943776\n","in artificial 0.0006319799246959994\n","artificial intelligence 1.1026348333927194\n","intelligence . 0.0004892602506180622\n",". Identifying 0.0012253617153184545\n","Identifying semantic 0.0009725748115261023\n","semantic relationships 0.00708075510604144\n","relationships has 0.00012831328880631414\n","has become 0.030213296932055704\n","become increasingly 0.0788316862094031\n","increasingly important 0.034841241718435244\n","important for 0.0010962689624417134\n","for multiple 0.00038937160390601115\n","multiple downstream 0.002457480172014038\n","downstream applications 0.028769421040056413\n","applications like 0.0055519932061139626\n","like Question 0.0019516511218712332\n","Question Answering 1.12384981220606\n","Answering , 0.00019243133822951543\n",", Information 0.00011139693831854389\n","Information Retrieval 0.9928904919977459\n","Retrieval and 0.0006673574497208342\n","and Event 0.00015820545010784812\n","Event Prediction 0.13048080427539974\n","Prediction . 0.0007671754436533553\n",". In 0.0027249125447563697\n","In this 0.014283312464096446\n","this work 0.00776097059520656\n","work , 0.001003330936300224\n",", we 0.0025081579242105675\n","we tackle 0.002743733682569565\n","tackle the 0.0013242660928743143\n","the problem 0.0009024169483368535\n","problem of 0.001086710067616486\n","of causal 0.0007721631302687103\n","causal relationship 0.021739867869560527\n","relationship extraction 0.0026573549374660083\n","extraction from 0.0013709757480054384\n","from financial 0.00099814716092684\n","financial news 0.027321491975376037\n","news using 8.79382042248963e-05\n","using the 0.0004264134810667095\n","the FinCausal 0.0019568592137280526\n","FinCausal 2020 0.5254189987531663\n","2020 dataset 0.0007755452597634016\n","dataset . 0.0005035234473118969\n",". We 0.002884268286393878\n","We tackle 0.001476854551279921\n","tackle two 0.0008155464088029068\n","two tasks 0.0017995239683919395\n","tasks - 0.0004045854938444603\n","- 1 0.000693324347934038\n","1 ) 0.009149682772155892\n",") Detecting 0.00031489356641774643\n","Detecting the 0.00020598383610113271\n","the presence 0.00252075948118089\n","presence of 0.003923091418078902\n","of causal 0.0007721472332088176\n","causal relationships 0.05464249939715906\n","relationships and 0.00017626509926313032\n","and 2 0.0004598199484616283\n","2 ) 0.009335804703789304\n",") Extracting 0.00025351286858522193\n","Extracting segments 0.022270948976924707\n","segments corresponding 0.001640431927464983\n","corresponding to 0.0012127750765829155\n","to cause 0.0002947789267850338\n","cause and 0.0004628877002705593\n","and effect 0.00012126966603089631\n","effect from 8.341618072153107e-05\n","from news 0.0009337225474659089\n","news snippets 0.010029106920427081\n","snippets . 0.0005104999342556413\n",". We 0.00288419698284044\n","We propose 0.0050495848656992295\n","propose Transformer 8.749232303538152e-05\n","Transformer based 0.0016635801046478748\n","based sequence 0.000398535420926037\n","sequence and 0.00010784475549928963\n","and token 0.0002728228858503551\n","token classification 0.0022816387482169974\n","classification models 0.001058765021851111\n","models with 0.0005532316950136307\n","with post-processing 0.00036165269356873403\n","post-processing rules 0.009837252737820728\n","rules which 0.0007056928677189568\n","which achieve 0.00018326336999362712\n","achieve an 0.001433270297869875\n","an F1 0.0031323688909108333\n","F1 score 0.11290549633208452\n","score of 0.0014576637059234448\n","of 96.12 0.004601255502846918\n","96.12 and 0.005259994108041109\n","and 79.60 0.005259978369615598\n","79.60 on 0.013792753398949552\n","on Tasks 0.0002089808736452893\n","Tasks 1 0.021862527253915968\n","1 and 0.00020823099846022124\n","and 2 0.00045980687315524895\n","2 respectively 0.003834434228162844\n","respectively . 0.002225982328554779\n","The paper 0.0011804282859343336\n","paper describes 0.03181390585469251\n","describes the 0.0014673336121477992\n","the work 8.718580402851208e-05\n","work that 0.00018085388702449644\n","that the 0.00035832068328583993\n","the team 0.00030950275213278674\n","team submitted 0.009124152162812331\n","submitted to 0.0019825490927170676\n","to FinCausal 0.0006699257295252996\n","FinCausal 2020 0.5253871272263996\n","2020 Shared 0.05408202803064964\n","Shared Task 0.3093008721117519\n","Task . 0.00024552846174228494\n",". This 0.0020918625383227864\n","This work 0.003636900603009712\n","work is 0.0005869450308808538\n","is associated 0.0010338635785707123\n","associated with 0.011743710612840503\n","with the 0.0004556216789623947\n","the first 0.0015353378492495757\n","first sub-task 0.00502434954930492\n","sub-task of 0.0003554501161147817\n","of identifying 0.000883037195204664\n","identifying causality 0.017063789654656103\n","causality in 0.0006710259052513913\n","in sentences 0.00011634632569227007\n","sentences . 0.0006110210314620822\n",". The 0.0029044659807955274\n","The various 5.808282623828561e-05\n","various models 0.0004095862763882343\n","models used 0.00019157267030205872\n","used in 0.0016689294820961874\n","in the 0.0006973227088174053\n","the experiments 0.0001302672482534247\n","experiments tried 0.0021410117984536968\n","tried to 0.003940628256574147\n","to obtain 0.00384031233414978\n","obtain a 0.0009982033495257306\n","a latent 0.0005520957222706375\n","latent space 0.04260239072930954\n","space representation 0.0012437124691698697\n","representation for 0.0005583311291389262\n","for each 0.0025239697747380194\n","each of 0.0003803428779655298\n","of the 0.0005953258033484786\n","the sentences 0.00015123914333246916\n","sentences . 0.0006110047801959254\n",". Linear 0.00022036152317035418\n","Linear regression 0.04779852734512508\n","regression was 0.0005117076406774312\n","was performed 0.013989588130517951\n","performed on 0.0018667020325577799\n","on these 0.0005480615064654711\n","these representations 0.0015976958554791063\n","representations to 0.00018715176016074876\n","to classify 0.004088309264265308\n","classify whether 0.019226586890268182\n","whether the 0.0006196296419902162\n","the sentence 0.0003674533213908666\n","sentence is 0.0003043114833734661\n","is causal 5.092216764238493e-05\n","causal or 0.0004502287030301142\n","or not 0.0024837191173200067\n","not . 7.930196732890563e-05\n",". The 0.002904347137444076\n","The experiments 0.0012830265302739502\n","experiments have 0.0007636821559214113\n","have shown 0.01856183417712636\n","shown BERT 0.00021291854652983976\n","BERT ( 0.0007760356107003749\n","( Large 0.0001702383055072052\n","Large ) 0.0003286964390132204\n",") performed 0.0001375686015024121\n","performed the 8.141847698955277e-05\n","the best 0.0018007453795308745\n","best , 4.26024607995045e-05\n",", giving 0.001342900546099116\n","giving a 0.0008409061045163093\n","a F1 5.655969134676301e-05\n","F1 score 0.11289402363304632\n","score of 0.0014575155879257935\n","of 0.958 0.004600787955759012\n","0.958 , 0.0016621046374162302\n",", in 0.00012846302624993668\n","in the 0.0006972787076350189\n","the task 0.0006491029178800164\n","task of 0.0006873321185760255\n","of detecting 0.0007430198592662626\n","The max and min values are 1933404.0531139784 and 9.747849721852596e-08 and their tokens are well-labeled_OLD and a_and\n"]}]},{"cell_type":"markdown","metadata":{"id":"RIEw6FcsdZ1f"},"source":["### Question 8B: Free response (3 points)\n","\n","Characterize the different mutual information values of the sentence you used. What values are highest? What values are lowest? When do you think mutual information would be a better statistic to compute than a conditional probability?"]},{"cell_type":"markdown","metadata":{"id":"EzshSnbtGMCo"},"source":["Word Tokenization has been used to tokenize the abstracts. Tokens that are used in bigrams are the punctuations and words that are present in abstracts. In order to calculate the mutual information, first I calculated the event of token 1 and token 2 occurred. Then, I calculated the event of both words occuring at the same time and finally calculated mutual information by dividing the above two values. After calculating 1 million bigrams, the bigram \"**well-labeled**\" and \"**OLD**\" has the highest MI value which is **1933404.0531139784** and the lowest bigram is \"**a**\" and \"**and**\" whose MI value is **9.747849721852596e-08**.Conditional probability will work better if we have same data or knowledge about an event and mutual information works on uncertainty. So, I think that mutual information would be better than a conditional probability where the events are uncertain."]},{"cell_type":"markdown","metadata":{"id":"PCgDQrOaICRl"},"source":["# Submission guidelines (1 point)\n","\n","Please upload your completed notebook file to UBLearns in the following format:\n","\n","Lastname\\_Firstname\\_HW2.ipynb\n","\n","e.g., Smith\\_John\\_HW2.ipynb."]}]}
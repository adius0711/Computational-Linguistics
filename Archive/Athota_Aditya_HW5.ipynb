{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Athota_Aditya_HW5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0pCRaCjtDiua"},"source":["# HW5: Building and evaluating a part-of-speech tagger\n","\n","In this assignment, we are interested in what factors affect how well a trained part-of-speech tagger will do on unseen data. The homework will assess your ability to manipulate data (the output of neural language models) and your ability to discuss the results from each of the classifiers that you train. You will train four classifiers that will focus on different ways of looking at the data.\n","\n","We specifically want you to assess the performance of a deep neural language model ([RoBERTa; Liu et al., 2019](https://arxiv.org/abs/1907.11692)) that is readibly available in the `huggingface` package. We would like you to compare and contrast the performance of classifiers trained on representations obtained from a lower layer of RoBERTa with classifiers trained on a higher layer from RoBERTa. This will allow you to see how different parts of a neural model's architecture do or do not encode the same kind of information.\n","\n","We also want to see the effect of how close the training data is to the test data. This is like a real-world scenario, where we often train on the recent past to predict the present. So, we want to get the right part-of-speech tags for this year's abstracts (2021) either from classifiers trained on abstract part-of-speech tags from the immediately preceding year (2020) or any year prior to 2020. Think about how language and science can change while you are building these classifiers. What does it mean if 2020 and pre-2020 influence performance on tagging the 2021 dataset?\n","\n","### Before you start: What are part-of-speech tags?\n","\n","Part of speech tags are labels we assign to words depending on what kind of syntactic role they play in a sentence. While we have not studied part-of-speech tags yet in class, they have come up when we have talked about nouns, verbs, modifiers, etc. Building a good classifier that can do part-of-speech tagging can help us better understand things like the syntactic structure of a sentence. In order to understand the meaning of a chunk of language, we need to know what kind of \"role\" each word is playing.\n","\n","There are lots of resources for learning lots about part of speech tags. For this assignment, we will work with the most basic of categories: \"Universal\" labels. These categories are designed to work for as many languages as possible. We are trying to predict the following categories in context to the best of our ability:\n","\n","    VERB - verbs (all tenses and modes)\n","    NOUN - nouns (common and proper)\n","    PRON - pronouns\n","    ADJ - adjectives\n","    ADV - adverbs\n","    ADP - adpositions (prepositions and postpositions)\n","    CONJ - conjunctions\n","    DET - determiners\n","    NUM - cardinal numbers\n","    PRT - particles or other function words\n","    X - other: foreign words, typos, abbreviations\n","    . - punctuation\n","\n","So, our classifiers will try to learn what makes something an \"adjective\", what makes something a \"noun\", and so on.\n","\n","## Warning: This assignment will probably take a long time!!\n","## Lots of moving parts and many computations are very slow.\n","## Please heed the advice below:\n","\n","* ### We recommend that you prototype on very small subsets of the data (e.g., `train_2020_only[0:5]` and `test_2021[0:5]`)\n","* ### Only once you are ready to submit your assignment and start writing up your results should you run through the whole dataset.\n","* ### Running RoBERTa and training your classifier can easily take half an hour or more to run depending on the efficiency of your implementation. When you have finished prototyping, expect for this to take a full 3-4 hours, just in case.\n","\n","## DO NOT start until the last minute. It will only lead to avoidable suffering."]},{"cell_type":"markdown","metadata":{"id":"Nf0Zg3C6R0iO"},"source":["# Q1: Installing prerequisites (2 points)\n","\n","In order to do this assignment, you need to install the `transformers` package from `huggingface`. Do that in the cell below."]},{"cell_type":"code","metadata":{"id":"YvrD2GRyukeg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637035312950,"user_tz":300,"elapsed":3778,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"beab98e8-b973-446f-9136-f06bfa88f30f"},"source":["!pip3 install transformers"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"GOvM2GhGRW6Z"},"source":["# Q2: Imports (1 point)\n","\n","Put all of the imports you will use here. Also include the neural language model and tokenizer that you will use. We will be using the RoBERTa models; for examples, refer to lecture notebooks. Keep in the `model.eval()` code below."]},{"cell_type":"code","metadata":{"id":"dqAwrluGRa07","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637035316045,"user_tz":300,"elapsed":3103,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"56f01392-8d6d-498a-e362-edba45bc1383"},"source":["# your imports go here\n","from google.colab import drive, files\n","import json\n","from transformers import RobertaModel, RobertaTokenizer\n","from sklearn.linear_model import LogisticRegression\n","import numpy as np\n","from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\n","\n","model = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","model.eval()"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":2},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"PgUe790dAbGc"},"source":["#Q3: Data preprocessing (6 points)"]},{"cell_type":"markdown","metadata":{"id":"OA3sAwSuRD1j"},"source":["## Q3A: Loading in the three datasets (3 points)\n","\n","For this assignment, we are going to use best practices in  machine learning and split our training data and our test data apart. We have two training datasets for you, which we described above. The 2020-only dataset is called `train_2020-only.json` and the pre-2020 dataset is called `train_pre-2020.json`. Each line correponds to one `json` object. Load in each of these training datasets as `train_2020_only` and `train_pre2020` respectively using our familiar friend `json.loads`, reading in the data line by line.\n","\n","Our test data is stored in the file `test_2021.json`. It is structured exactly the same way as the training files, but when you load it in, name it `test_2021`.\n","\n","All of the datasets are stored in the `data/` subdirectory."]},{"cell_type":"code","metadata":{"id":"VTEpHZzyRbnL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637035320486,"user_tz":300,"elapsed":4453,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"d8868d77-6d84-45de-f7ed-af64e725e3c2"},"source":["drive.mount(\"/content/drive/\", force_remount=True)\n","\n","with open(\"/content/drive/MyDrive/Fall 2021 Computational Linguistics Notebooks/Archive/data/train_2020-only.json\") as train_data:\n","  train_2020_only = []\n","  for x in train_data:\n","      if x != '':\n","          train_2020_only.append(json.loads(x))\n","\n","with open(\"/content/drive/MyDrive/Fall 2021 Computational Linguistics Notebooks/Archive/data/train_pre-2020.json\") as pre_train:\n","  train_pre2020 = []\n","  for x in pre_train:\n","      if x != '':\n","          train_pre2020.append(json.loads(x))\n","\n","with open(\"/content/drive/MyDrive/Fall 2021 Computational Linguistics Notebooks/Archive/data/test_2021.json\") as test_data:\n","  test_2021 = []\n","  for x in test_data:\n","      if x != '':\n","          test_2021.append(json.loads(x))"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","metadata":{"id":"G8ElXY_40BZ6"},"source":["## Q3B: Preview data (1 point)\n","\n","Print out the first two entries of `train_2020_only`."]},{"cell_type":"code","metadata":{"id":"_KZ08zlz0Eck","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637035320486,"user_tz":300,"elapsed":30,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"f5f43bb9-c5cf-4beb-be5e-e755d17fbf73"},"source":["train_2020_only[0:2]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[['Dialogue', 'NOUN'],\n","  ['Act', 'NOUN'],\n","  ['(', '.'],\n","  ['DA', 'NOUN'],\n","  [')', '.'],\n","  ['tagging', 'NOUN'],\n","  ['is', 'VERB'],\n","  ['crucial', 'ADJ'],\n","  ['for', 'ADP'],\n","  ['spoken', 'ADJ'],\n","  ['language', 'NOUN'],\n","  ['understanding', 'VERB'],\n","  ['systems', 'NOUN'],\n","  [',', '.'],\n","  ['as', 'ADP'],\n","  ['it', 'PRON'],\n","  ['provides', 'VERB'],\n","  ['a', 'DET'],\n","  ['general', 'ADJ'],\n","  ['representation', 'NOUN'],\n","  ['of', 'ADP'],\n","  ['speakers', 'NOUN'],\n","  ['{', '.'],\n","  [\"'\", 'PRT'],\n","  ['}', '.'],\n","  ['intents', 'NOUN'],\n","  [',', '.'],\n","  ['not', 'ADV'],\n","  ['bound', 'VERB'],\n","  ['to', 'PRT'],\n","  ['a', 'DET'],\n","  ['particular', 'ADJ'],\n","  ['dialogue', 'NOUN'],\n","  ['system', 'NOUN'],\n","  ['.', '.']],\n"," [['Unfortunately', 'ADV'],\n","  [',', '.'],\n","  ['publicly', 'ADV'],\n","  ['available', 'ADJ'],\n","  ['data', 'NOUN'],\n","  ['sets', 'NOUN'],\n","  ['with', 'ADP'],\n","  ['DA', 'NOUN'],\n","  ['annotation', 'NOUN'],\n","  ['are', 'VERB'],\n","  ['all', 'DET'],\n","  ['based', 'VERB'],\n","  ['on', 'ADP'],\n","  ['different', 'ADJ'],\n","  ['annotation', 'NOUN'],\n","  ['schemes', 'NOUN'],\n","  ['and', 'CONJ'],\n","  ['thus', 'ADV'],\n","  ['incompatible', 'ADJ'],\n","  ['with', 'ADP'],\n","  ['each', 'DET'],\n","  ['other', 'ADJ'],\n","  ['.', '.']]]"]},"metadata":{},"execution_count":4},{"output_type":"execute_result","data":{"text/plain":["[[['Dialogue', 'NOUN'],\n","  ['Act', 'NOUN'],\n","  ['(', '.'],\n","  ['DA', 'NOUN'],\n","  [')', '.'],\n","  ['tagging', 'NOUN'],\n","  ['is', 'VERB'],\n","  ['crucial', 'ADJ'],\n","  ['for', 'ADP'],\n","  ['spoken', 'ADJ'],\n","  ['language', 'NOUN'],\n","  ['understanding', 'VERB'],\n","  ['systems', 'NOUN'],\n","  [',', '.'],\n","  ['as', 'ADP'],\n","  ['it', 'PRON'],\n","  ['provides', 'VERB'],\n","  ['a', 'DET'],\n","  ['general', 'ADJ'],\n","  ['representation', 'NOUN'],\n","  ['of', 'ADP'],\n","  ['speakers', 'NOUN'],\n","  ['{', '.'],\n","  [\"'\", 'PRT'],\n","  ['}', '.'],\n","  ['intents', 'NOUN'],\n","  [',', '.'],\n","  ['not', 'ADV'],\n","  ['bound', 'VERB'],\n","  ['to', 'PRT'],\n","  ['a', 'DET'],\n","  ['particular', 'ADJ'],\n","  ['dialogue', 'NOUN'],\n","  ['system', 'NOUN'],\n","  ['.', '.']],\n"," [['Unfortunately', 'ADV'],\n","  [',', '.'],\n","  ['publicly', 'ADV'],\n","  ['available', 'ADJ'],\n","  ['data', 'NOUN'],\n","  ['sets', 'NOUN'],\n","  ['with', 'ADP'],\n","  ['DA', 'NOUN'],\n","  ['annotation', 'NOUN'],\n","  ['are', 'VERB'],\n","  ['all', 'DET'],\n","  ['based', 'VERB'],\n","  ['on', 'ADP'],\n","  ['different', 'ADJ'],\n","  ['annotation', 'NOUN'],\n","  ['schemes', 'NOUN'],\n","  ['and', 'CONJ'],\n","  ['thus', 'ADV'],\n","  ['incompatible', 'ADJ'],\n","  ['with', 'ADP'],\n","  ['each', 'DET'],\n","  ['other', 'ADJ'],\n","  ['.', '.']]]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"qeu9UjV20QIW"},"source":["## Q3C: What are each of the lines? (2 points)\n","\n","What kind of data structure is it? What are the elements?"]},{"cell_type":"markdown","metadata":{"id":"6n7-6Vg50Vfk"},"source":["The data structure consists of a list of lists within a list. The word is the first member of the innermost list, and the part of speech of that word is the second element of the innermost list."]},{"cell_type":"markdown","metadata":{"id":"rHJyTlK_R-UC"},"source":["# Q4: Creating embeddings for each utterance (25 points) for your training data and producing four models\n","\n","For hints, check out the `natural_language_inference.ipynb` functions."]},{"cell_type":"markdown","metadata":{"id":"kyi4JkZV0hqY"},"source":["Use the below function to take a single sentence and turn it into an embedding that we can use for our classifiers. This model will automatically ignore all non-initial morphemes so you do not have to worry about how RoBERTa handles word pieces.\n","\n","Pay attention to the `# note` in the below for a clue to a later question."]},{"cell_type":"code","metadata":{"id":"lqHAAHVe05aY","executionInfo":{"status":"ok","timestamp":1637035320487,"user_tz":300,"elapsed":26,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}}},"source":["def embed_words_roberta(single_data_entry, model, tokenizer):\n","  words_only = [x[0] for x in single_data_entry] # note\n","  tokenized = tokenizer(words_only, return_tensors='pt',\n","                        is_split_into_words=True)\n","  embedded = model(**tokenized)\n","  embeddings = embedded['hidden_states']\n","  token_strings = tokenizer.convert_ids_to_tokens(tokenized['input_ids'][0].tolist())\n","  dimensions_to_keep = [i for i, x in enumerate(token_strings)\n","                        if x.startswith(\"Ġ\") or i==1]\n","  subsetted_embeddings = [x[:, dimensions_to_keep].detach().numpy()\n","                          for x in embeddings]\n","  return subsetted_embeddings"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gF_q8JPV5ihG"},"source":["## Q4A: Extract the embeddings at a specific layer (2 points)\n","\n","Please print out the 7th layer from the output of `embed_words_roberta(train_2020_only[0])`.\n","\n","Then print out the 3rd layer.\n","\n","Remember Python indexing."]},{"cell_type":"code","metadata":{"id":"jJuqm7Tj5d6z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637035321006,"user_tz":300,"elapsed":544,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"4074be7e-8a6d-49f8-f58e-2202b6ff4edc"},"source":["print(embed_words_roberta(train_2020_only[0], model, tokenizer)[6])\n","print(embed_words_roberta(train_2020_only[0], model, tokenizer)[2])"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[ 0.14016572 -0.11448487 -0.011288   ...  0.22353216 -0.01660346\n","   -0.11273213]\n","  [ 0.4115105   0.99736625 -0.31471416 ...  0.20526893  0.00814523\n","   -0.41467914]\n","  [-0.03960834 -0.93892    -0.17954792 ...  0.2429632  -0.00834235\n","   -0.452601  ]\n","  ...\n","  [ 0.35614103 -0.38965848 -0.27384546 ... -0.02938731 -0.15403344\n","    0.07221778]\n","  [ 0.02199963 -0.27218857  0.13880713 ... -0.04169824  0.26157796\n","    0.09182303]\n","  [ 0.17606992 -1.1034262  -0.12621473 ... -0.2525679   0.11628371\n","    0.2366817 ]]]\n","[[[ 9.3297042e-02 -2.7078649e-01 -1.0472690e-02 ...  1.2782261e-01\n","   -1.6816127e-01  9.0169740e-01]\n","  [ 3.3740988e-01  4.2805174e-01 -4.9100149e-01 ...  2.8090021e-01\n","    2.3827232e-01  4.1614282e-01]\n","  [ 8.2606383e-02 -1.1995723e+00  1.2631418e-02 ...  3.7670016e-01\n","   -1.9059345e-01  4.3387216e-01]\n","  ...\n","  [ 1.7335072e-02 -8.9443630e-01 -3.9580902e-01 ... -1.1768020e-01\n","   -1.0037434e-01  1.4096325e-02]\n","  [ 6.3420452e-02 -2.1349691e-01  1.3322832e-01 ...  3.3628538e-01\n","    9.3517121e-04  1.6878422e-01]\n","  [-2.4333674e-01 -8.5275257e-01  7.2279170e-02 ...  6.5237910e-01\n","   -1.1932980e-01 -3.8119316e-01]]]\n","[[[ 0.14016572 -0.11448487 -0.011288   ...  0.22353216 -0.01660346\n","   -0.11273213]\n","  [ 0.4115105   0.99736625 -0.31471416 ...  0.20526893  0.00814523\n","   -0.41467914]\n","  [-0.03960834 -0.93892    -0.17954792 ...  0.2429632  -0.00834235\n","   -0.452601  ]\n","  ...\n","  [ 0.35614103 -0.38965848 -0.27384546 ... -0.02938731 -0.15403344\n","    0.07221778]\n","  [ 0.02199963 -0.27218857  0.13880713 ... -0.04169824  0.26157796\n","    0.09182303]\n","  [ 0.17606992 -1.1034262  -0.12621473 ... -0.2525679   0.11628371\n","    0.2366817 ]]]\n","[[[ 9.3297042e-02 -2.7078649e-01 -1.0472690e-02 ...  1.2782261e-01\n","   -1.6816127e-01  9.0169740e-01]\n","  [ 3.3740988e-01  4.2805174e-01 -4.9100149e-01 ...  2.8090021e-01\n","    2.3827232e-01  4.1614282e-01]\n","  [ 8.2606383e-02 -1.1995723e+00  1.2631418e-02 ...  3.7670016e-01\n","   -1.9059345e-01  4.3387216e-01]\n","  ...\n","  [ 1.7335072e-02 -8.9443630e-01 -3.9580902e-01 ... -1.1768020e-01\n","   -1.0037434e-01  1.4096325e-02]\n","  [ 6.3420452e-02 -2.1349691e-01  1.3322832e-01 ...  3.3628538e-01\n","    9.3517121e-04  1.6878422e-01]\n","  [-2.4333674e-01 -8.5275257e-01  7.2279170e-02 ...  6.5237910e-01\n","   -1.1932980e-01 -3.8119316e-01]]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"7HsuM-UT33v3"},"source":["##Q4B: Complete the function `process_training_dataset` (7 points)\n","\n","Your code below should be able to take a given dataset that you loaded in above and produce word embeddings for each word. Then, these word embeddings will be used to train a classifier. All you need to do is make sure your Xs and ys are shaped right and you should be good to go.\n","\n","The function `process_training_dataset` critically must take in:\n","\n","* A dataset (e.g., any of the above)\n","* A neural language model\n","* A tokenizer that the neural language model can work with\n","* A specific layer number that we subset to when building our training data\n","\n","And it will return:\n","* A trained classifier that can assign part-of-speech tags given word embeddings\n","\n","**Note**: Pay attention to your answer in the previous question so you can better extract the right word embeddings for all of your classifiers!"]},{"cell_type":"code","metadata":{"id":"69lnZN3RzIvD","executionInfo":{"status":"ok","timestamp":1637035321007,"user_tz":300,"elapsed":14,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}}},"source":["def process_training_dataset(dataset, neural_model, neural_tokenizer, layer_number):\n","    # define your Xs and ys (embeddings and POS tags)\n","    Xs, ys = [], []\n","    # loop over every sentence in the dataset\n","    for records in dataset:\n","            for record in records:\n","                # extract POS tags for that sentence\n","                # combine ys with the POS tags\n","                ys.append(record[1])\n","                # extract embeddings for that sentence\n","                # get embeddings at a specific layer\n","            embeddings = embed_words_roberta(records, neural_model, neural_tokenizer)[layer_number]\n","            embeddings = embeddings[0]\n","            Xs.append(embeddings)\n","    \n","    Xs = np.vstack(Xs)\n","    classifier = LogisticRegression(max_iter=1000)\n","    classifier.fit(Xs, ys)\n","    return classifier"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krSPKzmHJiyF"},"source":["**NOTE: I have used only 1000 lines from both the json files as I am running into run time issues.**"]},{"cell_type":"markdown","metadata":{"id":"np9O4uLoyDZt"},"source":["##Q4C: Train your model @ layer 0 on the 2020-only data (4 points)"]},{"cell_type":"code","metadata":{"id":"GGSIuQYCPNVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637039148469,"user_tz":300,"elapsed":340233,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"b279d0e8-6681-4dc1-f2e6-3adc3fbc69de"},"source":["train_2020_layer_0 = process_training_dataset(train_2020_only[0:1000], model, tokenizer, 0)\n","print(train_2020_layer_0)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression(max_iter=1000)\n"]}]},{"cell_type":"markdown","metadata":{"id":"2am1nWNJyGXf"},"source":["## Q4D: Model @ layer 0, on pre-2020 data (4 points)"]},{"cell_type":"code","metadata":{"id":"ohn8fdhNPLSL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637035975931,"user_tz":300,"elapsed":334785,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"2bfed7bb-ee58-43a4-dd86-7bbb8946d593"},"source":["train_pre2020_layer_0 = process_training_dataset(train_pre2020[0:1000], model, tokenizer, 0)\n","print(train_2020_layer_0)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression(max_iter=1000)\n","LogisticRegression(max_iter=1000)\n"]}]},{"cell_type":"markdown","metadata":{"id":"38d-8wOVyIs5"},"source":["## Q4E: Model @ layer 10, on 2020-only data (4 points)"]},{"cell_type":"code","metadata":{"id":"e68D2clLPLt6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637036364734,"user_tz":300,"elapsed":388838,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"4a82d4ca-d898-400b-9ea4-404e93b9d3d1"},"source":["train_2020_layer_10 = process_training_dataset(train_2020_only[0:1000], model, tokenizer, 10)\n","print(train_2020_layer_0)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression(max_iter=1000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["LogisticRegression(max_iter=1000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}]},{"cell_type":"markdown","metadata":{"id":"AzwTrjEOyKVm"},"source":["## Q4F: Model @ layer 10, on pre-2020 data (4 points)"]},{"cell_type":"code","metadata":{"id":"XoV94YzePMOM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637036743519,"user_tz":300,"elapsed":378791,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"0b753107-8409-4d1c-9b74-d8c002769c34"},"source":["train_pre2020_layer_10 = process_training_dataset(train_pre2020[0:1000], model, tokenizer, 10)\n","print(train_2020_layer_0)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression(max_iter=1000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]},{"output_type":"stream","name":"stdout","text":["LogisticRegression(max_iter=1000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}]},{"cell_type":"markdown","metadata":{"id":"gHFfDgejSCnG"},"source":["# Q5: Evaluate and compare all models (18 points)\n","\n","For this question, we would like you to compare each of the models to each other along several dimensions. The four models cross the layer within the model (0 or 10) and whether the model is trained on older or newer data (pre-2020 or 2020 data). In order to compare the models, you need to get each of your models to generate **predicted** labels for each of the test items. First, you will need to **construct your test dataset** and then evaluate each model along the following dimensions using the following functions:\n","\n","* Precision (`sklearn.metrics.precision_score`)\n","* Recall (`sklearn.metrics.recall_score`)\n","* F1 (`sklearn.metrics.f1_score`)\n","\n","Then, you will be asked to fill in the performance of each of these four models in the form of a table. You will find instances of this around some of the previous lectures (e.g., the natural language inference and evaluation lecture notebooks)."]},{"cell_type":"markdown","metadata":{"id":"KJVtas9uQ-rd"},"source":["## Q5A: Test data processing (3 points)\n","\n","In order for us to assess the ability for our models above to do well, we have to also process our test data. The way `scikit-learn` expects to produce predictions is very simple. When we _train_ a `LogisticRegression` model, we give the model as input some set of $X$ values (e.g., a matrix of word embeddings). The model we train tries to optimize the fit between $X$ and the $y$ values we give -- such as the labels associated with part-of-speech tags. Getting _predictions_ from our trained models is simply a matter of giving it new $X$ values -- from our test dataset.\n","\n","In order for this to work, we also have to process our test data to conform to the same structure as our training data. So, for this question, we would like you to make a function `process_test_dataset` that is just like the `process_train_dataset` but there is no need to train a model at the end at all. Instead, the function _only_ needs to return `Xs` (a matrix containing word embeddings) and `ys` (part-of-speech tags). The stub of what you need to do is below."]},{"cell_type":"code","metadata":{"id":"x9RfgiucZEex","executionInfo":{"status":"ok","timestamp":1637036743520,"user_tz":300,"elapsed":26,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}}},"source":["def process_test_dataset(layer_number):\n","    Xs, ys = [], []\n","    # implement the test version of process_train_dataset\n","    for records in test_2021:\n","            for record in records:\n","                # extract POS tags for that sentence\n","                # combine ys with the POS tags\n","                ys.append(record[1])\n","                # extract embeddings for that sentence\n","                # get embeddings at a specific layer\n","            embeddings = embed_words_roberta(records, model, tokenizer)[layer_number]\n","            embeddings = embeddings[0]\n","            Xs.append(embeddings)\n","    \n","    Xs = np.vstack(Xs)\n","    return Xs, ys"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSBN-mGKSV26"},"source":["## Q5B: Score all four models (12 points)\n","\n","Loop through each of your four models (output of last four notebook cells), print the precision, recall, and f1 scores. "]},{"cell_type":"code","metadata":{"id":"ZPiGfp1yfvmG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637038803694,"user_tz":300,"elapsed":2060196,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"f7a6b223-9791-4387-c579-7a8dd17945dd"},"source":["test_X_0, test_y_0 = process_test_dataset(0)\n","test_X_10, test_y_10 = process_test_dataset(10)\n","# use precision_score, recall_score, f1_score\n","\n","#train_2020_layer_0\n","print(precision_score(test_y_0, train_2020_layer_0.predict(np.vstack(test_X_0)), average='macro'))\n","print(recall_score(test_y_0, train_2020_layer_0.predict(np.vstack(test_X_0)), average='macro'))\n","print(f1_score(test_y_0, train_2020_layer_0.predict(np.vstack(test_X_0)), average='macro'))\n","\n","#train_pre2020_layer_0\n","print(precision_score(test_y_0, train_pre2020_layer_0.predict(np.vstack(test_X_0)), average='macro'))\n","print(recall_score(test_y_0, train_pre2020_layer_0.predict(np.vstack(test_X_0)), average='macro'))\n","print(f1_score(test_y_0, train_pre2020_layer_0.predict(np.vstack(test_X_0)), average='macro'))\n","\n","#train_2020_layer_10\n","print(precision_score(test_y_10, train_2020_layer_10.predict(np.vstack(test_X_10)), average='macro'))\n","print(recall_score(test_y_10, train_2020_layer_10.predict(np.vstack(test_X_10)), average='macro'))\n","print(f1_score(test_y_10, train_2020_layer_10.predict(np.vstack(test_X_10)), average='macro'))\n","\n","#train_pre2020_layer_10\n","print(precision_score(test_y_10, train_pre2020_layer_10.predict(np.vstack(test_X_10)), average='macro'))\n","print(recall_score(test_y_10, train_pre2020_layer_10.predict(np.vstack(test_X_10)), average='macro'))\n","print(f1_score(test_y_10, train_pre2020_layer_10.predict(np.vstack(test_X_10)), average='macro'))"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8861238524782241\n","0.8734297379642223\n","0.8779961787711147\n","0.9043365410857915\n","0.8712839775343517\n","0.876164333102519\n","0.9117594565613115\n","0.8934593470549509\n","0.9010359861706775\n","0.9226297220235957\n","0.8956899743416384\n","0.90478276877555\n","0.8861238524782241\n","0.8734297379642223\n","0.8779961787711147\n","0.9043365410857915\n","0.8712839775343517\n","0.876164333102519\n","0.9117594565613115\n","0.8934593470549509\n","0.9010359861706775\n","0.9226297220235957\n","0.8956899743416384\n","0.90478276877555\n"]}]},{"cell_type":"markdown","metadata":{"id":"bw7VwboyPXwb"},"source":["## Q5C: Free response (3 points)\n","\n","Using the outputs above, fill out a table showing performance across each of the 4 models, along all 3 measures. Describe in words how the models differ in their performance. Are there any patterns you notice that determine model performance? Were any of the results surprising to you? Why or why not? If the differences are small, can you think of a reason why we might not trust these results?"]},{"cell_type":"markdown","metadata":{"id":"ZIo3lBuJPfq4"},"source":["<table style=\"width:100%\">\n","  <tr>\n","    <th>Models</th>\n","    <th>Precision</th>\n","    <th>Recall</th>\n","    <th>F1 Score</th>\n","  </tr>\n","  <tr>\n","    <td>train_2020_layer_0</td>\n","    <td>0.8861238524782241</td>\n","    <td>0.8734297379642223</td>\n","    <td>0.8779961787711147</td>\n","  </tr>\n","  <tr>\n","    <td>train_pre2020_layer_0</td>\n","    <td>0.9043365410857915</td>\n","    <td>0.8712839775343517</td>\n","    <td>0.876164333102519</td>\n","  </tr>\n","  <tr>\n","    <td>train_2020_layer_10</td>\n","    <td>0.9117594565613115</td>\n","    <td>0.8934593470549509</td>\n","    <td>0.9010359861706775</td>\n","  </tr>\n","  <tr>\n","    <td>train_pre2020_layer_10</td>\n","    <td>0.9226297220235957</td>\n","    <td>0.8956899743416384</td>\n","    <td>0.90478276877555</td>\n","  </tr>\n","</table>\n","\n","When compared to the f1 score and recall score, precision has a high value in all three models. And when compared to layer 0, layer 10 has better performance in all scores. As a result, we may deduce that scores will improve as the number of layers is increased exponentially. Furthermore, in both layers 0 and 10, the dataset train_pre2020 outperforms the dataset train_2020 in Precision score but in Recall and F1 score, in layer 0, dataset train_2020 has high value and in layer 10, train_pre2020 has high value. The results were unexpected and surprising because the dataset train_pre2020 outperformed train_2020 in both layers in Precision score but has less value in Recall and F1 score in layer 0. Yes, the differences between these results across all the models are small, and I find it difficult to believe these results because all the scores in all the models are above 0.85 and I find hard to believe the the clasifier is trained properly as it is trained with small amount of data."]},{"cell_type":"markdown","metadata":{"id":"XkkRxQe3SEnb"},"source":["# Bonus: Error analysis (6 points; 3 for code, 3 for free response)\n","\n","*   Take the best-performing model\n","*   Construct a confusion matrix in any way that you would like, comparing the output of the best model on your test set and the true labels.\n","*   What categories are most confusable? What linguistic reasons might that be the case?"]},{"cell_type":"code","metadata":{"id":"tYcR1M6Fc37B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637038805646,"user_tz":300,"elapsed":1966,"user":{"displayName":"Aditya Athota","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12252802024110680972"}},"outputId":"333a04d5-feb8-424a-8020-ac7e37acb275"},"source":["confusion_matrix(test_y_10, train_pre2020_layer_10.predict(np.vstack(test_X_10)))"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[18258,     7,     0,     0,     1,     0,    12,     4,     0,\n","           18,     2,     0],\n","       [    1, 14975,    14,   294,     4,    30,  1968,    42,     2,\n","            0,   818,     1],\n","       [    1,    32, 17059,    67,     5,   111,    32,     0,     0,\n","            1,    54,     2],\n","       [    5,   261,    58,  4853,     3,    14,   128,     0,     7,\n","            2,    86,     0],\n","       [    0,     3,     6,    24,  4627,    12,     1,     0,     1,\n","            0,     1,     3],\n","       [    0,    81,    80,    17,     0, 13948,    18,     8,    15,\n","            1,     2,     0],\n","       [   18,  1964,    28,   107,     1,    29, 43327,    31,    13,\n","            2,  1119,    16],\n","       [    1,    35,     0,     4,     0,    14,    20,  2199,     0,\n","            0,     1,     0],\n","       [    0,    23,     3,     9,     0,    36,    13,     1,  4636,\n","            0,     2,     0],\n","       [    9,     3,    41,     1,     0,     0,    11,     0,     0,\n","         3685,    13,     0],\n","       [    4,   725,    47,    78,     3,     5,  1026,     0,     0,\n","            0, 19455,     1],\n","       [    0,    15,     0,     8,     0,     0,    49,     0,     0,\n","            0,    12,    37]])"]},"metadata":{},"execution_count":14},{"output_type":"execute_result","data":{"text/plain":["array([[18258,     7,     0,     0,     1,     0,    12,     4,     0,\n","           18,     2,     0],\n","       [    1, 14975,    14,   294,     4,    30,  1968,    42,     2,\n","            0,   818,     1],\n","       [    1,    32, 17059,    67,     5,   111,    32,     0,     0,\n","            1,    54,     2],\n","       [    5,   261,    58,  4853,     3,    14,   128,     0,     7,\n","            2,    86,     0],\n","       [    0,     3,     6,    24,  4627,    12,     1,     0,     1,\n","            0,     1,     3],\n","       [    0,    81,    80,    17,     0, 13948,    18,     8,    15,\n","            1,     2,     0],\n","       [   18,  1964,    28,   107,     1,    29, 43327,    31,    13,\n","            2,  1119,    16],\n","       [    1,    35,     0,     4,     0,    14,    20,  2199,     0,\n","            0,     1,     0],\n","       [    0,    23,     3,     9,     0,    36,    13,     1,  4636,\n","            0,     2,     0],\n","       [    9,     3,    41,     1,     0,     0,    11,     0,     0,\n","         3685,    13,     0],\n","       [    4,   725,    47,    78,     3,     5,  1026,     0,     0,\n","            0, 19455,     1],\n","       [    0,    15,     0,     8,     0,     0,    49,     0,     0,\n","            0,    12,    37]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"bypCHKcc-tqJ"},"source":["<font color=\"red\">Your bonus question answer goes here.</font>"]}]}